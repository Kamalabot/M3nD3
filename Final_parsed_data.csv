"index","topic_head","links","data","code"
0,in_python_using_plotnine/,https://analyticsindiamag.com/a-hands-on-guide-to-implementing-ggplot-in-python-using-plotnine/,"['Visualization of the data plays a crucial role in the majority of data analytics tasks. The ggplot package of the R programming language makes the R richer on the side of data visualization. In python as well, various packages are also available for data visualization. If the features and capabilities of ggplot can be used in python, it will be a valuable advantage in many visualization specific tasks. In this article, we are going to explore how we can use ggplot in python for visualizing data using a package named plotnine that is based on ggplot2. The major points to be discussed in the article are listed below.', 'Table of content', 'Let’s begin with having a brief introduction to ggplot.', 'What is ggplot?', 'We mainly know ggplot as a package used in R for data visualizations. This package is responsible for making R one of the best tools in the world of data visualization.\xa0 This package is created by Hadley Wickham and can be considered as an implementation of the grammar of graphics suggested by Leland Wilkinson.\xa0', 'Grammar of graphics can be considered as a scheme that stands for breaking the graphs into semantic components. Examples of components of graphs can be scales and layers. Ggplot becomes more useful because of its ability to create stylish and clear graphs. From a normal user to a high end-user of the R language, they use this package for visualization.\xa0', 'In this article, we are going to use the plotnine package for the implementation of the ggplot package in python. There are several python packages like matplotlib, plotly, ggpy, etc. in python for visualization, but ggplot’s capabilities also need to be explored. To explore this, we will use the plotnine package that covers all the ggplot features and extend them to python.', 'We can install plotnine using the following lines of codes:', ""pip install 'plotnine[all]'"", 'After installation, we are ready to use ggplot for visualizing data in python.', 'First plot', 'In this section, we will get to know about how we can use the practice datasets of plotline packages that are available in the form of pandas tabular data. We can say that each dataset available in this package is in the form of a pandas data frame. We can call them using the plotnine subpackage plotnine.data. We can find the list of practice datasets of plotnine here.', 'Let’s import the mtcars dataset.', 'Here we can see values in our data frame. Let’s use ggplot for making a plot choosing any two variables.', 'Here we can see we have plotted mpg against displacement of the cars using the ggplot inside the plotnine. Let’s move to the deep side of ggplot.', 'Factorization of datapoints', '\xa0In the dataset, we have seen that we have many categorical values like we can categorize our data based on the number of cylinders used by the engine. The above plot can also be factored using colours according to the number of cylinders in the following way.', 'We can also factor graphs instead of just factorization using points using the following lines of code.', 'output:', 'Let’s make it more stylish.', 'Here we have used the theme of ggplot to make the visualization of data more attractive.', 'Ggplot with pandas data frame\xa0', 'In this section we will work with pandas data frame for making plots using ggplot, for this, we are using the titanic dataset that can be found here. Let’s import the dataset.', 'Let’s draw a plot that can tell us how many people from titanic data survived according to their passenger class.', 'Here we can see a bar chart we made using ggplot for titanic data. Let’s make a plot that can tell us the average age of people who survived and did not survive.', 'Here we can see that in data we have people who survived are mostly of age 20. Let’s take a look at the graph categorization based on sex.', 'Here we have segregated the people based on their sex and survival status using a box plot that also represents a range according to age. Let’s segregate the plot more.', 'Here we can see a segregated box plot according to sex and class.\xa0', 'Final words', 'In the article, we have seen how we can use the data visualization features of ggplot in python. For this purpose, we have used plotnine as our base package for ggplot. Using ggplot we have made our visualization procedure more attractive and easy.\xa0']","""pip install 'plotnine[all]'""
 'from plotnine import *\nfrom plotnine.data import mtcars\nmtcars\n', ""plot = (ggplot(mtcars, aes('disp', 'mpg'))\n + geom_point())\n \nplot\n"", ""plot = (ggplot(mtcars, aes('disp', 'mpg'))\n + geom_point())\n \nplot\n"", ""plot =(ggplot(mtcars, aes('disp', 'mpg', color='factor(cyl)'))\n + geom_point()\n + facet_wrap('~cyl'))\nplot\n"", ""plot =(ggplot(mtcars, aes('disp', 'mpg', color='factor(cyl)'))\n + geom_point()\n + facet_wrap('~cyl')\n + theme_xkcd())\n \nplot\n"", ""import pandas as pd\ndata = pd.read_csv('https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/'+'titanic.csv')\ndata.head()"", 'data[\'Survived\'] = data[\'Survived\'].astype(\'category\')\nggplot(aes(x=""Pclass"", fill=""Survived""), data) + geom_bar(stat = \'count\') + theme_xkcd() ', ""ggplot(data, aes(x='Survived', y='Age')) + \\\n    geom_violin()"", ""ggplot(data, aes(x='Survived', y='Age')) + \\\n    geom_boxplot() + \\\n    facet_wrap(['Sex'])"", ""ggplot(data, aes(x='Survived', y='Age')) + \\\n    geom_boxplot() + \\\n    facet_wrap(['Sex','Pclass']) + theme_xkcd()"""
1,for_drawing_interactive_joyplots/,https://analyticsindiamag.com/a-hands-on-guide-to-joypy-for-drawing-interactive-joyplots/,"['Understanding the distribution of data is very important in data analytics and it can be done easily using different types of visualizations. Also, sometimes, we need to make density plots stacked and partially overlapped for better understanding. JoyPY is a python package that helps us in plotting such visualizations through Joyplot. A Joyplot is a series of histograms, density plots or time series for a number of data segments, all aligned to the same horizontal scale. In this article, we are going to discuss how we can make Joyplots using the JoyPy package. The major points to be discussed in the article are listed below.', 'Let’s begin with having a quick introduction to JoyPy.', 'JoyPy is a low code python package that can help us in visualization based on ridgeline plots. It is mainly designed using Matplotlib and Pandas. To draw the ridgeline plots which they say joyplots, this package takes codes from Pandas kdes plots. This library can be compared to R package ggridges that is also named as ggjoy in its older version.', 'In simple words, joyplots are density plots but stacking and overlapping make them different from density plots. We mainly use this kind of plot for cross-check distribution of the data. Density plots are very helpful for measuring the changes in the data across one dimension. Stacking and partially overlapping make them more helpful to understand the distribution of the data. We can also call these plots ridgeline plots.\xa0\xa0', 'The above visualization can be considered as an example of joyplots. Let’s see how we can start with joyplots using JoyPy.', 'In the implementation, we will start with very basic joyplots using the iris dataset from scikit learn. Before plotting data we need to install joyPy that can be performed using the following lines of codes.', '!pip install joypy', 'Now we are ready to draw joyplots using python language. Let’s call the important libraries\xa0', 'Let’s call the sklearn provided iris data.', 'In the above, we can see our dataset. Let’s check the density of our data using JoyPy.', 'Here we can see an example of a joyplot or we can also call it a ridgeline plot of iris data.', 'We also know that with the data we have a group of names we can also plot a joyplot using the different groups. For doing this we are just required to pass the name of the variable that has group information in the data.\xa0', 'fig, axes = joypy.joyplot(iris, by=""Name"")', 'Let’s say in any dataset we have the size of the y-axis in a larger size than just by just defining limits we can compress it like following:', 'fig, axes = joypy.joyplot(iris, by=""Name"", ylim=\'own\')', 'In above visualization, we can see that the subplot is not comparable directly because of overlapping we can adjust it by using the overlap factor.', 'fig, axes = joypy.joyplot(iris, by=""Name"", overlap=3)', 'We can also check the distribution of the data using the histogram.', 'Here we got to know about how we can use JoyPy for generating joyplots of the data efficiently.\xa0 Of course, we can perform more things in our joyplots. Let’s use some other datasets, for example, we are using the global temperature as our dataset which can be found here.', 'Let’s import and see the details of our dataset.', 'In the data, we can see the anomaly columns that represent the difference between daily values. Using this data we are going to draw a joyplot by grouping the years.\xa0', 'Here in the plot, we can see how the daily temperature distribution of our data shifted across time. We can also make it more use grid function to map the plot better.', 'Here we have also provided zero value to the linewidth function. We can also make it faded for a better understanding of the data.', 'Here we have a much clearer view of the temperature distribution. We can also change the background and color of the lines.', 'Now the distribution of the data has been differently plotted than the other plots. Maybe things are not clear but I performed it just to let us know how using a single command we can perform changes in the visualization using the JeoPy package.', 'Final words', 'In this article, we have gone through the usage of the JoyPy package that is similar to the ggjoy package in R. We have performed some of the visualizations and seen how we can change them according to different situations and measurements to make joyplots.\xa0\xa0\xa0']","!pip install joypy'
 'import joypy\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib import cm\nfrom sklearn.datasets import load_iris', 'iris, y = load_iris(as_frame=True, return_X_y=True)\niris.columns = [""SepalLength"",""SepalWidth"",""PetalLength"",""PetalWidth""]\niris[""Name""] = y.replace([0,1,2], [\'setosa\', \'versicolor\', \'virginica\'])\niris\n', '%matplotlib inline\nfig, axes = joypy.joyplot(iris)', 'fig, axes = joypy.joyplot(iris, by=""Name"")', 'fig, axes = joypy.joyplot(iris, by=""Name"", ylim=\'own\')', 'fig, axes = joypy.joyplot(iris, by=""Name"", overlap=3)', 'fig, axes = joypy.joyplot(iris, by=""Name"", column=""SepalWidth"",\n                          hist=True, overlap=0)', 'df = pd.read_csv(\'https://raw.githubusercontent.com/leotac/joypy/master/data/daily_temp.csv\',comment=""%"")\ndf\n', 'labels=[y if y%10==0 else None for y in list(df.Year.unique())]\nfig, axes = joypy.joyplot(df, by=""Year"", column=""Anomaly"", labels=labels, range_style=\'own\', \n                          linewidth=1, legend=True, figsize=(6,5),\n                          title=""Global daily temperature 1880-2014"",\n                          colormap=cm.autumn_r)', 'fig, axes = joypy.joyplot(df, by=""Year"", column=""Anomaly"", labels=labels, range_style=\'own\', \n                          grid=""y"", linewidth=0, legend=True, figsize=(6,5), fade=True,\n                          title=""Global daily temperature 1880-2014"",\n                          colormap=cm.autumn_r)', 'fig, axes = joypy.joyplot(df,by=""Year"", column=""Anomaly"", ylabels=False, xlabels=False, \n                          grid=False, fill=False, background=\'k\', linecolor=""g"", linewidth=1,\n                          legend=False, overlap=0.5, figsize=(6,5),kind=""counts"", bins=80)'"
2,dimensional_data_using_hypertools/,https://analyticsindiamag.com/how-to-visualize-and-manipulate-high-dimensional-data-using-hypertools/,"['When working with large real-world datasets, dealing with the curse of dimensionality is a common problem. The dimensionality reduction techniques can help overcome these problems. The first step in dimensionality reduction is feature extraction. It’s simply removing redundant and irrelevant features from a dataset in order to extract relevant features. Choosing a visualization method for such high-dimensional data is a time-consuming task. To automate this process, we can use HyperTools, a Python-based tool designed specifically for higher-dimensional data visualization. In this article, we will discuss HyperTools in detail and how it can help in this task. Below are the major points that we are going to discuss.', 'Let’s first discuss Data visualization.', 'Data visualization is a graphical representation of information and data. Data visualization techniques, such as charts, graphs, and maps, make it simple to see and understand trends, outliers, and patterns in data.', 'We have a lot of data in our hands nowadays, so data visualization tools and technologies are essential for analyzing massive amounts of data and making data-driven decisions. It is used in a variety of fields, such as, and. Visualize phenomena that you can’t see directly, such as weather patterns, medical conditions, or mathematical relationships, in order to simulate complex events.', 'While plotting the change in a variable over time as coordinates on a graph is a relatively simple way to visualize low-dimensional data, it is not always obvious how to visualize high-dimensional datasets in a similarly intuitive way.', 'In this section, we’ll look at the open-source ToolBox Hypertools, which is based on Python and creates visualizations from high-dimensional datasets by reducing the dimensionality on its own. It is primarily based on matplotlib, sklearn, and seaborn. The core of this tool is that it uses dimensionality reduction techniques to embed high-dimensional datasets in a lower-dimensional space, then plots the data using a simple yet powerful API with numerous data manipulation (e.g., hyperalignment, clustering, normalizing, etc.) and plot styling options.', 'Data trajectories and point clouds are central to the toolbox’s design. HyperTools uses dimensionality reduction algorithms to create similar 2D and 3D trajectories for time series of high-dimensional observations. The trajectories can be visualized as animations or as interactive static plots. Static datasets (e.g. collections of observations or attributes) can benefit from the same dimensionality reduction and alignment algorithms.', 'In summary, the HyperTools toolbox provides a set of powerful functions for visualizing and manipulating high-dimensional data with the help of dimensionality reduction and data alignment algorithms. The toolbox is built with ease of use in mind, so even complex visualizations and analyses can be done with just a single line of code. Another major goal is to make it simple for users to create visually appealing publication-quality plots with only a single line of code, which is often the case.', 'The Below picture depicts how the toolbox uses the Dimensionality Reduction algorithms to map the data in 2D and 3D plots (a). And the second (b) one is about to show the data from different coordinate systems. The left panel shows three trajectories with similar geometries but different coordinate systems, while the right panel shows how those trajectories can be aligned into a common space using hyperalignment (via linear transformations).\xa0', 'To use this toolbox, we need to install it and this can be done by using simply pip. Directly installing using pip without specifying version will install the latest version and there Version Conflict issue with the latest package to avoid this Install 0.6.3 version otherwise, you will end with a dependencies issue.', '! pip install hypertools==0.6.3', 'To use the HyperTools plot functionality, first, load the dataset to be analyzed into the Python workspace and convert it to a Numpy array or a Pandas data frame. The data format should be sampled (S) by features (F). Once the dataset has been converted to this format, simply import the library and call the plot function, as shown below for various plots. We’ll be using built-in datasets in this case.', 'For the first plot, we are using inbuilt spiral data which is a NumPy array containing data of 3D spirals used to highlight the linear transformation of data.\xa0', 'This plot function will perform dimensionality reduction (by default, using PCA), convert the S x F data matrix to an S x 3 matrix, and then generate an interactive 3D line plot that can be visually explored as below.', 'To facilitate automatic data clustering, HyperTools includes the K-Means clustering algorithm. The cluster keyword argument to the plot function, given a predetermined number of clusters, k, uses k-means clustering to automatically assign each observation to a cluster and then colours each observation’s point based on its cluster membership.', 'In this case, we are obtaining a cluster for the digits of the sklearn-created dataset, which represents 64 different pixel data in the NumPy array. We’ll draw a cluster for the first five digits.', 'To have a 3D animated plot simply toggle the animate keyword to create an animation inside the plot function. This will result in a 3D animated representation of the data, with the animation taking place across the rows of the data matrix.\xa0', 'To plot multiple datasets on a single plot, the user can pass a list of data matrices, as with static plots, and format strings and keyword arguments can be used to customize the plot appearance. Each frame of the animation depicts a subset of the total data trajectory contained within a cube.\xa0', 'The displayed portion of the data trajectory advances by a small amount in each frame, and the camera angle rotates around the cube, providing visual access to various aspects of the data as the animation progresses.', 'The 3D animation can be obtained as below,']","'! pip install hypertools==0.6.3'
 ""import hypertools as hyp\n# load the sample data\nweights = hyp.load('spiral')\n# Creating plot\nweights.plot(size=[7,5])"", ""from sklearn import datasets\ndata = datasets.load_digits(n_class=5)\ndf = data.data\nhue = data.target.astype('str')\nhyp.plot(df, '.', hue=hue, ndims=3,legend=[0,1,2,3,4],size=[7,5])"", ""import numpy as np\n \ndata = hyp.tools.load('weights', align=True)\n \ngroup1 = np.mean(data.data[:17], 0)\ngroup2 = np.mean(data.data[18:], 0)\n \nhyp.plot([group1, group2], animate=True, save_path='animation.mp4',zoom=2)"""
3,detection_with_alibi_detect/,https://analyticsindiamag.com/a-hands-on-guide-to-outlier-detection-with-alibi-detect/,"['The detection of dataset elements that differ significantly from the majority of instances is known as outlier detection. There are various visualization methods and statistical tests, such as z-test, Grubb’s test and other algorithms used to detect them. The Alibi Detect is a toolbox, which is used to detect anomalies such as outliers, dataset drift, and adversarial attacks in a variety of data types such as tabular data, images, time series, and so on, in the context of AutoML. We will discuss this toolbox in detail in this post. Below is a list of the major points to be discussed.', 'Let’s start the discussion by understanding Outlier Detection.', 'Data points that are unusually far apart from the rest of the observations in a dataset are known as outliers. They are primarily caused by data errors (measurement or experimental errors, data collection or processing errors, and so on) or naturally very singular and different behaviour from the norm, for example, in medical applications, very few people have upper blood pressure greater than 200, so If we keep them in the dataset, our statistical analysis, and modelling conclusions will be skewed.\xa0', 'To name a few, they can alter the mean and standard deviation values. As a result, it’s critical to accurately detect and handle outliers, either by removing them or reducing them to a predefined value. Outlier detection is thus critical for identifying anomalies whose model predictions we can’t trust and shouldn’t use in production.\xa0', 'The type of outlier detector that is appropriate for a given application is determined by the data’s modality and dimensionality, as well as the availability of labelled normal and outlier data and whether the detector is pre-trained (offline) or updated online. The offline detector can be deployed as a stateful application, while the pre-trained detector can be deployed as a static machine learning model.', 'The goal of the Mahalanobis online outlier detection is to predict anomalies in tabular data. The algorithm computes an outlier score, which is a measure of distance from the feature distribution’s centre (Mahalanobis distance). If this outlier score exceeds a user-specified threshold, the observation is marked as an outlier.\xa0', 'The algorithm is online, which means it begins with no knowledge of feature distribution and learns as requests arrive. As a result, you should expect the output to be poor at first and improve over time. The algorithm works well with low to medium dimensional tabular data.', 'Isolation forests (IF) are tree-based methods for detecting outliers. The IF isolates observations by randomly selecting a feature and then randomly determining a split value between the feature’s maximum and minimum values. The number of splittings necessary to isolate a sample is equal to the length of the path from the root node to the terminating node.\xa0', 'When averaged over a forest of random trees, this path length is a measure of normalcy that is used to create an anomaly score. Outliers are typically isolated more quickly, resulting in shorter routes. The technique performs effectively with tabular data in the low to medium dimension range.', 'The outlier detector, the Variational Auto-Encoder (VAE), is first trained on a batch of unlabeled but normal (inlier) data. Because labelled data is often scarce, unsupervised or semi-supervised training is preferable. The VAE detector makes an attempt to reconstruct the data it receives. The reconstruction error is high if the input data cannot be reconstructed well, and the data can be flagged as an outlier.\xa0', 'The mean squared error (MSE) between the input and the reconstructed instance or the probability that both the input and the reconstructed instance are generated by the same process is used to calculate the reconstruction error. This algorithm works well with both tabular and image data.', 'The Sequence-to-Sequence (Seq2Seq) outlier detector is made up of two main components: an encoder and a decoder. A Bidirectional LSTM processes the input sequence and initializes the decoder in the encoder. The LSTM decoder then predicts the output sequence sequentially. The decoder’s goal, in this case, is to reconstruct the input sequence.\xa0', 'If the input data cannot be well reconstructed, the reconstruction error is high, and the data is flagged as an outlier. The mean squared error (MSE) between the input and the reconstructed instance is used to calculate the reconstruction error.', 'Below Table is shown summarizes which algorithms under the hood of this toolbox can be used for outlier detection based on the type of data.\xa0', 'Alibi Detect is a Python library for detecting outliers, adversarial data, and drift. The package aims to include detectors for tabular data, text, images, and time series that can be used both online and offline. For drift detection, both TensorFlow and PyTorch backends are supported.', 'In fact, Alibi Detect supports a variety of outlier detection techniques, including Mahalanobis distance, Isolation forest, and Seq2seq. The library can also handle a variety of data types, including tabular, image, text, and time series. Different types of algorithms are required depending on the type of data. Now let’s take a look at that algorithm briefly so that we can have a basic understanding.', 'To find outliers, we’ll use the Inception forest algorithm. The dataset we’re using here is built-in toolbox data for detecting computer network intrusions using Transmission Control Protocol (TCP) dump data for a simulated local-area network (LAN).', '\xa0A connection is a set of TCP packets that start and stop at predetermined times and transport data from a source IP address to a destination IP address using a predetermined protocol. Each connection is classified as either safe or dangerous.', 'Let’s start by installing and importing the dependencies.', 'Now we will load the dataset and define a normal batch of data and normalize the data.', 'Next, we will define an outlier detector.', 'The above definition will return a warning as:', 'We still need to establish the outlier threshold, according to the warning. The infer_threshold method can be used to accomplish this. We’ll need to pass a batch of instances and use threshold_perc to define what percentage of them we consider typical. Assume we have some data with a known percentage of outliers of roughly 5%. In the create_outlier_batch function, perc_outlier can be used to set the proportion of outliers.', 'Here is the settled outlier and threshold:', 'Now, similar to before, we construct a batch of data containing 10% outliers and use our detector to find the outliers in the batch.', 'Now to evaluate the performance of this model we will use the confusion matrix, Fa score and accuracy score for the actual outlier and predicted outlier.\xa0', 'Now we can also plot the instance level outlier scores Vs the outlier threshold for better understanding using the method plot_instance_score.\xa0', 'plot_instance_score(od_preds, y_outlier, labels, od.threshold)', 'With an outlier score of about 0, we can see that the isolation forest does not perform a good job of recognizing one type of outlier. This makes determining a good threshold without knowing the outliers type is difficult. Setting the threshold slightly below 0 would result in much-improved detector performance for the dataset’s outliers.']","'! pip install alibi-detect\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix
 f1_score\n \nfrom alibi_detect.od import IForest\nfrom alibi_detect.datasets import fetch_kdd\nfrom alibi_detect.utils.data import create_outlier_batch\nfrom alibi_detect.utils.fetching import fetch_detector\nfrom alibi_detect.utils.saving import save_detector, load_detector\nfrom alibi_detect.utils.visualize import plot_instance_score, plot_roc\n', ""# load data\nkddcup = fetch_kdd(percent10=True)  # only load 10% of the dataset\n\n# create normal batch\nnp.random.seed(0)\nnormal_batch = create_outlier_batch(kddcup.data, kddcup.target, n_samples=400000, perc_outlier=0)\nX_train, y_train = normal_batch.data.astype('float'), normal_batch.target\n\n# apply normalization\nmean, stdev = X_train.mean(axis=0), X_train.std(axis=0)\nX_train = (X_train - mean) / stdev\n"", ""# initialize outlier detector\nod = IForest(threshold=None,  # threshold for outlier score\n             n_estimators=100)\n# train\nod.fit(X_train)\n# save the trained outlier detector\nsave_detector(od, '/content/')"", ""# create batch of outlier\nnp.random.seed(0)\nperc_outlier = 5\nthreshold_batch = create_outlier_batch(kddcup.data, kddcup.target, n_samples=1000, perc_outlier=perc_outlier)\nX_threshold, y_threshold = threshold_batch.data.astype('float'), threshold_batch.target\nX_threshold = (X_threshold - mean) / stdev\nprint('{}% outliers'.format(100 * y_threshold.mean()))\n \n# add threshold to detector\nod.infer_threshold(X_threshold, threshold_perc=100-perc_outlier)\nprint('New threshold: {}'.format(od.threshold))"", ""# new batch for prediction\nnp.random.seed(1)\noutlier_batch = create_outlier_batch(kddcup.data, kddcup.target, n_samples=1000, perc_outlier=10)\nX_outlier, y_outlier = outlier_batch.data.astype('float'), outlier_batch.target\nX_outlier = (X_outlier - mean) / stdev\nprint(X_outlier.shape, y_outlier.shape)\nprint('{}% outliers'.format(100 * y_outlier.mean()))\n\n# predicting\nod_preds = od.predict(X_outlier, return_instance_score=True)"", ""labels = outlier_batch.target_names\ny_pred = od_preds['data']['is_outlier']\nf1 = f1_score(y_outlier, y_pred)\nacc = accuracy_score(y_outlier, y_pred)\nprint('F1 score: {:.4f},\\n Accuracy Score: {:.4f}'.format(f1, acc))\ncm = confusion_matrix(y_outlier, y_pred)\ndf_cm = pd.DataFrame(cm, index=labels, columns=labels)\nsns.heatmap(df_cm, annot=True, cbar=True, linewidths=.5)\nplt.show()\n"""
4,learning_using_pytorch_tabular/,https://analyticsindiamag.com/how-to-handle-tabular-data-for-deep-learning-using-pytorch-tabular/,"['In tabular data, deep learning has traditionally lagged behind the popular Gradient Boosting in terms of popularity and performance. However, newer models developed expressly for tabular data, such as XBNet, have recently pushed the performance bar. In this post, we will look into PyTorch Tabular, a framework designed specifically for tabular data with an intention to make deep learning with tabular data easy and accessible to real-world cases. We will discuss how this framework was made, what design principles it follows, and how it can be applied. The major points to be discussed in this article are listed below.', 'Let’s start the discussion by knowing the creation of the framework.', 'PyTorch Tabular is a framework for deep learning using tabular data that aims to make it simple and accessible to both real-world applications and academics. The following are the design principles for the library:', 'PyTorch Tabular aims to make dealing with Neural Networks’ software engineering as straightforward and painless as possible, enabling you to focus on the model. It also aims to bring together the many breakthroughs in the Tabular sector into a single framework with a common API that can be used with a variety of cutting-edge models. It also comes with a base model that can be readily customized to help Deep Learning researchers create new tabular data architectures.', 'The PyTorch Tabular stands on the shoulders of titans such as PyTorch, PyTorch Lightning, and Pandas.', 'Source', 'PyTorch Tabular is intended to make the standard modeling pipeline simple enough for practitioners while also being reliable enough for production use. It also focuses on customization so that it can be used in a variety of research settings. The below picture depicts the structure of the framework.', 'Source', 'Now let’s briefly discuss all the modules from the framework. We first start with Configuration Modules.', 'DataConfig is where we define the parameters for how we will manage data within the pipeline. This configuration differentiates between categorical and continuous features, determines normalization or feature transformations, and so on.', 'For each model implemented in the PyTorch Tabular, a new ModelConfig is defined. It derives from a base ModelConfig that contains common parameters such as job (classification or regression), learning rate, loss, metrics, and so forth. Each model developed inherits these parameters and adds model-specific hyperparameters to the configuration. PyTorch Tabular automatically initializes the correct model by selecting the matching ModelConfig.', 'TrainerConfig manages all of the parameters that control your training, with the PyTorch Lightning layer receiving the majority of them. Batch size, max epochs, early stopping, and other parameters can be set here.', 'Another important aspect of training a neural network is optimizers and learning rate schedules. The OptimizerConfig can be used to make these changes.', 'Experiment tracking is practically a requirement of machine learning. It’s crucial for maintaining reproducibility. Internally, the PyTorch Tabular recognizes this and provides experiment tracking. Tensorboard and Weights & Biases are the two experiment tracking frameworks that PyTorch Tabular currently supports.', 'PyTorch Tabular makes use of the abstract BaseModel class, which implements the standard parts of any model definition, such as loss and metric calculation, and so on. This class acts as a foundation for any other model and guarantees that the model and the training engine work together seamlessly. The model initialization component and the forward pass are the only two methods that a new model must implement if it inherits this class.', 'The Data Module, as specified by PyTorch Lightning, is used by PyTorch Tabular to unify and standardize data processing. It includes preprocessing, label encoding, category encoding, feature transformations, target transformations, and other data processing, as well as ensuring that the same data processing is performed to train and validate splits, as well as fresh and unseen data. PyTorch data loaders are provided for training and inference.', 'In this section, we will implement the framework with the support of the SK-Learn module for the dataset supplies and evaluation metrics.\xa0', 'Install all the PyTorch Tabular with its core functionality using the pip as', '\xa0! pip install PyTorch_tabular[all]', 'tabular_model.fit(train=train, validation=val)']","'\xa0! pip install PyTorch_tabular[all]'
 'tabular_model.fit(train=train, validation=val)'"
5,framework_for_tabular_data/,https://analyticsindiamag.com/inside-pytorch-tabular-a-deep-learning-framework-for-tabular-data/,"['Earlier this month, PyTorch Tabular v0.7.0 was released on PyPI. This latest version of PyTorch Tabular aims to make deep learning with tabular data easy and accessible to real-world use cases and research. The core principle behind the library’s design includes low resistance useability, easy customisation, and easy deployment and scalability. The source code is available on GitHub.\xa0', 'Despite the unreasonable effectiveness in modalities like image and text, deep learning has always lagged gradient boosting in tabular data, both in popularity and performance. But, in the last few years, newer models have been explicitly created for tabular data, pushing the performance of deep learning models. However, when it comes to popularity, there still exist challenges as there aren’t ready-to-use libraries such as Sci-Kit Learn for deep learning.\xa0', 'Developed by Manu Joseph, PyTorch Tabular is a new deep learning library that makes working with deep learning and tabular data easy and fast. The library has been built on frameworks PyTorch and PyTorch Lightning, and it works on pandas data frames directly. In addition, many state-of-the-art models like NODE and TabNet have already been integrated and implemented in the library with a unified API.\xa0', 'Besides PyTorch Tabular, other availability models include:\xa0', 'According to the author, PyTorch Tabular is designed to make the standard modelling pipeline easy enough for practitioners and standard enough for production deployment, alongside its focus on customisation to enable wide usage in research. To satisfy these objectives, PyTorch Tabular has adopted a ‘config-driven’ approach. It includes five config files to drive the whole process: DataConfig, ModelConfig, TrainerConfig, OptimizerConfig, and ExperimentConfig. These config files are set programmatically and through YAML files, making it easy for data scientists and ML engineers.\xa0', 'In addition to this, PyTorch Tabular uses BaseModel. This abstract class implements the standard part of any model definition like loss and metric calculation, etc., alongside Data Module and TabularModel. PyTorch Tabular uses a Data Module to unify and standardise the data processing, and TabularModel to bring together the configs, initialise the right model, the data module, and handle the train and prediction functions with methods like ‘fit’ and ‘predict.’\xa0', 'Deep learning for tabular data is gaining popularity in the research community and the industry. With the rise in popularity, it becomes essential to have a unified and easy to use API for tabular data, similar to what sci-kit learn has done for classical machine learning algorithms.\xa0', 'PyTorch Tabular plans to reduce the entry barriers in using new SOTA deep learning model architectures and reduce the ‘engineering’ work for researchers and developers.']",
6,tool_for_data_visualization/,https://analyticsindiamag.com/a-deep-dive-into-pyecharts-a-python-tool-for-data-visualization/,"['In the context of analytics, data visualization is critical because it allows users or clients to view large amounts of data and simultaneously extract important insights that can propel the business forward. When it comes to Pythonic methods, there are several to choose from, such as Matplotlib, Seaborn, and others. However, there are relatively few that have an interactive plot in a pythonic approach. In this post, we will look into PyeCharts, a Python-based tool that can generate interactive plots in a few lines of code in a variety of formats. The major points to be discussed in this article are listed below.', 'Let’s start the discussion by understanding what data visualization is.', 'The practice of putting information into a visual context, such as a map or graph, to make it easier for the human brain to absorb and extract insights from. The primary purpose of data visualization is to make identifying patterns, trends, and outliers in huge data set easier. The phrase is frequently used interchangeably with other terms such as information graphics, information visualization, and statistical graphics.', 'Data visualization is a data science technique that asserts that after data has been collected, processed, and modelled, it must be seen in order to draw conclusions. Data visualization is part of a larger field called data presentation architecture (DPA), which aims to find, retrieve, alter, format, and transfer data as quickly as possible.', 'Data visualization is required for almost every operation. Teachers may use it to display test results, computer scientists can use it to improve artificial intelligence (AI), and executives can use it to share information with stakeholders. Large-scale data projects also necessitate it. Businesses needed a way to quickly and easily gain an overview of their data as they accumulated large volumes of data during the early years of the big data trend.\xa0', 'In this section, we will overview some of the famous python-based tools used for data visualization.', 'Matplotlib is a Python data visualization and 2-D plotting library. It is the Python community’s most popular and widely used plotting library. It comes with an interactive environment that can be used on a variety of platforms. Matplotlib is a Python library that can be used in Python scripts, the Python and IPython shells, Jupyter notebooks, web application servers, and more.\xa0', 'Tkinter, GTK+, wxPython, Qt, and other GUI toolkits can be used to embed plots into applications. So we can make plots, bar charts, pie charts, histograms, scatterplots, error charts, power spectra, stemplots, and any other visualization chart you want with Matplotlib.', 'Seaborn is a Matplotlib-based Python data visualization framework that is tightly linked with NumPy and pandas data structures. Seaborn offers a number of dataset-oriented plotting routines that work with data frames and arrays that contain entire datasets.\xa0', 'Then it executes the necessary statistical aggregation and mapping tasks on its own to build the user’s desired informative visualizations. It’s a high-level interface for making visually appealing and useful statistical visuals, which are crucial for studying and comprehending data.', 'Plotly is a graphing library that can be used to create data visualizations for free. Plotly (plotly.py) is a web-based data visualization tool that is built on top of the Plotly JavaScript library (plotly.js).\xa0', 'It can be used to create web-based data visualizations that can be displayed in Jupyter notebooks or web applications using Dash or saved as individual HTML files. Scatter plots, histograms, line charts, bar charts, pie charts, error bars, box plots, multiple axes, sparklines, dendrograms, 3-D charts, and other chart types are available in Plotly.', 'Most data visualization libraries don’t have much support for making maps or working with geographic data, which is why Geoplotlib is such a useful Python library. It facilitates the creation of geographical maps in particular, with a variety of map types such as dot-density maps, choropleths, and symbol maps available.', 'One thing to keep in mind is that installation requires NumPy and Pyglet, but this isn’t a major drawback. Especially since you want to make geographical maps, and Geoplotlib is the only excellent map-making option available.', 'In this section, we will try to plot various types of charts using Pyecharts.', 'PyeCharts is a class library that allows you to create an Echarts chart. Echarts is a Baidu open-source data visualization JS library. Echarts generates excellent graphical visual effects, and Pyecharts is docked with Python, making it easy to use the data generation map directly in Python.', 'In our Jupyter Notebook, Pyecharts work similarly to how we use visualization libraries in Python. PyeCharts offers a variety of configuration options, allowing you to quickly create the chart you want.', 'Let’s start with the library by first installing it. While I was searching for this tool, the code available in its official documentation works perfectly on a specific version of it. If you install directly without specifying the version, some of the plots may not be available.', 'All the charts that we will be creating here will get available as the HTML files in the local directory.', '! pip install pyecharts==0.5.11', 'Please note that all the charts that we will be creating here will get available as the HTML files in the local directory.', 'Here we will plot a simple Multibar chart that shows sales of cars across various manufacturers.', 'As we know the scatter plots very well, here dynamic scatter plot is nothing but we have added some extra animation with distinct points with distinct symbolic representation.\xa0', 'Here we will plot a nested version of Pie plot and which is pretty simple to plot. We just need to add further labels and values by using .add method and by specifying the radius carefully.', 'The liquid plot is one such plot which I have found only this framework, and plotting it very simple just takes 3 lines. Animation of the plot does satisfy its name.\xa0\xa0', 'You have seen various analogue meters or processing meters, PyeCharts does provide it under the Guage plot as shown below.', 'A funnel chart is used to show the chronological events, ascending or descending numbers. PyeChart example of funnel chart as shown below.\xa0', 'The line chart is used to show the information as a series of data points connected by a line. Below is an example of a line chart.', 'Polar charts are circular charts that display information as polar coordinates using values and angles. Polar charts can be used to display scientific data. Below is an example of a Polar Chart.', 'A radar chart is a two-dimensional graphical representation of multivariate data with three or more quantitative variables represented on axes that start at the same point. Below is an example of a Radar Plot.']",'! pip install pyecharts==0.5.11'
7,plotly_express_for_practitioners/,https://analyticsindiamag.com/a-guide-to-different-visualizations-with-plotly-express-for-practitioners/,"['Plotly Express is a free and open-source Python visualization library for creating interactive and beautiful visualizations. It’s a great way to find patterns in a dataset before diving into machine learning modelling. In this article, we will look at how to use it in an example-driven way, giving you an overview of Plotly Express using the built-in datasets, explaining it from the ground up, and covering all of the most commonly used charts. The following are the points and plots that this article will cover.', 'Let’s start the discussion by understanding what is Plotly Express?', 'Plotly Express is a new Python visualization library that acts as a wrapper for Plotly, exposing a simple syntax for complex charts. It was inspired by Seaborn and ggplot2 and was specifically designed to have a terse, consistent, and easy-to-learn API: with a single import, you can make richly interactive plots with faceting, maps, animations, and trendlines in a single function call.\xa0', 'Plotly Express, like Plotly.py, includes on-board datasets, colour scales, and themes, and it’s completely free with the permissive open-source MIT license, you can use it however you want (yes, even in commercial products!). Plotly Express is fully compatible with the rest of the software.', 'Plotly Express is a plotly.express module (usually imported as px) that contains functions that can create entire figures at once. Plotly Express is a part of the Plotly library that comes pre-installed and is the recommended starting point for most common figures. Every Plotly Express function returns a plotly.graph object.figure instance and uses graph objects internally.', 'Plotly Express includes over 30 functions for creating various types of figures. The API for these functions was carefully designed to be as consistent and easy to learn as possible, allowing you to easily switch from a scatter plot to a bar chart to a histogram to a sunburst chart during a data exploration session.', 'Let us have a look at how the basic plots can be created using Plotly express.', 'Scatterplots are excellent for determining whether two numerical variables have a relationship or correlation. Each data point is represented as a marker point by px.scatter, whose location is determined by the x and y columns.', 'With the trendline argument in Plotly Express, you can add an Ordinary Least Squares regression trendline to scatter plots. Marginal distribution plots, which are small subplots above or to the right of the main plot and show the distribution of data along only one dimension, can also be used. Plotly Express functions like scatter and histogram have marginal distribution plot capabilities built-in.', 'When displaying a categorical column and a numerical column, a Bar Plot is an excellent visualization. It displays the number of a specific numerical column in each category. Plotly Express makes it very simple to create one.', 'A graphical representation of a binned numerical data distribution is a histogram. The count for each bin is then displayed. Aggregation functions such as sum and average can be used to combine plotly data. The data to be binned in Plotly can also be categorical. Here’s an illustration:', 'Sunburst plots depict hierarchical data that extends radially from root to leaves. Labels and parents’ attributes define the hierarchy. Children are added to the outer rings as the root grows from the center. Each row of the DataFrame is represented as a sector of the sunburst by px.sunburst.', 'Funnel charts are frequently used to visualize data at various stages of a business process. It’s a crucial mechanism in Business Intelligence for identifying potential process flaws. It’s used to track revenue or loss in a sales process at each stage, for example, and it shows values that are decreasing over time. Each stage is represented by a percentage of the total value.', 'The 3D function px.scatter 3d plots individual data in three-dimensional space, similar to the 2D scatter plot px.scatter.', 'A scatterplot matrix is a matrix that is linked to n numerical arrays (data variables) of the same length, $X1, X2,…, X n$. The scatter plot of the variable Xi versus Xj is displayed in cell (i,j) of such a matrix.', 'To plot the scatter matrix for the columns of the data frame, use the Plotly Express function px.scatter matrix. By default, all columns are taken into account.', 'Each row of the DataFrame is represented by a polyline mark that traverses a set of parallel axes, one for each dimension,', 'Data is represented on radial and angular axes in a polar chart. Polar data can be represented as scatter markers with px.scatter polar and as lines with px.line polar in Plotly Express. The r and theta arguments of px.scatter polar are used to specify radial and angular coordinates. Theta data are categorical in the example below, but numerical data are also possible and the most common cause.']","'! pip install plotly\nimport plotly.express as px\ndf = px.data.iris()\nfig = px.scatter(df
 x=""sepal_width"", y=""sepal_length"", color=""species"")\nfig.show()\n', 'fig = px.scatter(df, x=""sepal_width"", y=""sepal_length"", color=""species"", marginal_y=""violin"",\n           marginal_x=""box"", trendline=""ols"", template=""simple_white"")\nfig.show()\n', 'import plotly.express as px\ndf = px.data.tips()\nfig = px.bar(df, x=""sex"", y=""total_bill"", color=""smoker"", barmode=""group"")\nfig.show()\n', 'import plotly.express as px\ndf = px.data.tips()\nfig = px.histogram(df, x=""total_bill"", color=""sex"")\nfig.show()\n', ""df = px.data.tips()\nfig = px.sunburst(df, path=['sex', 'day', 'time'], values='total_bill', color='time')\nfig.show()\n"", 'data = dict(\n    number=[39, 27.4, 20.6, 11, 2],\n    stage=[""Website visit"", ""Downloads"", ""Potential customers"", ""Requested price"", ""invoice sent""])\nfig = px.funnel(data, x=\'number\', y=\'stage\')\nfig.show()\n', ""df = px.data.iris()\nfig = px.scatter_3d(df, x='sepal_length', y='sepal_width', z='petal_width',\n              color='petal_length', size='petal_length', size_max=18,\n              symbol='species', opacity=0.7)\n \n# tight layout\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))"", 'df = px.data.iris()\nfig = px.scatter_matrix(df,\n    dimensions=[""sepal_width"", ""sepal_length"", ""petal_width"", ""petal_length""],\n    color=""species"")\nfig.show()\n', 'df = px.data.iris()\nfig = px.parallel_coordinates(df, color=""species_id"", labels={""species_id"": ""Species"",\n                ""sepal_width"": ""Sepal Width"", ""sepal_length"": ""Sepal Length"",\n                ""petal_width"": ""Petal Width"", ""petal_length"": ""Petal Length"", },\n                             color_continuous_scale=px.colors.diverging.Tealrose,\n                             color_continuous_midpoint=2)\nfig.show()\n', 'df = px.data.wind()\nfig = px.scatter_polar(df, r=""frequency"", theta=""direction"",\n                       color=""strength"", symbol=""strength"", size=""frequency"",\n                       color_discrete_sequence=px.colors.sequential.Plasma_r)\nfig.show()\n'"
8,use_plotly_in_colab/,https://analyticsindiamag.com/how-to-use-plotly-in-colab/,"['Plotly is now more powerful than ever with a new open source library named JupyterDash. JupyterDash is developed on top of the Dash framework to make it completely suitable for notebook environments such as Colab. The open source JupyterDash library makes the plots real-time interactive in Colab with hovers, handles, and other good controls. Changes in data or code causes immediate effect in visualizations, making Plotly a handy solution to streaming data. Furthermore, it makes Colab visualizations be displayed on a separate web page with hot reloading and input/output interactions.', 'Visualizations talk better than words! Let’s start exploring some cool and beautiful plots made using Plotly along with JupyterDash.', 'Requirements: Python 3.6 or above, Plotly 4.4.0 or above', 'It is recommended that Plotly be upgraded to its latest version using following command', '!pip install --upgrade plotly', 'JupyterDash can be installed in Colab using\xa0 the following command', '!pip install jupyter-dash', 'Plotly offers most of its attractive plotting methods with two major interfaces namely, express and graph-objects. JupyterDash works with dependency modules such as dash_core_components, dash_html_components, and dependencies class from dash library.', 'Let us begin plotting by importing the frameworks and libraries.', '\xa0\xa0\xa0 Sunburst plots in Plotly is one among the famous and interactive plots. It follows a path to burst out data in the form that looks like the solar system. It helps in identifying categories of data based on one or more features. With the famous in-built gapminder dataset, we can have a sunburst plot using the code below:', 'We can notice that the mode provided in run_server command is ‘inline’. Leaving it to default will let us open the JupyterDash visualization in a browser. If the notebook is prepared in JupyterLab, mode can be set to ‘jupyterlab’, which will open the visualization in a new tab of JupyterLab.\xa0', 'We can look at how the plot can be read or well utilized by hovering over the plot or tapping any region of the plot. If we tap on the continent region ‘Europe’, the plot will burst out of that particular ‘Europe’ family by hiding all other regions and their families. As said, we can further explore a particular country to learn how population or life_expectancy has changed over years.', 'The two plots are first-level and second-level burst-out explorations of ‘Europe’ and ‘Turkey’ respectively.\xa0', 'To know all the necessary details of a particular block in the plot, we can hover the mouse pointer over it.', 'Bubble charts of Plotly are the simple scatter plots. They are so popular than scatter plot versions of Matplotlib library or Seaborn library in such a way that they can be plotted quickly, differentiate features easily with colors and size without hassle. On top of all, these plots are more interactive. We can understand its interactivity through an example. The following codes develop a JupyterDash visualization with ‘GDP per capita’ in x-axis and ‘Population expectancy’ in y-axis. Size of bubbles are determined by ‘Population’ and the color of bubbles are determined by ‘Continent’.\xa0', 'More number of bubbles make the plot difficult to read. This is where Plotly proves its plots are interactive. By clicking or double-clicking on any one or more of legends, here ‘Continent’, we can visualize bubbles belonging to desired ‘Continent’ only. The following plot shows Asian countries alone by hiding all other continents.\xa0', 'Further, as discussed in Sunburst plots, hovering over bubbles yields more details about them.\xa0', 'These bubble charts can be explored as 3D plots with the following codes:', '3D plots can be panned, zoomed, rotated along one or more axes to get better insights of data.', 'Sankey diagrams are well suited for data which has features interacting at multiple levels. Sankey diagrams essentially have three important parameters: source, target, value. Source and target can be grouped together as nodes. Size of a node is determined by the value it holds. Sources and targets are connected through ribbon-like connectors. These nodes and ribbon-like links can be moved within the diagram to visualize the plot conveniently or to focus on certain details. The following codes generate a JupyterDash based Plotly Sankey diagram. In order to explore one of the greatest features of JupyterDash, receiving real-time inputs through plots, a slider bar is incorporated. It helps adjusting the opacity of chart by sliding the pointer left or right.']","'!pip install --upgrade plotly'
 '!pip install jupyter-dash'"
9,3. https://analyticsindiamag.com/plotly_vs_seaborn_compari/,https://analyticsindiamag.com/plotly-vs-seaborn-compari/,"['By the advancements of technology, we are generating huge amounts of data in multiple ways. The data generated from the origin of the earth to the 20th century is equal to the data generated from 2001 to 2020. It means the data generated from the past 20 years is more than ever generated. The data is useless without getting insights from it so we need to preprocess the data and need to find the trends in the data. ', 'While working with machine learning projects, 70% of the time we spend in preprocessing of the data. By using a pictorial representation of data we can understand the data quickly and easily. So some researchers created visualization tools and libraries that are very useful in preprocessing. In this article, we will demonstrate how to use Plotly and seaborn tools.', 'In this article, we will explore two popular visualization libraries in Python for data visualization – Plotly and Seaborn –\xa0 and demonstrate the following basic types of visualization for comparison:-', 'Box-plot\xa0\xa0', 'A box-plot is a visualization technique that indicates the outliers in the data and this is the standardized way of displaying our data based on outliers, Outliers are nothing but the values away from the mean. Using this Box-plot we can compare the distribution of data between different datasets. Now let’s visualize Box-plot using Plotly and seaborn.', 'Using Seaborn\xa0', 'import seaborn as sns', 'import pandas as pd', 'df = sns.load_dataset(""tips"")', 'sns.boxplot( x=df[""tip""], y=df[""sex""], palette=""Accent"");', 'plt.show()', 'Using Plotly', 'import plotly.express as px', 'df = px.data.tips()', 'fig = px.box(df, x=""day"", y=""total_bill"", color=""smoker"")', 'fig.show()', 'Bar-plot\xa0', 'Bar-plots are the most common type of plots used for visualization. It displays the relationship between the absolute value and numerical value, They are represented in rectangular blocks. For example, in the data, if you need to find which country has the highest population, by using box-plot we can quickly get insights from it.\xa0', 'Using Seaborn', 'import seaborn as sns', 'import pandas as pd', 'df = sns.load_dataset(""tips"")', 'sns.barplot(x=""sex"", y=""total_bill"", data=df)', 'Using Plotly', 'import plotly.express as px', 'df = px.data.tips()', 'fig = px.bar(df, x=""sex"", y=""total_bill"", color=\'day\')', 'fig.show()', 'Pair-plot', 'Pair plot is used to visualize the relationship in-between each variable in the dataset. In the X-axis and Y-axis, the data columns are placed, and by using multiple graphs we can get insights into the entire dataset at once. For example, let us have data on cars and we need to predict the millage using our model. Then in Exploratory Data Analysis, using pair plot we can know what are variables influencing the millage. Mostly the mileage of the car is influenced by weight, speed, fuel type. We can get this type of visualization using a pair-plot.', 'Using Seaborn', 'import seaborn as sns', 'import pandas as pd', 'df = sns.load_dataset(""tips"")', 'sns.pairplot(df)', 'Using Plotly', 'import plotly.express as px', 'df = px.data.tips()', 'px.scatter_matrix(df)', 'Conclusion']","'import seaborn as sns'
 'import pandas as pd', 'df = sns.load_dataset(""tips"")', 'sns.boxplot( x=df[""tip""], y=df[""sex""], palette=""Accent"");', 'plt.show()', 'import plotly.express as px', 'df = px.data.tips()', 'fig = px.box(df, x=""day"", y=""total_bill"", color=""smoker"")', 'fig.show()', 'import seaborn as sns', 'import pandas as pd', 'df = sns.load_dataset(""tips"")', 'sns.barplot(x=""sex"", y=""total_bill"", data=df)', 'import plotly.express as px', 'df = px.data.tips()', 'fig = px.bar(df, x=""sex"", y=""total_bill"", color=\'day\')', 'fig.show()', 'import seaborn as sns', 'import pandas as pd', 'df = sns.load_dataset(""tips"")', 'sns.pairplot(df)', 'import plotly.express as px', 'df = px.data.tips()', 'px.scatter_matrix(df)'"
10,resources_to_learn_plotly/,https://analyticsindiamag.com/10-free-resources-to-learn-plotly/,"['Data visualisation plays an integral part in our lives as it helps in drawing meaningful insights from various types of data. Plotly is one of the powerful libraries for data science, machine learning and artificial intelligence-related operations where it helps to create multiple types of interactive visualisations by using Python, R and Java.', 'In this article, we list down – in no particular order – the top 10 free resources to learn Plotly:-', 'Source: DataCamp', 'About: In this tutorial, you will understand how easily you can use Plotly to create data visualisations with R. You will learn how to use Plotly to generate heatmaps and 3D surface plots, a choropleth map, and how to add slides. You will also learn a brief introduction to ggplotly, the interactive sister of ggplot2.\xa0', 'Get the resource here.', 'Source: Official Site', 'About: This is the official documentation for the Python implementation of Dash. Dash is a productive Python framework for building web applications which are built on Flask, Plotly.js, and React.js. Dash is ideal for building data visualisation apps with highly custom user interfaces in pure Python, and it is particularly suited for anyone who works with data in Python.\xa0\xa0', 'Get the resource here.', 'Source: DataCamp', 'About: In this tutorial, you will receive an introduction to basic graphics with Plotly. You will create your first interactive graphics by displaying both univariate and bivariate distributions, learn how to customize the appearance of your graphics and use opacity, symbol, and color to clarify your message, how to transform axes, label your axes, and customize the hover information of your graphs.', 'Get the resource here.', 'Source: Coursera', 'About: This is a project-based course on data visualisation with Plotly Express where you will learn to create quick and interactive data visualisations with Plotly Express. Plotly Express is a high-level data visualisation library in Python inspired by Seaborn and ggplot2. In this course, you will explore the various features of the in-built Gapminder dataset, and produce interactive, publication-quality graphs to augment analysis.', 'Get the resource here.', 'Source: Blog', 'About: In this tutorial, you will learn all the features, bundles and chart types available in the library. You will understand how to create a basic line chart in Plotly, and how to style the chart lines using different attributes.', 'Get the resource here.', 'Source: DataCamp', 'About: Dash is a Python framework for building web applications, which is built on top of Flask, Plotly.js, React and React Js. In this tutorial, you will learn how to build dashboards in Python using Dash Plotly. You will understand how dash app layout works, and how to generate scatter plots, core components.', 'Get the resource here.', 'Source: Kaggle', 'About: In this tutorial, you will learn how to use Plotly library, how to create line plots, scatter plots, area charts, bar charts, error bars, box plots, histograms, heatmaps, subplots, multiple-axes, etc. You will also understand inset plots, 3D scatter plot with colour scaling and more.', 'Get the resource here.', 'Source: Youtube', 'About: This is a video tutorial where you can learn how to use Plotly online, offline and in Jupyter Notebooks, as well as how you can build interactive visualisations using data. You will be able to get to explore the library while creating interactive visualisations using data.\xa0\xa0', 'Get the resource here.', 'Source: Blog', 'About: In this blog, you will learn the basics of Plotly, understand the plot method, understand how to create different traces/plots like a scatter plot, or a line plot, as well as how to have a chart with multiple traces.\xa0\xa0\xa0', 'Get the resource here.', 'Source: PDF', 'About: In this cheat sheet, you will learn how to get started with Plotly and codes to create basic charts, layout, statistical charts, maps, 3D charts and figure hierarchy.\xa0']",
11,with_creme_in_python/,https://analyticsindiamag.com/how-to-learn-from-streaming-data-with-creme-in-python/,"['In a traditional paradigm of machine learning, we often work in the offline learning fashion where we start with data preprocessing and end with data modelling with an algorithm to satisfy the requirements. This becomes a storage-dependent and time-consuming process. To overcome this, we can use streaming data for predictive analysis or any other modelling process. We don’t need to store the data before modelling it. This can be accomplished by stream learning and online learning. In this article, we will understand how we can make streaming data useful in machine learning. We will also learn how to implement online machine learning to learn from streaming data. The major points that we will cover in this are listed below.', 'Table of Contents', 'What is Streaming Data?', 'The data which is generated continuously in an incremental manner from different sources can be considered as the streaming data. Basically, this type of data is generated frequently and flowing across different websites and platforms which are designed to provide information as a function of time. Usually, streaming data is used in the context of big data where it is generated using different sources at stable or unstable high speed. In the context of technology, it is a system that is used for providing information to devices over the internet by allowing users to access the content immediately instead of using the content offline or after downloading it.\xa0', 'As we know, big data mainly focuses on the storage of the data which causes a huge amount of cost for storage and the storage consists of the tendency to make the data unstructured. This is not fruitful for the machine learning algorithms. Datastream is a sequence of digitally encoded coherent signals used to transfer the data that is in the data transfer process. Datastream can be considered as set to information that is extracted from the receiver and provided by the data provider. Using various methods we can make streaming data useful for us like making reports, providing on-demand data-driven decisions,\xa0 and machine learning algorithms.', '\xa0We can extract useful information from a data stream or many data streams for modelling purposes. Since the offline machine learning models work on a trained or offline data, in the case of modelling with streaming data online machine learning comes into the picture.', 'Online Machine Learning', 'Online machine learning is a method that combines the machine learning models or predictors with the data which comes in the sequential order so that the best predictor can be used to provide updates according to the future data at every step. In a traditional machine learning program, we use stable data to predict the values for which the model is designed where on the other hand online machine learning programs are designed to adopt the new patterns in the data or adopt the data when it is generated as a function of time. Online machine learning models have their own benefits and uses like the changes and updates on the model can be done automatically in case of changes in the data and they are highly preferable models in the fields like finance market, economics where the new data is emerging on a frequent basis.\xa0', 'The above image represents the flow chart of any online machine learning system where streaming data can be used to train an online machine learning algorithm while the precision label of the model is not enough we can analyze the problem to make the algorithm perform better with the new data.', 'There are various frameworks available to perform online machine learning. A few of them are:', 'This article is mainly focused on the creme library which provides the functionality for performing online machine learning on streaming data. So let us understand creme in detail with its implementation.\xa0', 'About Creme', 'The creme is a python library for online machine learning which provides most of the packages designed especially for online machine learning. Using the library we can learn the stream of data using different approaches. This library allows models to learn one data point at a time so that updates can be done if required. This approach helps to learn from big data that isn’t stored in the main memory. The library gives integration options to online machine learning where the new data stream is constantly arriving.\xa0', 'The following benefits can be achieved using creme:', 'The following are the features of the creme library:', 'Implementing Online Learning Using Creme', 'Now, let us implement a linear model that is a logistic regression for binary classification using the creme library.', 'Installation\xa0', 'The basic requirement to work with the creme is python 3.6 or above. We can install the library using pip and the following command:', '\xa0!pip install creme', 'Now let’s start building a logistic regression model for classifying the website phishing dataset. Before going for modelling let’s check for the entities dataset consists of:', 'Importing dataset from the creme library', 'The above output represents some basic details of the phishing dataset. Let’s divide the dataset into dependent and independent variables.', 'Now we can train a logistic regression model on the data in a streaming fashion.\xa0', 'Building the Model', 'Importing the packages for data preprocessing, models, and accuracy metrics.', 'from creme import compose, linear_model, metrics, preprocessing', 'Defining a pipeline for a model instance and scaling the data.', 'Making an accuracy metrics instance:', 'Making a prediction using a model instance and sequentially updating the model we can also update an accuracy metric using the metric.update module.', 'After running the above-given code we can check for the accuracy of the model which we have updated in the loop.', 'Here we have created and trained a linear regression model by interleaving predictions and model updates.\xa0 The model has performed well and we can see the accuracy of the model is around 89% which is pretty good.', 'There are various tasks we can perform using the creme library as we have explained in the features.\xa0\xa0\xa0', 'Final words\xa0', 'In this article, we had an overview of streaming data and online machine learning. There are various application domains of online machine learning such as time series forecasting, spam filtering, recommender systems, CTR prediction, and IoT applications. The creme library provides most of the features of stream learning that are required in online learning. We can use them according to the requirements.\xa0']","'from creme import datasets\nA_b = datasets.Phishing()\nprint(A_b)'
 'from pprint import pprint\nfor A, b in A_b:\n\xa0\xa0\xa0\xa0\xa0pprint(A)\n\xa0\xa0\xa0\xa0\xa0print(""independent variable ="", b)\n\xa0\xa0\xa0\xa0\xa0break', 'lm = compose.Pipeline(\n\xa0\xa0\xa0\xa0\xa0preprocessing.StandardScaler(),\n\xa0\xa0\xa0\xa0\xa0linear_model.LogisticRegression()\n)\npprint(model)', 'metric = metrics.Accuracy()', 'for A, b in A_b:\n\xa0\xa0\xa0\xa0\xa0pred = lm.predict_one(A)\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0\xa0\xa0metric = metric.update(b, pred)\xa0\xa0\n\xa0\xa0\xa0\xa0\xa0model = lm.fit_one(A, b)\xa0\xa0\xa0', 'print(metric)'"
12,for_geospatial_data_visualization/,https://analyticsindiamag.com/a-guide-to-geopandas-for-geospatial-data-visualization/,"['In the field of data science, at the beginning of any sort of analysis or modelling, we tend to visualize the data so that we can have some insight from the problem domain. This can be achieved using various data visualization tools such as Matplotlib, Pandas, Plotly, Seaborn, Bokeh, etc. All these mentioned tools are widely used with tabular data. Similarly, to visualize geospatial data or to plot a map of any geographical location and show some of the interesting facts, we can leverage a Python library named GeoPandas. This library is exclusively used for plotting and manipulating geospatial data. In contrast to GeoPandas, we are going to discuss the following points in this article:', 'Let’s proceed with the discussions one by one.', 'Spatial data refers to any type of data that directly or indirectly refers to a certain geographical area or location. Spatial data is a numerical representation of a physical item in a geographical coordinate system, often known as geospatial data or geographic information. Geographic data, on the other hand, is considerably more than just the geographical component of a map. Users can save geographical data in a number of formats because it can contain more than just location-specific information. We can learn more about how each variable affects people, communities, and populations by analyzing this data.\xa0', 'There are various types of spatial data, but the two most common are geometric data and geographic data. Let us try two understand these two kinds of data.', 'Geometric data is a sort of spatial data that is represented on a two-dimensional flat surface. For example, geometric data is used in floor plans. Google Maps is a navigation program that uses geometric data to generate precise directions. It is, in fact, one of the most basic examples of geographical data in action.', 'Geographic data is the information that has been plotted around a sphere. Most of the time, the sphere is the planet Earth. Geographic data emphasizes the relationship between latitude and longitude to a given object or area. A Global Positioning System (GPS) is a well-known example of geographic data.', 'Geospatial data often includes vast sets of spatial data gathered from a variety of sources in various forms and might contain information such as census data, satellite imagery, meteorological data, mobile phone data, drawn images, and social media data. When geospatial data can be discovered, shared, analyzed, and used in conjunction with traditional business data, it is the most useful.', 'Geospatial analytics adds time and location granularity to standard data sets. Maps, graphs, statistics, and cartograms can all be used to depict historical and current events in various ways. This additional information helps to paint a clearer picture of what transpired. Visual patterns and images that are easy to recognize reveal insights that could otherwise be lost in a huge spreadsheet. Forecasts can be generated more quickly, conveniently, and precisely with this technique.', 'The GeoDataFrame, which extends the Pandas DataFrame, is the main data structure of GeoPandas.\xa0', 'Expected Data Format by GeoPandas', 'The GeoDataFrame can perform all of the underlying DataFrame operations. It includes one or more GeoSeries (which extend pandas Series), each with geometries in a different projection (GeoSeries.crs). Though a GeoDataFrame can have many GeoSeries columns, only one of them will be the active geometry, which means that all geometric operations will be performed on that column.\xa0', 'Visualizing Geographical Data using GeoPandas', 'Using the GeoPandas.read_file method, which recognizes the file type automatically and builds a GeoDataFrame, we can quickly read a file that contains both data and geometry (e.g., GeoPackage, GeoJSON, Shapefile).', 'Description of Shapefiles', 'Here in our case, we are using Indian district-level shapefiles. Shapefiles are nothing but another coded version of files that stores different attributes of Geospatial data in separate files. The data set we are using is taken from this Kaggle Repository, make sure to store all the files in the working directory.', 'Each item in a GeoSeries like (pandas series) is a set of shapes corresponding to a single observation. A single shape (such as a single polygon) can constitute an entry, or numerous shapes can be combined into a single observation (like the many polygons that make up the State of Maharashtra or a country India). GeoPandas has three basic classes of geometric objects (which are actually shapely objects):', 'All objects have their own physical types, such as Point for a structure, Line, Polygon, and MultiPolygon for a country with several cities. Each of them can be used for a distinct form of physical item.', 'In this implementation, we are going to plot a Map of India and then the Maharashtra state separately with a satellite view as well.', 'Load our .shp file using the attribute read_file of GeoPandas.', 'We can use the GeoDataFrame.area attribute, which returns pandas series, to find out how much area each polygon (or MultiPolygon) has. Series. GeoDataFrame.area is nothing more than GeoSeries.area applied to a geometry column that is currently active.', 'Now simply we need to call geo_data.plot(‘area’) which will return the geography of India.', ""geo_data.plot('area',figsize=(15,10))"", 'Similarly to plot the state, in the geo_data exclude all the states except you wanted one, here I’m going to plot a map of Maharashtra\xa0\xa0\xa0', 'Now we are going to plot a satellite view of the map. For that, we are using a python library named contextily that is designed to retrieve the map from the internet and further we require coordinates of the desired location which can be obtained by GeoPandas library.\xa0\xa0', 'The Coordinate Reference System (CRS) is available for each GeoSeries as GeoSeries.crs. With the help of the CRS, GeoPandas is able to determine the geometries’ coordinates anywhere on the globe. Geographic CRS refers to coordinates in latitude and longitude. Its CRS is WGS84, and the authorization code is EPSG:3857 in certain situations.']","""geo_data.plot('area'
figsize=(15,10))"""
13,for_data_preparation_eda/,https://analyticsindiamag.com/exploring-dataprep-a-python-library-for-data-preparation-eda/,"['Big Data comes along with its complications. Collecting and managing data properly and the methods used to do so play an important role. With such underlying concerns, the method of Data Preparation becomes very helpful and a crucial aspect to begin with. A good data preparation procedure allows for efficient analysis, limits and minimizes errors and inaccuracies that can occur during processing, making the processed data more accessible to all users. Lately, the process of Data Preparation has gotten easier with new tools and technologies that enable anyone to cleanse and clarify data independently. Data preparation is the process of cleaning and transforming the raw data before preprocessing and analysis. It is a small yet important step before processing and often involves reformatting the data, making corrections, and combining multiple data sets to enrich the present data. Data preparation is often considered a lengthy undertaking for data professionals or business users, but it is an essential prerequisite to put data in context to turn it into insights that might help in decision making, eliminating the bias resulting from poor data quality.\xa0', 'The data preparation process first begins with finding the right data. This can come from an existing data catalogue, warehouse or can be added ad-hoc. After the data is collected, it is important to discover and explore each dataset to prepare and process. This step is essential and helps get to know the data and understand what has to be done before the data can be called useful in a particular context. Cleaning up the data is traditionally another most time-consuming part of the data preparation process, but it’s crucial for removing forward and dealing with faulty data to help fill in the gaps present. Data cleansing is a process where you go through all of the data to be processed and either remove or update information that is considered to be incomplete, incorrect, improperly formatted, duplicated, or highly irrelevant. The Data cleansing process usually also involves cleaning up all the data compiled previously in one area. The data cleansing process is done all at once and can take quite a while if the information has been piling and stacking up for years. That’s why it’s important to perform data cleansing and taking care of the data regularly.\xa0', 'Then, data is further transformed by updating the format or value entries to reach a clean and well-defined outcome or to make the data more easily understood by a wider spectrum of audiences. Once all the mentioned processes are done, the data is prepared. This data can be stored or inculcated into a third-party application, such as a business intelligence tool, clearing the way for processing and analysis. A thorough data preparation process can give an organization many advantages and rather give it a headstart. It must be clean and free of errors before using data for analysis or plugging it into dashboards for visualizations. Preparing data for analysis will help avoid mistakes, saving more time to be invested down the line. These errors will be much more difficult to catch and fix after the data has been transferred out of its original format. Using properly cleaned and formatted data while building data models or applications will ensure top-quality reporting and analysis with proper accuracy. This eventually helps receive game-changing and revolutionary business insights.\xa0', 'DataPrep is an open-source library available for python that lets you prepare your data using a single library with only a few lines of code. DataPrep can be used to address multiple data-related problems, and the library provides numerous features through which every problem can be solved and taken care of. Using the DataPrep Library, one can collect data from multiple data sources using the dataprep.connector module, perform intense exploratory analysis using the dataprep.eda module and clean and standardize datasets using the dataprep.clean module. DataPrep automatically detects and highlights the insights present in the data, such as missing data, distinct count and statistics.\xa0 A whole detailed profile report can be created in a matter of seconds by using just a single line of code, which makes it ten times faster than other libraries to perform data preparation or EDA on.\xa0', 'In this article, we will be exploring the different functionalities of the DataPrep library for ease in Data Preparation and EDA, which will help us understand the library even better. The following implementation is partially inspired by the official DataPrep Documentation, which can be accessed using the link here.', 'To install the library, you can use the following line of code,\xa0', 'Further, from the DataPrep library itself, we can import the required dependencies for the task to be performed,', 'As we want to create different plots from our dataset, we have imported plot, plot_correlation to create correlation graphs,plot_missing to plot the number of missing data.', 'Now, Lets load our data into the data frame,\xa0', 'Using DataPrep, we can create all the possible visualizations for the data using just a single line of code. Let us plot our loaded data and see what it looks like,\xa0', 'As we can see, the library itself detects the data and plots all the necessary data graphs in a single-window itself!', 'You can also get a detailed plot for a single column with all its statistics to understand the column better,', 'We also create a dataframe, taking care of the missing values and then create a correlation plot,\xa0', 'Whether it be Pearson, Spearman or Kendall-Tau, any correlation graph can be easily plotted using the DataPrep library.', 'Let us now explore some more operations on another dataset. Here I have used the titanic dataset to perform further functional operations on using DataPrep.', 'Exploring the missing data in the dataset,\xa0', 'DataPrep will automatically analyse the data and provide the necessary graph such as a Bar Chart, Heat Map or a Dendogram.', 'Creating Word Clouds have never been easier as well, such functionalities can be used in NLP tasks which will be highly helpful.', 'To further analyse and understand what necessary steps would be needed to take on the loaded dataset, we can generate an instant report of the dataset in one go, which will provide us with all the necessary information and metrics to analyse where the focus during the data preparation stage must be, particular columns if any.', 'The detailed statistics for each column is generated with options to perform interactions between columns, check correlation or plots of missing values.', 'In this article, we understood the importance of Data Preparation in Big Data Analytics and the necessary steps required to do so. We also explored a library known as DataPrep, and tested its different functionalities that might help during the Data Preparation and EDA phase. Although there is still a lot more the DataPrep library can do, I would recommend encouraging the reader to explore further and understand the library’s immense power. The following implementations above can be found as Colab notebooks in two separate notebooks. You can access them using the links here: Notebook 1 – Titanic Dataset Notebook 2.']","'# Run the code to install DataPrep\n!pip install dataprep'
 'import pandas as pd\nfrom dataprep.eda import plot, plot_correlation, plot_missing', '#loading data in to the DataPrep dataframe\ndf = pd.read_csv(""/content/master.csv"")', 'df[""year""] = df[""year""].astype(""category"")\nplot(df)\n', 'plot(df, ""gdp_per_capita ($)"")', ""df_without_missing = df.dropna('columns')\nplot_correlation(df_without_missing)\nplot_correlation(df_without_missing, k=1)\nplot_correlation(df_without_missing, value_range=(0,1)"", '#plot missing\nplot_missing(train_df)\n', ""plot(train_df,'name' )"", '#creating a full report\ncreate_report(train_df)\n'"
14,your_data_before_training/,https://analyticsindiamag.com/tutorial-for-dataprep-a-python-library-to-prepare-your-data-before-training/,"['Preparing your data before using it to train or test the machine learning model is really important to get accurate and precise results. Preparing the data can be a tiresome task because it takes a lot of effort and time to analyze the data and prepare it according to our requirements.', 'Dataprep is an open-source python library that allows you to prepare your data and that too with just a few lines of code. By preparing data it means that we can analyze the properties of the attributes that are there in the data. In the current version of DataPrep, they have a very useful module named EDA(Exploratory Data Analysis).', 'In this article, we will explore what all we can do using DataPrep with using its features.', 'Like any other python library, we need to install DataPrep using pip install dataprep', 'DataPrep contains different functions for different operations. We will start by importing the plot function which is used to visualize the statistical plots and properties of the dataset. Also, we will be importing plotly express as we will use it to download the dataset we will be working on.', 'import plotly.express as px', 'from dataprep.eda import plot', 'In this article, we will be using the sample dataset named ‘tips’ which can be downloaded using plotly express. The dataset contains certain attributes related to hotel bills and tips.', 'df = px.data.tips()\xa0', 'df', 'We will start with statistical data exploration and analysis. The plot function is used for preparing this statistical report. This single line of code will create the whole statistical analysis.\xa0', 'plot(df)', 'As you can see in the output image above it displays the statistical properties frequency and count of all the attributes. If we click the button ‘Show Stats Info’ we will see the statistical information as shown below.', 'We can also perform statistical data analysis for individual attributes also, which will give us a clear idea about each attribute also it supports different plots like KDE Plot, Box Plot, etc.', 'plot(df, ‘tip’)', 'By clicking different plots we can visualize the ‘tip’ attribute in that plot. For example, in the image below you can see the box plot of the ‘tip’ column.\xa0', 'The next function we will be importing and using is the plot_correlation which allows us to create a heatmap of the correlations of the different attributes of the dataset. Heatmaps give us a clear view of the relationship between different attributes. Let us plot the correlation heatmap. Dataprep not only plots the heatmap it gives you three variants of it namely Pearson, Spearman, and Kendall Tau.', 'from dataprep.eda import plot_correlation', 'plot_correlation(df)', 'Dataprep allows us to visualize any missing data in our dataset, finding out missing data is mandatory while preparing the data so that we can replace it with useful data accordingly. For visualizing the missing data we will use an advertising dataset that has some missing values. You can use any dataset which contains some missing values to appreciate the visualization of the missing data.\xa0\xa0\xa0', 'This visualization can be viewed in 3 different plots as you will see in the output images below.', 'import pandas as pd', 'df1= pd.read_csv(‘Advertising.csv’)', 'from dataprep.eda import plot_missing', 'plot_missing(df)', 'These 3 images clearly help us visualize the missing data in the dataset which will help us to prepare the data by removing these missing values or replacing them with relevant data.', 'This is how we can use DataPrep to prepare our data for further processing.', 'Conclusion:']","'import plotly.express as px'
 'from dataprep.eda import plot', 'df = px.data.tips()\xa0', 'df', 'plot(df)', 'plot(df, ‘tip’)', 'from dataprep.eda import plot_correlation', 'plot_correlation(df)', 'import pandas as pd', 'df1= pd.read_csv(‘Advertising.csv’)', 'from dataprep.eda import plot_missing', 'plot_missing(df)'"
15,tabular_data_using_rtdl/,https://analyticsindiamag.com/creating-deep-learning-models-for-tabular-data-using-rtdl/,"['The development of better deep learning models in recent times and their ability to extract relevant information from various kinds of data has led to the creation of further possibilities in training the algorithms to identify decisive patterns and discover clinical findings that general practitioners would not be able to discern. More research in this field of data science has only recently started to appear. However, it has been getting lots of attention among the masses interested lately. The recent developments have led to delivering certain results that were not thought to be possible anytime before. Deep learning can be defined as a machine learning technique that teaches computers to learn by example, just like humans do. For example, deep learning has been a key technology behind driverless-self driving cars, enabling them with the power and thinking to recognize a stop sign or to distinguish between a pedestrian and a lamppost. It is also the key to many voice control technologies in consumer devices like phones, tablets and smart TVs. In Deep Learning, the created computer model learns to perform classification tasks directly from images, text, or sound data.\xa0', 'These models can help achieve state-of-the-art accuracy, sometimes even exceeding human-level performance. Models can be trained using a large set of labelled data, and its neural network architecture comprises several processing layers, which makes the network actually “Deep”. The larger the amount of labelled data, the more the recognition and classification accuracy at higher levels. For example, creating a driverless car model would require millions of images and thousands of hours of videos to train the model on and help it understand better. Deep learning requires substantially high computing power as well. Well, structured models along with high-performance GPUs would make a more efficient deep learning architecture. When combined with clusters or cloud computing, this enables one to reduce the training time for a deep learning network to hours or less. Iterations within the model are continued until the output reaches an acceptable level of accuracy.\xa0', 'Deep learning techniques help eliminate some of the data pre-processing typically involved with traditional machine learning techniques. The input and output layers present in a deep neural network are known as visible layers. The input layer is a point where the deep learning model ingests the data from to process, and the output layer is where the final prediction or classification for the given problem is made. Real-world deep learning applications are a part of our daily lives these days. In most cases, they have been so well integrated into products and services that we users are at times unaware of the complex data processing that has been taking place in the background. Overall, using automatic feature engineering and Deep Learning’s self-learning capabilities, the algorithms need little to no human intervention. This also shows and tells us about the huge potential of Deep Learning and helps brainstorm and develop more ideas.\xa0', 'RTDL (Revisiting Tabular Deep Learning) is an open-source\xa0 Python package based on implementing the paper “Revisiting Deep Learning Models for Tabular Data”. The library leverages the ease of creating a Deep Learning Model and can be used by practitioners and programmers looking to implement Deep Learning models in tabular data. It can also serve as a source of comparative baselines for researchers with other traditional libraries. Given the library’s high performance and simplicity, it can help for future works on tabular DL. It comprises a design based on Attention Transformer architectures.\xa0', 'In this article, we will implement a basic Deep Learning Model using RTDL on tabular Data and predict the RMSE score that will provide us with the variability of the prediction accuracy and relative the variance of the model. The following implementation is partially inspired by the creators of RTDL, which can be accessed using the link here.\xa0', 'To start with our model creation, we will first be installing the required libraries. The following lines can be run to do so,', 'We are also installing the libzero package here, which is a zero-overhead library for Pytorch.', 'We will now be importing our further required dependencies for the RTDL library,\xa0', 'Next, we will be loading our data to be processed. We will be using the California housing data set, readily available in sklearn, which contains housing data drawn from the 1990 U.S. Census. We will be splitting it into train and test and also preprocessing the features present.\xa0', 'We will be applying a feature transformer, which will help improve the model’s performance by reducing bias, defining relationships, removing outliers, and more.\xa0', 'Finally, let us set up the model pipeline that will help us apply the feature transformer and metrics to calculate the RMSE value.', 'We derive our final output and test the model metrics while printing the best validation epoch value to see how good our model can perform.', 'In this article, we understood what Deep Learning models are and their importance, also discussed how these models and algorithms are being used. We also explored the RTDL library and implemented a basic Deep Learning model for tabular data with it. The following implementation above can be found as a Colab notebook and accessed using the link here.']",'Epoch 001 | Validation RMSE: 0.8441 | Test RMSE: 0.5852\nEpoch 002 | Validation RMSE: 0.8252 | Test RMSE: 0.5731\nEpoch 003 | Validation RMSE: 0.8061 | Test RMSE: 0.5749 <<< BEST VALIDATION EPOCH\nEpoch 004 | Validation RMSE: 0.7962 | Test RMSE: 0.5685 <<< BEST VALIDATION EPOCH\nEpoch 005 | Validation RMSE: 0.8068 | Test RMSE: 0.5722\nEpoch 006 | Validation RMSE: 0.8254 | Test RMSE: 0.5689\nEpoch 007 | Validation RMSE: 0.7939 | Test RMSE: 0.5705 <<< BEST VALIDATION EPOCH\nEpoch 008 | Validation RMSE: 0.7914 | Test RMSE: 0.5724 <<< BEST VALIDATION EPOCH\nEpoch 009 | Validation RMSE: 0.8042 | Test RMSE: 0.5596\nEpoch 010 | Validation RMSE: 0.8531 | Test RMSE: 0.5759\nEpoch 011 | Validation RMSE: 0.7862 | Test RMSE: 0.5678 <<< BEST VALIDATION EPOCH\nEpoch 012 | Validation RMSE: 0.7942 | Test RMSE: 0.5651\nEpoch 013 | Validation RMSE: 0.7868 | Test RMSE: 0.5715\nEpoch 014 | Validation RMSE: 0.7719 | Test RMSE: 0.5812 <<< BEST VALIDATION EPOCH\nEpoch 015 | Validation RMSE: 0.7440 | Test RMSE: 0.5695 <<< BEST VALIDATION EPOCH\nEpoch 016 | Validation RMSE: 0.7833 | Test RMSE: 0.5657\nEpoch 017 | Validation RMSE: 0.8052 | Test RMSE: 0.5711\nEpoch 018 | Validation RMSE: 0.7634 | Test RMSE: 0.5750\nEpoch 019 | Validation RMSE: 0.7330 | Test RMSE: 0.5661 <<< BEST VALIDATION EPOCH\nEpoch 020 | Validation RMSE: 0.7520 | Test RMSE: 0.5582\nEpoch 021 | Validation RMSE: 0.8038 | Test RMSE: 0.5611\nEpoch 022 | Validation RMSE: 0.7813 | Test RMSE: 0.5636\nEpoch 023 | Validation RMSE: 0.7614 | Test RMSE: 0.5764\nEpoch 024 | Validation RMSE: 0.7748 | Test RMSE: 0.5704\nEpoch 025 | Validation RMSE: 0.7430 | Test RMSE: 0.5589\nEpoch 026 | Validation RMSE: 0.7686 | Test RMSE: 0.5487\nEpoch 027 | Validation RMSE: 0.7350 | Test RMSE: 0.5523\nEpoch 028 | Validation RMSE: 0.7862 | Test RMSE: 0.5596\nEpoch 029 | Validation RMSE: 0.7472 | Test RMSE: 0.5727\nEpoch 030 | Validation RMSE: 0.7427 | Test RMSE: 0.5603\nEpoch 031 | Validation RMSE: 0.7618 | Test RMSE: 0.5583\nEpoch 032 | Validation RMSE: 0.7394 | Test RMSE: 0.5573\nEpoch 033 | Validation RMSE: 0.7671 | Test RMSE: 0.5607\nEpoch 034 | Validation RMSE: 0.7604 | Test RMSE: 0.5633\nEpoch 035 | Validation RMSE: 0.7439 | Test RMSE: 0.5540\nEpoch 036 | Validation RMSE: 0.7596 | Test RMSE: 0.5533\nEpoch 037 | Validation RMSE: 0.7731 | Test RMSE: 0.5621\nEpoch 038 | Validation RMSE: 0.7589 | Test RMSE: 0.5584\nEpoch 039 | Validation RMSE: 0.7883 | Test RMSE: 0.5617\nEpoch 040 | Validation RMSE: 0.7690 | Test RMSE: 0.5644\nEpoch 041 | Validation RMSE: 0.7461 | Test RMSE: 0.5623\nEpoch 042 | Validation RMSE: 0.7671 | Test RMSE: 0.5659\nEpoch 043 | Validation RMSE: 0.7668 | Test RMSE: 0.5668\nEpoch 044 | Validation RMSE: 0.7702 | Test RMSE: 0.5544\nEpoch 045 | Validation RMSE: 0.7772 | Test RMSE: 0.5570\nEpoch 046 | Validation RMSE: 0.7692 | Test RMSE: 0.5698\nEpoch 047 | Validation RMSE: 0.7707 | Test RMSE: 0.5696\nEpoch 048 | Validation RMSE: 0.7631 | Test RMSE: 0.5704\nEpoch 049 | Validation RMSE: 0.7253 | Test RMSE: 0.5638 <<< BEST VALIDATION EPOCH\nEpoch 050 | Validation RMSE: 0.7693 | Test RMSE: 0.5623\n'
16,for_exploratory_data_analysiseda/,https://analyticsindiamag.com/guide-to-mito-a-low-code-tool-for-exploratory-data-analysiseda/,"['In data science, exploratory data analysis(EDA) is one of the most important processes that need to be done before modeling or any other processes regarding data usage. There are various fields instead of data science like business analytics and graph representations where the EDA is required. This enables us to find insights and patterns present inside the data which we can’t understand by seeing the data in raw formats like CSV, excel and pandas data frames.\xa0', 'EDA itself is a combination of a lot of processes. We go through many processes like validating the data, finding out the null values present inside the data, making graphs using data to understand the patterns, finding dependency of the data, extracting features from the data etc. Hence, as we go deeper, we know details about the data, and it takes a lot of time to complete all the EDA processes. There are various libraries present in python to complete those tasks. Also, several tools like tableau, Microsoft BI etc., are available to visualize the data, which helps us in EDA.', 'Mito is an open-source python library. For creating interactive dashboards and graphs with the help of python-like matplotlib, seaborn, plotly, geoplotlib etc. using Mito, we can also complete most of the steps of our EDA. But the thing which makes it different from all these libraries is it helps us to go through EDA faster without having so much knowledge about the coding, unlike some other libraries. So in this article, we are going to explore the features and functionalities of Mito.', 'Let’s start with the installation of the library in jupyter lab.', 'Before installing Mito, we need to satisfy the requirements of the library. The only requirement of the library is Python 3.6 or above.', 'We can check the version from the command prompt or anaconda prompt using the following command.', 'If you have the above version, you don’t have to worry about the requirement. And if you are having lower than python 3.6, you can upgrade it by using the following command.', 'Installing Mito:', 'We can download the Mito installer using the following command in the command prompt.', 'Now we can install the Mito library using the command prompt and the Mito installer which we had downloaded from this command:', 'Finally, we can launch the jupyter lab:', 'If there is any other jupyterlab notebook that is already open, you need to refresh or restart the notebook. After launching the jupyterlab we are ready to make our new mitosheet. Also, Mito provides its own free hosted version to practice and make visualisations of Mito where data sets like airport-pets and Zipcode-data are already present. You can click on this link for the Mito free hosted version.', 'In this article, we are using Mito locally.', 'After the installation, we can start the Mito sheet by using this command.', 'Input:', 'We can directly import our data using the import button present in the upper left corner on the second position.', 'For basic graph work, I am importing the alcohol dataset, a time series dataset and having the sales value of the data with their date value.', 'For visualisation, we need to change the Date value to the datetime value. Then, as we click on the column, we will have the hover on the right side of the window to select the data type we want to proceed with.', 'From here, we can select our required data type.', 'Above we have got some options shown in the image below.', 'Using the graph option.', 'After selecting the graph option on the right side of the screen, we will have this option from where we can select the chart type, and by providing the access, we can easily make a graph faster and interactive.', 'Here I have generated a scatter plot of our time series, and we can see the visualization of our data; also, if you are using IoT on your computer and generating the graph, the graph will be a live graph where you can see the value for a particular value by moving your cursor. Using Mito we can also perform most of the functionality similar to the excel and google sheets. Next, I am performing the addition function in the UNITS column where I am adding 1 to every UNIT value.', 'Here we can see how simple the task was. I just need to click on any data point and define the task. We can also pass the data using the pandas data frame.\xa0', 'Input:', 'Also, we can see that we can pivot, merge and download and save the data from the reports using Mito. Here you can see all the functionality and formulas supported by Mito. Also, you can see the functionality and formula from the doc option given in the right corner of the page.', 'Here we can see how easy it becomes to perform EDA with the Mito package. Also, if we want to learn or see the code part in the next code cell, you will see all the codes for every step you performed.', 'We can know how we performed all these steps using pandas and the NumPy library from this cell. Also, we can copy paste and perform them separately for manipulating and visualizing the data using python.']","'python --version'
 'conda update python', 'python -m pip install mitoinstaller', 'python -m mitoinstaller install', 'python -m jupyter lab', '\xa0# Run command to create a Mito sheet\n\nimport mitosheet\n\nmitosheet.sheet()', ""import pandas as pd\n\n# initialize list of lists\n\ndata = [['tom', 10], ['nick', 15], ['juli', 14]]\n\n# Create the pandas DataFrame\n\ndf = pd.DataFrame(data, columns = ['Name', 'Age'])\n\nmitosheet.sheet(df)"""
17,modern_data_augmentation_library/,https://analyticsindiamag.com/complete-guide-to-augly-a-modern-data-augmentation-library/,"['The recent advances in Deep Learning and Deep Learning models have been largely connected to the quantity and diversity of data gathered. Data augmentation enables practitioners to significantly increase the diversity of data available for training the models without actually collecting new data. Data augmentation is a technique that helps to create new training data from the existing training data artificially. This is done by applying specific techniques to training data to create new and different training examples. The most commonly used Data augmentation techniques to train large neural networks are cropping, padding, and horizontal flipping. Most approaches used in training neural networks only use basic types of augmentation. Although neural network architectures have been investigated in-depth, strong types of data augmentation and data augmentation policies that capture data invariances are yet to be discovered.', 'During training, even after using a proper model, we do not get satisfactory results. In the end, it all comes to the data used to train the network. Having a large dataset is crucial for the performance of a deep learning model. Lack of quantity and diversity of data thereby hampers the model performance. Data Augmentation helps us increase the size of the dataset and introduce variability in the dataset. During Data Augmentation, the neural network treats each data as a distinct data point. Data Augmentation also helps reduce the problem of overfitting.\xa0', 'Our dataset may have images taken in a limited set of conditions, but we might fall short in various conditions; the modified/augmented data helps deal with such scenarios. We only need to make minor alterations to our existing training data to get more data, especially when talking about Image Data Augmentation. The alterations might include flipping the image horizontally, vertically, padding, cropping, rotating, scaling and few other translations of the input image. For machine learning and deep learning models, collecting and labelling data can be exhausting and costly. Transformations in datasets by using data augmentation techniques allow companies to reduce these extra and unwanted operational costs. Companies also need to build evaluation systems for the quality of augmented datasets. As the use of data augmentation methods increases, assessing the quality of their output will be highly required.', '\xa0A more ambitious data augmentation technique consists of leveraging segmentation annotations, either obtained manually or from an automatic segmentation system, and creating new images with objects placed at various positions in existing scenes.\xa0', 'Image Source', 'What is AugLy?', 'AugLy, a new open source Python library that helps AI researchers to make use and create data augmentations to improve the robustness of their created machine learning models. Augmentations can include a wide variety of modifications to a dataset, ranging from cropping a photo to changing the pitch of a voice recording. In addition, AugLy provides sophisticated data augmentation tools to create samples to train and test different systems. AugLy currently supports four modalities, namely audio, image text and video and over 100 other augmentations. Each modality’s augmentations are contained within its sub-library. These sub-libraries can help perform both function-based, and class-based transforms, composition operators and are also available with an option to provide metadata about the transformation applied.', 'AugLy is a great library to utilize for augmenting data during model training.It was designed to include and avail many specific data augmentations that users perform in real life on internet platforms like Facebook, for example, turning an image into a meme, overlaying text and emojis on images and videos or reposting a screenshot from social media. It has four sub-libraries, each corresponding to a different modality, following the same interface. AugLy can also generate useful metadata to help you understand how your data was transformed.', 'Image Source', 'This article will try to explore the different augmentation services available in the AugLy library developed by Facebook’s AI Research Team. We will perform augmentations on image and video data, making use of the components from the library. The following implementation is partially inspired by the creators of AugLy, whose GitHub repository can be accessed using the link here.\xa0', 'We will first perform image augmentation using Augly. To do so, we will first have to install the required library for image-based augmentation. It can be done using the following code,', 'Now that our library is installed, we will import some further dependencies,\xa0', 'Now we will be setting the image path and resizing it so that the image, when displayed, does not take up the whole screen where we cannot see the changes.', 'Let’s perform some augmentations and explore what the library can offer,', 'Just Like Images, similar Augmentations can also be performed on video data. To do so, we’ll follow the same process. First, install the video augmentation library and its dependencies, then we’ll use different sets of codes to perform augmentation on the input video.', 'Let us now perform different types of augmentations on the input video,']","'!pip install -U augly\n!sudo apt-get install python3-magic'
 '\n# Image Desizing\nmeta = []\naug = imaugs.PerspectiveTransform(sigma=20.0)\ndisplay(aug(input_img, metadata=meta))\nmeta', '#Changing Aspect Ratio\nmeta = []\naug = imaugs.RandomAspectRatio()\ndisplay(aug(input_img))\nmeta', '# Applying several transformations together to create a new image\naug = imaugs.Compose(\n    [\n        imaugs.Saturation(factor=0.7),\n        imaugs.OverlayOntoScreenshot(\n            template_filepath=os.path.join(\n                utils.SCREENSHOT_TEMPLATES_DIR, ""mobile.png""\n            ),\n        ),\n        imaugs.Scale(factor=0.9),\n    ]\n)\ndisplay(aug(input_img))', '#instaling the AugLy Video Library\n \n!pip install -U augly[av]\n!sudo apt-get install python3-magic\n\n# Installing ffmpeg for the video module of augly\n!sudo add-apt-repository ppa:jonathonf/ffmpeg-4\n!apt install ffmpeg\n\n#setting video hyperparameters\n \nfrom IPython.display import display, HTML\nfrom base64 import b64encode\n \ndef display_video(path):\n  mp4 = open(path,\'rb\').read()\n  data_url = ""data:video/mp4;base64,"" + b64encode(mp4).decode()\n  display(\n    HTML(\n      """"""\n          <video width=400 controls>\n                <source src=""%s"" type=""video/mp4"">\n          </video>\n      """""" % data_url\n    )\n  )\n \nimport os\nimport augly.utils as utils\nimport augly.video as vidaugs\n \n# Set video path and trim to first 3 seconds\ninput_video = os.path.join(\n    utils.TEST_URI, ""video"", ""inputs"", ""input_1.mp4""\n)\ninput_vid_path = ""/tmp/in_video.mp4""\nout_vid_path = ""/tmp/aug_video.mp4""\n \n# We can use the AugLy trim augmentation, and save the trimmed video\nvidaugs.trim(input_video, output_path=input_vid_path, start=0, end=3)\ndisplay_video(input_vid_path)', '# Overlaying the video with random text\nvidaugs.overlay_text(input_vid_path, out_vid_path)\ndisplay_video(out_vid_path)', '#code for looping video repeatedly\nmeta = []\nvidaugs.loop(\n    input_vid_path,\n    out_vid_path,\n    num_loops=1,\n    metadata=meta,\n)\ndisplay_video(out_vid_path)\nmeta\n\n#displaying randomly generated emoji in video \nmeta = []\naug = vidaugs.RandomEmojiOverlay()\naug(input_vid_path, out_vid_path, metadata=meta)\ndisplay_video(out_vid_path)\nmeta', '# Degrading image quaity using blur\naug = vidaugs.Compose(\n    [\n        vidaugs.AddNoise(),\n        vidaugs.Blur(sigma=5.0),\n        vidaugs.OverlayDots(),\n    ]\n)\naug(input_vid_path, out_vid_path)\ndisplay_video(out_vid_path)'"
18,newly_open_sourced_library/,https://analyticsindiamag.com/the-good-the-bad-and-the-augly-behind-facebooks-newly-open-sourced-library/,"['If there is anything to be learnt from machine learning (ML), it is that data is critical. However, when developers do not have data to build their machine learning models, augmentation comes to the rescue. Data Augmentation is the practice of creating new, synthetic data from the already available data. The technique can be applied to any form of data, and the result is similar to the actual data available.\xa0', 'In a recent blog post, Facebook’s AI Research team announced open-sourcing a new Python library, AugLy, developed at Facebook’s Seattle and Paris offices. The social media giant will provide sophisticated data augmentation tools to AI researchers to evaluate and build their ML models.\xa0', 'The library offers more than 100 data augmentations that focus on modifications done to images and videos on various social media platforms, like Facebook and Instagram. It includes features such as cropping, overlaying meme-style text, emoji and screenshot transformations.\xa0', 'Source: Facebook AI', 'The pictures have fairly innocuous images and overlaid text—individually. The text is such that it would be considered a compliment if it were presented on its own. However, the memes above present the images and text together, thus generating context that individuals will understand as unfriendly, but a machine might not. To combat this, AugLy combines different modalities such as audio, video, image and text, which helps algorithms better understand and deal with complex content.\xa0', 'As per Facebook, many of the augmentations in AugLy are informed in ways in which users have earlier tried to evade the social media giant’s automatic systems. Thus, making AugLy specifically useful for models and data related to social media applications.\xa0', 'How does this work?', 'Source: Facebook AI', 'For this project, Facebook aggregated multiple augmentations from many libraries—some of which Facebook wrote for this purpose itself. One of Facebook’s augmentations takes images or videos and overlays them on a social media interface to make it seem like the image or video in question was reshared by a user after being screenshotted. Given how commonplace it is to take screenshots and share such media across apps such as Instagram or Facebook, working on libraries like these help AI systems understand that the content is still the same regardless of any distracting interface elements.', 'AugLy comprises four sub-libraries, each of which corresponds to a different modality. For each library, Facebook provides transforms in function-based and class-based formats. AugLy also uses intensity functions that help users understand the intensity of any transformations based on given parameters. Finally, AugLy can also create metadata to help understand how one may have transformed the data.', 'Source: Facebook AI', 'Data Augmentations are necessary to maintain the strength of AI models. Teaching models to be resilient to unimportant data attributes will allow them to focus on more essential data characteristics for a particular use case.\xa0', 'For instance, Facebook used AI to detect COVID-19 misinformation and exploitative content using a neural net-based model, SimSearchNet. Facebook AI built the model specifically to detect near-exact duplicates and was trained using AugLy data augmentations. Such models allow Facebook to see such misinformation even if it reappears in slightly different forms, such as using a slightly modified image or a new filter or overlaid text.\xa0', 'Facebook also used the AugLy library to test the strength of other models on a set of augmentations. In 2019, Facebook held a Deepfake Detection Challenge designed to look at progress in deepfake detection technology. For this, Facebook created and shared a dataset containing more than 100,000 videos and had experts come in and benchmark their deepfake detection models against them. AugLy was employed to evaluate the robustness of these deepfake detection models in the challenge and influenced the choice for the top five winners.', 'Summing up\xa0']",
19,for_handling_audio_files/,https://analyticsindiamag.com/hands-on-guide-to-librosa-for-handling-audio-files/,"['Every beginner in data science goes through simple projects like diabetes prediction, Market basket analysis, and some basic regression problems. All these projects we have carried out are based on structured data and that too in the tabular format. But this will not be the situation every time; the data in real-time is much more complex. So you have first to collect it from various sources, understand it and arrange it in a format that is ready for processing.', 'This way is even more difficult when data is in an unstructured format such as Audio, Images, Text, and so on; this is because you can not represent these data in its raw format, which is really hard to interpret; therefore, standard representation way has to be adopted for the better understanding of these data.\xa0', 'By the facts, a very large amount of unstructured data represents huge under-exploited opportunities. For example, if you look at our daily communication, you get what the person wants to say or convey and easily interpret their attitude towards you. So, in short, unstructured data is complex, but with the right tools and proper techniques, we can easily get those insights.\xa0\xa0\xa0', 'Today, in this article, we will talk about handling audio files, different representation techniques, and decomposing files.', 'The first step is to load the file into the machine to be readable by them. At this step, we simply take values after every specific time step. For example, for a 30 seconds audio file, we extract values for the 10th second this is called sampling and the rate at which these samples are collected is called the sampling rate.', 'Proceeding further, below practical implementation shows how to load audio files, how to represent the files in time and frequency domain using various plots, how to do feature extraction like chromogram and tempogram, and lastly, we will see how to do the remix.\xa0', 'Here we are loading single-channel and two-channel audio files from the librosa audio sample. Single referred as mono in signal domain contains only one channel, and other is stereo which contains multiple channels. You can differentiate between these files easily with your headset; if you hear sound simultaneously at both headsets, it is a single channel file; otherwise, it is \xa0 multi-channel.\xa0', 'Below I loaded both types of files; you can check the effect.\xa0\xa0\xa0\xa0', 'You can load your own file by specifying the path inside librosa.load()', 'Let’s check some basic information about file,\xa0', 'The sampling rate is nothing but samples taken per second, and by default, librosa samples the file at a sampling rate of 22050; you can override it by your desired sampling rate. Take the product of sampling rate and length of the file you will get the total number of samples.\xa0\xa0', 'If you are dealing with large audio files, this sampling rate is a more concerning factor as it adds a significant amount of load time. For this case, you can define resample type inside load function as like below;', 'Wave Plots are used to plot the natural waveform of an audio file for time, and ideally, it is sinusoidal. Here I have plotted the wave plot for both mono and stereotype of the same audio file.', 'Look closer to the waveform;', 'The spectrogram is a visual representation of a spectrum of different frequencies for time. Here we are plotting the spectrogram for linear frequencies and log frequencies. The use of these two types of spectrogram completely relies on matters of interest. If you are more interested in higher frequencies than lower ones, you should go with a linear frequencies plot.\xa0', 'From the above two spectrograms, we can easily say that the file contains the most information at lower frequencies; both graphs contain the same information. Therefore, to carry out further analysis, we should use a log frequency plot.\xa0', 'Chromagram closely relates to the twelve different pitch classes; these features are powerful tools for analysing music whose pitches can be meaningfully categorized often into twelve categories. One main property of chroma features is capturing harmonic and melodic characteristics of music while being an extensive change in the timbre and instrumentation.\xa0\xa0\xa0', 'It is a time pulse representation of an audio signal laid out such that it indicates the variation of pulse strength over time, given a specific time lag or BPM value. The construction of a tempogram can be divided into two parts. First, the onset detection characterizes a series of musical events constituting the basic rhythmic content of the audio. This is followed by the estimation of local tempo using the autocorrelation or Fourier transform of the onset detection function computed over a short time window.\xa0\xa0\xa0', 'For further information regarding these kinds of terminologies, you can refer to this paper.\xa0', 'This section will show how to separate the frequencies component like harmonic and percussive component from an audio file, later how to manipulate the audio stream like slowing and fasting the tempo; lastly, we will see how to sonify the click sound with the audio file.', 'By listening to the above files, we can easily differentiate between these frequency components and the tempo effect.\xa0', 'This was all about starting with librosa, where we have seen from loading files to plotting different graphs and manipulating the files. The use of audio features is completely dependent on the type of problem you are dealing with. For example, you are dealing with detecting pitches present in the audio file; in this case, chromogram should be the choice for tasks like these; each feature has a different role and helps to get insights. Here you can see the use of spectrogram to recognize keywords present in the audio clips.\xa0\xa0']","'! pip install librosa\n! pip install mir_eval\nimport librosa\nimport librosa.display as dsp\nimport mir_eval\nfrom IPython.display import Audio\nimport matplotlib.pyplot as plt\nimport numpy as np'
 '## mono file\ndata1, sample_rate = librosa.load(librosa.util.example_audio_file(),duration=60)\nAudio(data=data1,rate=sample_rate)', '## stereo file\ndata2, sample_rate = librosa.load(librosa.util.example_audio_file(), mono=False,duration=60)\nAudio(data=data2,rate=sample_rate)', ""print('Total number of samples: ',data1.shape[0])\nprint('Sample rate: ',sample_rate)\nprint('Lenngth of file in seconds: ',librosa.get_duration(data1))"", ""start = time.clock()\ndata, rate = librosa.load('/content/Electronic-house-background-music-118-bpm.mp3',res_type='kaiser_fast')\nafter = time.clock()-start\nprint('Time taken to load file before and after applying resampling type resp: ',before, after)"", 'plt.plot(data1)\nplt.xlim([2000,3000])', ""fig, ax = plt.subplots(nrows=2, sharex=True,figsize=(10,7))\nlibrosa.display.waveshow(data1, sr=sample_rate, ax=ax[0])\nax[0].set(title='Envelope view, mono')\nax[0].label_outer()\nlibrosa.display.waveshow(data2, sr=sample_rate, ax=ax[1])\nax[1].set(title='Envelope view, stereo')\nax[1].label_outer()"", ""d = librosa.stft(data1)\nD = librosa.amplitude_to_db(np.abs(d),ref=np.max)\nfig,ax = plt.subplots(2,1,sharex=True,figsize=(10,10))\nimg = dsp.specshow(D, y_axis='linear', x_axis='s',sr=sample_rate,ax=ax[0])\nax[0].set(title='Linear frequency power spectrogram')\nax[0].label_outer()\ndsp.specshow(D,y_axis='log',x_axis='s',sr=sample_rate,ax=ax[1])\nax[1].set(title='Log frequency power spectrogram')\nax[1].label_outer()\nfig.colorbar(img, ax=ax, format='%+2.f dB')"", ""C = np.abs(librosa.stft(data1))\nchroma = librosa.feature.chroma_stft(S=C, sr=sr)\nfig, ax = plt.subplots(figsize=(10,6))\nimg = librosa.display.specshow(chroma, y_axis='chroma', x_axis='s', ax=ax)\nfig.colorbar(img, ax=ax)\nax.set(title='Chromagram')"", ""oenv = librosa.onset.onset_strength(y=data1, sr=sample_rate)\ntempogram = librosa.feature.tempogram(onset_envelope=oenv, sr=sample_rate)\n# Compute global onset autocorrelation\nac_global = librosa.autocorrelate(oenv, max_size=tempogram.shape[0])\nac_global = librosa.util.normalize(ac_global)\n# Estimate the global tempo for display purposes\ntempo = librosa.beat.tempo(onset_envelope=oenv, sr=sample_rate)[0]\nfig, ax = plt.subplots(nrows=2, figsize=(10, 8))\ntimes = librosa.times_like(oenv, sr=sample_rate)\nax[0].plot(times, oenv, label='Onset strength')\nax[0].label_outer()\nax[0].legend(frameon=True)\nlibrosa.display.specshow(tempogram, sr=sample_rate,x_axis='s',\xa0\n\xa0\xa0\xa0\xa0\xa0y_axis='tempo', cmap='magma',ax=ax[1])\nax[1].axhline(tempo, color='g', linestyle='--', alpha=1,\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0label='Estimated tempo={:g}'.format(tempo))\nax[1].legend(loc='upper right')\nax[1].set(title='Tempogram')"", ""files=librosa.ex('brahms')\ny,sr = librosa.load(files)\n# separate components\ny_harmonic, y_percussive = librosa.effects.hpss(y)"", '# Original file\nAudio(y,rate=sr)', '# harmonic component\nAudio(y_harmonic,rate=sr)', '# percussive component\nAudio(y_percussive,rate=sr)', '# slowing the tempo\ny_slow = librosa.effects.time_stretch(y, 0.7)\nAudio(y_slow,rate=sr)', '# remixing\ntempo, beats = librosa.beat.beat_track(y=y_slow, sr=sr)\nbeat_times = librosa.frames_to_time(beats)\ny_tone = mir_eval.sonify.clicks(beat_times,sr,length=len(y_slow))\nAudio(y_slow+y_tone,rate=sr)'"
20,package_for_excel_integration/,https://analyticsindiamag.com/a-guide-to-pyxll-jupyter-package-for-excel-integration/,"['Last year, PyXLL released its PyXLL-Jupyter plugin. The new extension combines the ease of use of Excel with the interactivity of Jupyter. Once installed, you will be able to plot your data in Jupyter and have the ability to share them with Excel seamlessly. It can also open on Jupyter Notebook in a browser as well as in an Excel task pane.\xa0', 'Excel comes with pivot tables, graphing tools, and a macro programming language known as Visual Basic for Applications (VBA). Jupyter Notebook, on the other hand, is an open-source web application that allows users to create and share documents that contain live code, visualisations, equations and narrative text. With the PyXLL-Jupyter package, you can use both together, side by side.\xa0', 'VBA is commonly used to automate Excel with macros, add new user-defined worksheet functions (UDFs) and react to Excel events. The same is achieved with Python, as it is much faster.\xa0', 'Founded in 2010, PyXLL is the brainchild of Tony Roberts. He started PyXLL after years of working with Python and Excel in the finance industry. PyXLL is one of the go-to tools for writing Excel add-ins in Python, especially for people working in data science roles.\xa0', 'xiwings, DataNitro, Pandas, NumPy, Anaconda, SciPy, PySpark and DataForm, are some of the alternatives to PyXLL.\xa0', 'Firstly, to run Python code in Excel, you need the PyXLL add-in. ‘PyXLL add-in’ lets you integrate Python into Excel and use Python instead of VBA. To set up ‘PyXLL Excel add-in (pip install pyxll) and then use the PyXLL command line tool to install the Excel add-in:\xa0', '> pip install pyxll', '> pyxll install\xa0', 'Beginners can check out PyXLL’s online documentation here.\xa0', 'Once you have successfully installed the PyXLL Excell add-in, the next step is to install the pyxll-jupyter package. This package essentially glues PyXLL and Jupyter so that you can use Jupyter Notebooks within Excel.\xa0', 'The PyXLL-Jupyter package is installed using pip:\xa0', '> pip install pyxll-jupyter\xa0', 'Once you have installed both the PyXLL Excell add-in and the PyXLL-Jupyter package, you will see a new ‘Jupyter’ tab in the PyXLL dashboard, as highlighted in the image below.', 'Upon clicking this button, Jupyter Notebook opens in a side panel in your Excel workbook. This new panel is part of the Excel Interface and can be dragged to different locations at your convenience.\xa0', 'In the Jupyter panel, you can select an existing notebook or create a new one. If you want to create a new notebook, select the ‘New’ tab followed by ‘Python 3,’ as shown in the image below.\xa0', 'Further, to upgrade to the latest version of PyXLL-Jupyter, you can run pip install –upgrade pyxll-jupyter.\xa0', 'Now that you have a complete Jupyter Notebook running inside of Excel, you can use Excel for working with your data and use Python to work on the same data. In other words, you can use Excel as an interactive playground for organising and visualising your data, effortlessly switching to Python for more sophisticated tools.\xa0', 'Also, you can use Jupyter Notebooks as a scratch-pad for trying out Python code and write Excel functions entirely in Python in a Jupyter notebook and test them out in real-time.\xa0', '“Once you have developed a useful re-usable function, add it to your PyXLL Python project. That way, you can use the same function every time you see Excel,” wrote Tony Roberts, the founder of PyXLL.\xa0', 'The key highlight of PyXLL-Jupyter include:', 'Python makes a powerful alternative to VBA. Using PyXLL, you can write fully featured Excel add-ins entirely in Python. At the same time, Excel is also a brilliant tool for interactive computation, and adding Python and Jyupter takes Excel to the next level.\xa0']",
21,exploratory_data_analysis_libraries/,https://analyticsindiamag.com/insane-exploratory-data-analysis-libraries/,"['For one to perform EDA on any dataset he/she must be well versed with some of the python visualization libraries such as seaborn, matplotlib, plotly etc. to make attractive graphs so as to find the insights of the data. Finding insights into any data is a preliminary step of any data science, machine learning project as the corresponding step that is feature selection depends on the results derived from EDA. This means EDA plays a crucial role in determining the accuracy of any data science, machine learning projects.', 'In this blog, we shall find easier ways of performing EDA on any dataset by using some automated libraries.', 'The first step is to install the library by running the command', '!pip install dtale', 'in the anaconda prompt or in the console itself. Once dtale library is being installed import the common operational libraries and the titanic dataset from seaborn by using the .load_dataset.', 'Once the dataset is loaded, embed the dataset into dtale using dtale.show(df), this will show the data frame in dtale window. On clicking the dropdown button various operations such as ranging from finding Pearson’s correlation between entities to plotting 3d,2d graphs and finding outliers in the data, every EDA function can be performed.', 'One amazing feature of this library is that the source code of the desired operation can be copied from the code export option which is available. For example, the code behind the Pearson’s correlation can be found by clicking on the <>Code Export button.', 'Pandas profiling is another amazing automated library that can perform EDA but its working is limited as its performance and operations compared to dtale is much lesser. To install this library run the command', '!pip install pandas-profiling', 'either in the console or in the anaconda prompt. Once the installation is done the following code needs to execute.', 'The tips dataset is being loaded from seaborn and the columns of the dataset are shown above.', 'Here, we are importing ProfileReport from the installed pandas profiling library and saving the output as an Html file. The Html file gets saved in the environment directory. The output looks like as shown below and various EDA operations can be performed by navigating through the options that are available.', 'Sweetviz is also a handy automated EDA library, here we again load the titanic dataset from seaborn, before that sweetviz library needs to install this can be done by running the', '!pip install sweetviz', 'command in the console or in the anaconda prompt.', 'On running the two lines of code the HTML page pops up naming output_report.', 'The ouput_report.html is shown below, here again, similar to other automated EDA libraries this too has the competence to perform high-level EDA.', 'Similar to other libraries we need to first install them', '!pip install autoviz', 'and run the following codes, here we are using the titanic dataset for performing EDA.', 'from autoviz, Autoviz_class is being imported and it’s being initialized using the object AV.', 'Since I’m loading the dataset locally I have assigned it to filename else it can also be loaded from seaborn. On running this code a series of basic EDA charts are formed.']",
22,python_module_for_excel/,https://analyticsindiamag.com/guide-to-openpyxl-a-python-module-for-excel/,"['OpenPyXL is a Python module for interacting with Excel(.xlxs) files. But can’t you do that with Pandas? Yes, you can, but Pandas does support charts, formulae or images. OpenPyXL enables data scientists and data analysts to perform all kinds of operations on Excel workbooks:', '\xa0Another notable thing about OpenPyXL is that it doesn’t require an install of Microsoft Excel.\xa0', 'Excel files that we will work with are called workbooks; these contain a minimum of one sheet and a maximum of tens of sheets. Each of the sheets consists of rows starting from 1 and columns starting from A. The intersections of these rows and columns form cells that can store data of any type.\xa0', 'Install OpenPyXL form PyPI:', 'You can either read an existing .xlxs file using the load_workbook() method or create a new Workbook object. Let’s start by creating a new workbook:\xa0', 'New Workbooks are always created with at least one sheet; you can access the current sheet using the active property:', 'You can also access the worksheets using the dictionary index, workbook[“sheet_name”], format. The name of a sheet can be changed using the title property:', 'You can create additional sheets using OpenPyXL’s create_sheet() method which takes two optional arguments: the name of the sheet and its index:', 'All the sheets in the Workbook can be listed using the sheetnames property:\xa0', 'When a new worksheet is created in OpenPyXL, it has no cells; the cells are created when they are accessed. You can access the individual cells of a sheet using the keys of the worksheet or use the row and column notation using cell() method:\xa0', 'Another way to add write data is to write rows using the append() method:\xa0', 'There are three ways to read from multiple cells in OpenPyXL. The first method is the range operator. Let’s try reading each cell of the ‘Spells’ sheet using the range operator:', 'The second way of reading multiple cells is by iterating over the rows using iter_row() function; which reads data one row at a time:', 'Similarly you can also read the data as columns using the \xa0iter_col() method:', 'Whatever you have done so far has been saved in a Workbook object in memory, but no .xlxs file has been created. To access the workbook outside OpenPyXL, you need to save it:', 'Excel uses charts to visualize data. To create a chart in our OpenPyXl worksheets, we first need to define the chart type such BarChart, LineChart, etc. We also need to define the data to be used for the chart using a Reference object. Let’s writing some student performance data and plot a bar chart:', 'You can learn more about the charts available in OpenPyXL here.', 'To add a formula to a cell in OpenPyXL, you simply need to define the formula as a string in the required cell. Let’s try calculating the average marks for the two subjects:', 'Although images are not as common as charts or formulae, sometimes they can be used to enrich worksheets and make them more visually appealing. To work with images in OpenPyXL, you’ll need to install the Pillow module.\xa0 OpenPyXL provides the add_image() method for adding images to a specified cell:', 'The code for the above implementation can be found in a Colab notebook here.']","'.xlxs'
 '.xlxs', 'load_workbook()', 'Workbook', 'Workbooks', 'active', 'workbook[“sheet_name”]', 'title', 'create_sheet()', 'sheetnames', 'cell()', 'append()', '‘Spells’', 'iter_row()', '\xa0iter_col()', '.xlxs', 'Reference', 'Pillow', 'add_image()'"
23,python_bi_analytics_tool/,https://analyticsindiamag.com/overview-of-atoti-a-python-bi-analytics-tool/,"['Atoti is a Python business intelligence analytics tool that creates a Tableau-like dashboarding interface inside Jupyter notebooks. It provides a BI web application that enables hassle-free dashboard creation and sharing. Notebooks on their own are an amazing tool but they very obvious limitation when it comes to analytics tasks:', 'Atoti Stores, in-memory tables, scale very efficiently and can handle a lot more data than DataFrames. Additionally, it enables analysts to create advanced data models using joins between stores without duplicating data as done by a merge in pandas. Atoti has embedded interactive data visualization tools that can be used to build scenarios, apply filters and compare different versions of the data.\xa0', 'That being said, atoti doesn’t aim to replace pandas or Spark; they are both good tools for cleaning and transforming data. Atoti, on the other hand, focuses on visualization, analysis and collaboration. If that’s the case, then why not just use a dedicated BI tool like Tableau or PowerBI? For starters, atoti eliminates the need to export the data and load it into another software. In addition to that, it enables analysts to create new measures in Python rather than using a niche language like PowerBI’s DAX.\xa0', 'Atoti can be installed from PyPI, if you want to use it interactively in notebooks, you’ll need to install its JupyterLab plugin as well.', 'load_all_data() needs to be called when there are more than 1000 rows; otherwise, the first 1000 rows are sampled. The keys argument is used to indicate the primary key of the table.\xa0', 'Read more about OLAP cubes here.', 'The cube has created sum and mean measures for all numeric fields.\xa0', 'Or dice the cube to get the value for each COUNTY', 'Or slice on a particular COUNTY', 'This web application is a “safe” environment; all the filters and queries are read-only and the original data is not affected by it.']","'Stores'
 'Store', 'load_all_data()', 'keys', 'query()', 'COUNTY', 'COUNTY'"
24,annotating_3d_point_clouds/,https://analyticsindiamag.com/labelcloud-python-tool-for-annotating-3d-point-clouds/,"['3D deep learning finds crucial applications nowadays in many domains, including robotics, autonomous driving, virtual reality, and medical diagnosis. The 3D data required for training is collected using 3D sensors such as LiDAR (Light Detection And Ranging) and Depth cameras (e.g. RGB-D cameras). With increased numbers of 3D sensors, collecting data for training and capturing data during deployment becomes easier. However, training data mostly requires annotated data. Annotating 3D data is time consuming, tedious, and needs skilled manpower.', 'The output of 3D sensors are mostly point clouds. Point clouds are 3D point sets unorderly arranged in a high-dimensional space. Annotating those point clouds with 3-dimensional bounding boxes are usually performed manually. In the recent past, some automatic labeling tools emerged to annotate data for autonomous driving. These tools use the fixed horizontal orientations of vehicles- they always stay on the road parallel to the road. Therefore, these automatic labeling tools convert the 3D cloud points data into 2D images. The vehicle height yields the third dimension to those 2D images.', 'However, these automatic labeling tools can not be applied to data belonging to domains other than autonomous driving. These tools offer high restrictions in the data format and give lesser convenience in functionality. To this end, German researchers, Christoph Sager from Technische Universität Dresden, Patrick Zschech from Friedrich-Alexander-Universität Erlangen-Nürnberg, and Niklas Kühl from Karlsruhe Institute of Technology have developed labelCloud, a domain-independent labeling tool for 3D object detection in 3D point clouds.', 'labelCloud is superior to any of its competitors, such as 3D Bat, LATTE, and SAnE, in every aspect:', 'labelCloud is written with Python in a modular design paradigm. It utilizes the Python libraries NumPy and Open3D for array calculations and cloud data processing, respectively. While labeling, labelCloud develops 3D bounding boxes over point clouds. Each bounding box is defined with 10 parameters in labelCloud: one for the object class and nine Degrees of Freedom – three for the object location (x,y,z), three for the dimensions (length, width, height), and three for the rotations (roll, pitch, yaw).\xa0', 'labelCloud offers a powerful GUI (Graphical User Interface) for visualizing the cloud points. It enables rotations, translations, selection and other processes using mouse movements and clicks, and keyboard presses. It incorporates the OpenGL library for quick and efficient visualizations. Further, it improves user interactions with text fields and buttons.', 'Three modules control the activities of the labelCloud tool:', 'Point cloud manager takes care of importing point clouds from different formats and their manipulations. The drawing manager gives different annotation options and formats. The label manager is responsible for exporting the annotated bounding boxes and their classes in the required format.\xa0', 'Major requirements are Python 3.6+, NumPy, OpenGL, and Open3D.\xa0', 'Clone the source code to the local or virtual machine using the following command.', 'Change directory to the downloaded source files for further processing.', 'Install the dependencies by reading the requirements.txt file.', 'A portion of the output:', 'Run labelCloud on a sample point cloud data stored in pointclouds directory. Users can opt for their own data in any supported format. An example point cloud data, ‘exemplary.ply’ is provided in the directory.\xa0', 'When the file is run, it opens the app as GUI with the help of the following codes.', 'Users can change config.ini file contents based on the requirements. The default configurations are provided as follows:', 'This article discussed the labelCloud Python tool, meant for annotating 3D point cloud data for object detection tasks. We have discussed its architecture, the superior qualities over other competing tools, and its code implementation. The research paper is yet to be published in an upcoming conference. The tool will find great usage among researchers and practitioners in the object detection domain with lightweight implementation and versatile support and flexibility.']","'pointclouds'
 'config.ini'"
25,using_plot3d_r_package/,https://analyticsindiamag.com/visualize-data-in-3d-using-plot3d-r-package/,"['Data visualization holds an important place among a wide range of tasks handled by data scientists and analysts for getting a proper insight of the data in hand. This post will talk about how to visualize data in 3D using the plot3D package of the R programming language.', 'Note: The code snippets throughout this article have been implemented using RStudio IDE (version 1.2.1335). RStudio can be downloaded from here.\xa0', 'data(iris)', 'Display initial records of the data.', 'head(iris)', 'scatter3D(a,b,c, clab = c(""Sepal"", ""Width (cm)""))', 'Here, a,b and c represent the x,y and z coordinates of points respectively. ‘Clab’\xa0', 'parameter specifies the label of the color key.', 'scatter3D(a, b, c, bty = ""f"", colkey = FALSE, main =""bty= \'f\' (full box)"")', 'scatter3D(a, b, c, bty = ""b2"", colkey = FALSE, main =""bty= \'b2\'(with back panels and grid line)"")', '6) The plotted points can be annotated using text3D() function.', '‘theta’ and ‘phi’ parameters define the azimuthal angle, and co-altitude of the scatter plot.', 'Load the dataset.', 'data(mtcars)', 'Display its initial records.', 'head(mtcars)']","'data(iris)'
 'head(iris)', 'scatter3D(a,b,c, clab = c(""Sepal"", ""Width (cm)""))', 'scatter3D(a, b, c, bty = ""f"", colkey = FALSE, main =""bty= \'f\' (full box)"")', 'scatter3D(a, b, c, bty = ""b2"", colkey = FALSE, main =""bty= \'b2\'(with back panels and grid line)"")', 'data(mtcars)', 'head(mtcars)'"
26,python_wrapper_for_mongodb/,https://analyticsindiamag.com/guide-to-pymongo-a-python-wrapper-for-mongodb/,"['MongoDB is a NoSQL database that stores data in JSON-like documents with flexible schemas instead of the traditional table-based database structure. The document model maps to objects in application code which makes it easier to work with. It has a rich-query language that supports dynamic queries on documents. MongoDB also has its own aggregation pipeline and support for map-reduce, eliminating the need for complex data pipelines.\xa0PyMongo is a Python library that contains tools for interacting with MongoDB databases.\xa0', 'To install PyMongo from PyPI:', 'For this article, we will be working with a local MongoDB instance. Instructions for downloading and installing can be found here. Additionally, I recommend installing MongoDB Compass to have a GUI to explore the data and see the changes made by the code.', 'When working with MongoDB databases, or any database for that matter, the first thing we need to do is to make a connection. You can do so using the MongoClient() method:', 'This establishes a connection to the default host and port, we can also specify the host and port:', 'Or use the MongoDB URI format:', 'You can use this default URI to connect to the local instance in Compass.', 'Now that we have established a connection to the local instance, let’s list the existing databases using the list_database_names() method:', 'A single MongoDB instance can have multiple databases, as can be seen above. To access one of these databases, you can either use attribute style access on the client object:', 'Or the dictionary-style access:', 'Collections are exactly that, collections of documents stored in MongoDB. They can be thought of as the MongoDB equivalent of tables. You can list the collections of a database using list_collection_names().', 'Like the databases, you can use either the attribute style access or the dictionary-style access to get a collection:', 'One thing to note about the databases and collections in MongoDB is that they are created lazily, i.e., they are created when the first document is inserted into them. Before we illustrate this, let’s briefly go through what a document is in MongoDB.', 'MongoDB stores data using JSON-style documents which are key-value pairs. Documents are the MongoDB equivalent of rows, just more flexible. Pymongo uses dictionaries to represent documents. As an example, the following dictionary may be used to store a LinkedIn connection:', 'Insert operations add document(s) to a collection, and if the collection does not exist the insert operation will create it. You can also create a collection by explicitly using the create_collection() method.', 'Let’s go back to the lazy creation of collections and databases.\xa0', 'Firstly we’ll need to create functions for verifying the existence of databases and collections.', 'Now let’s try to create a database and a collection.', 'None of the above commands has actually performed any operations on the local MongoDB instance. Inserting a document using insert_one() will create both the database and collection.', 'In addition to inserting a single document, you can also perform bulk creation operations using insert_many():', 'Each of the documents is assigned a unique ObjectID that act like primary keys. You can access the _id of the inserted documents using the inserted_ids attribute.', 'You can override the default unique Id by defining your own _id:', 'Ensure the _id of the records you insert is unique; otherwise, you’ll encounter a BulkWriteError.', 'The most basic read operation in MongoDB is the find_one() method; it returns a single document matching a query. Let’s use this to print the first document from the team collection:', 'To query more than one document, you can use the find() method. The find() method returns an iterable cursor object with all documents matching the query. Let’s say you are interested in documents from the collection team with an age greater than 25.', 'You can use the limit() method to specify the number of documents you want to display.', 'Like the create and read operations, MongoDB provides two methods for update operations: update_one() for updating a single document and update_many() for updating all documents matching the criteria.', 'To delete documents from a collection you can either use delete_one() or delete_many(). Let’s say you want to delete Kimmy’s data from the collection:', 'To delete the data for people over the age of 40:\xa0', 'To delete all the documents present in the collection you can just pass an empty dictionary:', 'To drop an entire collection, you can use the drop() method:', 'MongoDB supports three ways for performing aggregation: single-purpose aggregation methods, aggregation pipeline, and map-reduce function.\xa0', 'The following example illustrates how to create an aggregation pipeline using the aggregation() method. We will calculate the total occurrence of each pet in the pets array across all documents in the Students collection and sort them by count. You can’t directly perform operations on arrays so you’ll need to unwind them using the $unwind stage. You can find a list of all aggregation stages here. After unwinding the array, the documents are grouped by the pets, summed up, and finally sorted by count.', 'Another way of doing this aggregation is by using the map_reduce() function. You’ll have to write a map and reduce functions to count the occurrences of each pet across the collection.', 'The map function emits a (pet, 1) pair for each pet; the reduce function adds all the emitted value for a particular pet.\xa0', 'Aggregation pipelines provide better performance than map-reduce operation, all map-reduce methods can be rewritten using aggregation operators. MongoDB provides the $accumulator and $function operators to define map-reduce operations that require custom functionality.\xa0']","'MongoClient()'
 'list_database_names()', 'list_collection_names()', 'create_collection()', 'insert_one()', 'insert_many()', '_id', 'inserted_ids', '_id', '_id', 'BulkWriteError', 'find_one()', 'team', 'find()', 'find()', 'cursor', 'team', 'age', 'limit()', 'update_one()', 'update_many()', 'drop()', 'aggregation()', '$unwind', 'map_reduce()', '$accumulator', '$function'"
27,dataframes_with_python_codes/,https://analyticsindiamag.com/comprehensive-guide-to-pandas-dataframes-with-python-codes/,"['Python programming language provides a wide range of libraries and modules extensively used by Data Science enthusiasts. The most basic yet quite useful ones are NumPy and Pandas (built on the top of NumPy), which assist in systematic analysis and manipulation of data. This article talks about ‘DataFrame’ – a crucial concept for working with Pandas library.\xa0\xa0', 'DataFrame is a 2D mutable data structure that can store heterogeneous data in tabular format (i.e. in the form of labelled rows and columns). By heterogeneous data, we mean a single DataFrame can comprise different data types’ content such as numerical, categorical etc.', 'The building block of a DataFrame is a Pandas Series object. Built on the top of the concept of NumPy arrays, Pandas Series is a 1D labelled array that can hold heterogeneous data. What differentiates a Pandas Series from a NumPy array is that it can be indexed using default numbering (starting from 0) or custom defined labels. Have a look at how NumPy array and Pandas Series differ.', 'Where, (0,1,2) are the labels and (10,20,30) form the 1D Series object.', 'Here, we demonstrate how to deal with Pandas DataFrame using Pythonic code. Several (though not all) data operations possible with a DataFrame have been shown further in this article with explanation and code snippets.', 'Note: The code throughout this article has been implemented using Google colab with Python 3.7.10, NumPy 1.19.5 and pandas 1.1.5 versions.', 'Populate a DataFrame with random numbers selected from a standard normal distribution using randn() function.', 'Where,\xa0 the parameters of DataFrame() represents the data to be filled in the DataFrame, list of indices and list of column names respectively.', 'df \xa0 #Display the DataFrame', 'All the columns in the above DataFrame are Pandas Series objects having common index A,B,…,E. DataFrame is thus a bunch of Series sharing common indexes.', 'A basic summary of a number of rows and columns, data types and memory usage of a DataFrame can be obtained using info() function as follows:', 'df.info()', 'Aggregated information such as total count of entries in each column, mean, minimum element, maximum element, standard deviation etc. of numerical columns can be found as:', 'df.describe()', 'Column names of a DataFrame can be known using the ‘columns’ attribute as:', 'df.columns', 'To know the range of indexes of a DataFrame, use the ‘index’ property of the DataFrame object.', 'df.index', 'The index starts from the ‘start’ value and ends at (‘stop’-1), i.e., it is from 0 to 4, incrementing in the step of 1.', 'Suppose we want to extract the column ‘W’ from the above DataFrame df. It can be done as:', 'df[‘W’] or df.W', 'Multiple columns from a DataFrame can be extracted as:', ""df[['W','Z']]"", 'type(df)', 'type(df[‘W’])', 'The above output verifies that each column of DataFrame is a Series object.', 'Suppose we want to create a column with the label ‘new_col’ which has elements as the sum of elements of rows W and Y. It can be created as follows:', 'Let’s say we now need to drop the ‘new_col’ column from df DataFrame.', 'df.drop(‘new_col’)', 'Executing this line of code will result in an error as follows:', 'KeyError: ""[\'new_col\'] not found in axis""', 'To drop a column, we need to set the ‘axis’ parameter to 1 (indicates ‘column’). Its default value is 0, which denotes a ‘row’, and since there is no row with the label ‘new_col’ in df, we got the above error.\xa0', ""df.drop('new_col',axis=1)"", 'Why axis=0 denotes row, and axis=1 denotes column?', 'If we check the shape of DataFrame df as:', 'df.shape', 'We get the output as (5,4), which means there are 5 rows and 4 columns in df. The shape of a DataFrame is thus stored as a tuple in which 0-indexed element denoted number of rows and 1-indexed element shows a number of columns. Hence, axis value 0 and 1 denote row and column, respectively.\xa0', 'However, if we check the df DataFrame now, it will still have the ‘new_col’ column in it. To drop the column from the original DataFrame as well, we need to specify the ‘inplace=True’ parameter in the drop() function as follows:', ""df.drop('new_col',axis=1,inplace=True)"", '‘loc’ property can extract a column by specifying the row label, while ‘iloc’ property can be used to select a row by specifying the integer position of the row (starting from 0 for 1st row).', 'df.loc[‘A’]\xa0 #to extract row with index ‘A’', 'To extract a single element:', 'df.loc[‘B’,’Y’] \xa0 #element at row B and column Y', 'Multiple elements from specified rows and columns can also be extracted as:', 'Conditional selection of elements:', 'We can check specific conditions, say which elements are greater than 0. ‘True’ or ‘False’ in the output will indicate if the element at that location satisfies the condition or not.', 'df>0\xa0\xa0\xa0', 'To get the actual elements within the DataFrame which satisfy the condition:', 'df[df>0]\xa0\xa0', 'Here, NaN (Not a Number) is displayed in place of elements that are not greater than 0.', 'If we need only those rows for which a column say W has >0 elements:', ""df[df['W']>0]"", 'To extract even specific columns (say columns W and X) from the above result:', ""df[df['W']>0][['W','X']]"", 'Multiple conditions can also be applied using & (‘and’ operator) and | (‘or’ operator).', 'We add a new column called ‘States’ first to df', ""df['States'] = ['CA', 'NY', 'WY', 'OR', 'CO']"", 'Now, set this column as the index column of df', ""df.set_index('States')"", 'This resets the index of df, but the original indices remain intact under a new column called ‘index’. We can drop that column.', 'First, we create a DataFrame having some NaN values.', 'Drop rows with NaN', 'df1.dropna()', 'By default, axis=0 so the rows (and not columns) with at least one NaN value are searched on executing the above line of code.', 'To drop columns with NaN instead, specify axis=1 explicitly.', 'df1.dropna(axis=1)', 'We can also specify a ‘thresh’ parameter which sets a threshold, say if thresh=2, then rows/columns with less than 2 NaN values will be retained, rest will be dropped..', 'df1.dropna(thresh=2)', 'The third row in df1 had 2 NaN’s so it got dropped. We can fill the missing entries with custom value, say with the word ‘FILL’.', ""df1.fillna(value='FILL')"", 'Missing values can also be replaced with some computed values e.g. 1 and 2 are the non-null values of column ‘A’ whose mean will be 1.5. All the missing values of df1 can be replaced with this average value.', ""df1.fillna(value=df1['A'].mean())"", 'We can know if an element at is missing or not using isnull() function as follows:', 'df1.isnull()', 'GroupBy functionality allows us to group several rows of a DataFrame based on a column and perform an aggregate function such as mean, sum, standard deviation etc.', 'Suppose, we create a DataFrame having company names, employee names and sales as follows:', 'Details of all employees of each of the unique companies can be grouped together as:', 'We can now apply aggregate functions to this GroupBy object. E.g. getting the average sales amount for each company', 'byComp.mean()', 'We can obtain information of a particular company, say sum of the sales of FB can be obtained as:', ""byComp.sum().loc['FB']"", 'A complete summary of company-wise sales can be obtained using describe() function.', 'byComp.describe()', 'Let’s say we create a DataFrame as follows:', 'The DataFrame can be sorted based on the values of a particular column by passing that column’s name to ‘by’ parameter of sort_values() method.', ""df3.sort_values(by='col1')"", ""df3.sort_values(by='col2')"", 'By default, sorting occurs in ascending order of the values of a specified column. To sort in descending order, explicitly specify the ‘ascending’ parameter as False.', ""df3.sort_values(by='col3',ascending=False)"", 'Suppose we define a custom function to multiply an element by two as follows:', 'The function can then be applied to all the values of a column using apply() method.', ""df3['col1'].apply(times2)"", 'The same process can be done by defining a lambda function as:', ""df3['col1'].apply(lambda x: x*2)"", 'Where, (lambda x: x*2) means that for each element, say x of the selected column, multiply that x by 2 and return the value.', 'A pivot table can be created summarizing the data of a DataFrame by specifying custom criteria for summarization.', 'E.g. We create a DataFrame as:', 'This will create a pivot table from df4 with multi-level indexes – the outer index will have unique values of column A while the inner index will have unique values of column B. The unique values of column C, i.e. x and y will form the column names of the pivot table and the table with populated with values of column D.\xa0', 'There are NaN values in the table for which an entry does not exist in df4, e.g. there is no column in df4 for which A=’b’,B=’two’ and C=’x’ so the corresponding D value in the pivot table is NaN.']","'array([10
 20, 30])', 'df \xa0 #Display the DataFrame', 'df.info()', 'df.describe()', 'df.columns', ""Index(['W', 'X', 'Y', 'Z', 'States'], dtype='object')"", 'df.index', 'RangeIndex(start=0, stop=5, step=1)', 'df[‘W’]', 'df.W', ""df[['W','Z']]"", 'type(df)', 'pandas.core.frame.DataFrame', 'type(df[‘W’])', 'pandas.core.series.Series', 'df.drop(‘new_col’)', 'KeyError: ""[\'new_col\'] not found in axis""', ""df.drop('new_col',axis=1)"", 'df.shape', ""df.drop('new_col',axis=1,inplace=True)"", 'df.loc[‘A’]\xa0 #to extract row with index ‘A’', 'df.loc[‘B’,’Y’] \xa0 #element at row B and column Y', '1.5792128155073915', 'df>0', 'df[df>0]\xa0\xa0', ""df[df['W']>0]"", ""df[df['W']>0][['W','X']]"", ""df['States'] = ['CA', 'NY', 'WY', 'OR', 'CO']"", ""df.set_index('States')"", 'df1.dropna()', 'df1.dropna(axis=1)', 'df1.dropna(thresh=2)', ""df1.fillna(value='FILL')"", ""df1.fillna(value=df1['A'].mean())"", 'df1.isnull()', 'byComp.mean()', ""byComp.sum().loc['FB']"", 'byComp.describe()', ""df3.sort_values(by='col1')"", ""df3.sort_values(by='col2')"", ""df3.sort_values(by='col3',ascending=False)"", ""df3['col1'].apply(times2)"", ""df3['col1'].apply(lambda x: x*2)"""
28,data_reports_with_datapane/,https://analyticsindiamag.com/creating-interactive-data-reports-with-datapane/,"['One of the issues of using Python for data analytics is the inability to create shareable data visualization reports quickly. Other analytics tools like PowerBI and Tableau can readily publish and share reports. Sure, you can create shareable reports with modules like dash and Flask but they require quite a bit of extra code. Datapane is a Python module that enables you to quickly create shareable reports from existing data analysis components like Pandas DataFrames and plots from a wide range of visualisation libraries like Plotly and Bokeh. Check out this article to learn the basics of creating a report with Datapane. Recently Datapane has introduced new components that make creating basic reports even easier and allow analysts to exercise more control over the report layout and view.\xa0\xa0', 'One of these components is the DataTable block; this is an extremely powerful component, so much so that it facilitates the creation of a basic data report with just a few lines of code. The DataTable block takes a DataFrame and creates an interactive table that allows sort and search operations. In addition to that, it incorporates data analysis tools like Pandas-Profiling and SandDance by default.\xa0\xa0', 'Let’s create a barebones report for the wine quality dataset using just the DataTable block. First, let’s install and set up Datapane.', 'Install from PyPI', 'pip install -U datapane', '\xa0To publish and share reports, you’ll need an API token. Register on the Datapane website and get your token from the account settings.\xa0', 'Load the dataset and pass the DataFrame object to dp.DataTable. Publish the DataTable block using dp.Report(block_name).publish(‘Report Name’, ‘Report Description’)', 'That’s it! If you visit the above link you’ll find an interactive table like this.\xa0', 'Not only that, using the buttons in the top-right corner, you can analyze the data with Padas-Profiling and SandDance.', 'You can learn more about dp.DataTable here.', 'The default layout of Datapane reports is good enough for basic use cases, but if you want to create an intricate report layout with multiple pages, you can do so with Select, Group and Page components.', 'The dp.Select component enables you to create different tabs for different blocks. Let’s say we have three components we want to display: white wine interactive table, source code of the whole report and a 3D scatter plot of pH, alcohol content and density.\xa0', 'To share syntax-highlighted code instead of simple formatted text, we can use another one of the new components, dp.Code. It currently only supports Python and Javascript.', 'You can learn more about dp.Select here.', 'dp.Group can be used to display components side-by-side. Let’s say we want to display the interactive table for red wine data alongside a scatter plot of citric acid and quality.', 'You can learn more dp.Group here.', 'To facilitate the creation of multi-page reports Datapane provides the dp.Page component. Let’s create a combined report of both the wine types using the previously created Select and Group components.', 'You can learn more about dp.Page here.', 'The above report and its source code can be found here.\xa0', 'To learn more about Datapane please refer to the following resources:']","'pip install -U datapane'
 'dp.DataTable', 'dp.Report(block_name).publish(‘Report Name’, ‘Report Description’)', 'dp.DataTable', 'dp.Select', 'dp.Select', 'dp.Group', 'dp.Group', 'dp.Page', 'dp.Page'"
29,for_reporting_and_visualization/,https://analyticsindiamag.com/hands-on-guide-to-datapane-python-tool-for-reporting-and-visualization/,"['Creating reports using python is an easy task because we can use different python libraries and combine our exploration of the data with some meaningful insights. But sharing this report is not that easy because not everyone or your client is used to python so that he can open your jupyter notebook and understand what you are trying to tell.', 'Datapane is an open-source python library/framework which makes it easy to turn scripts and notebooks into interactive reports. We can share these reports with our viewers or clients so that they can easily understand what the data is trying to tell.', 'Datapane allows you to systematically create reports from the objects in your Python notebook, such as pandas DataFrames, plots from visualization libraries, and Markdown text. We can also choose to publish our datapane reports online by selecting the desired audience.', 'In this article, we will explore how we can create a data report using Datapane and publish it to an HTML file.\xa0', 'Like any other library, we will install datapane using pip install datapane.', 'We will be loading our dataset using pandas so we need to import pandas and for creating the report we will import datapane.', 'import pandas as pd', 'import datapane as dp', 'We will load an advertisement dataset that contains different attributes like ‘Sales’, ‘TV’ etc of an MNC. We will use this data to create a report of the data.', 'df = pd.read_csv(‘Advertisement’.csv’)', 'df.head()', 'We need to create the required visualizations in the jupyter notebook so that we can pass it to the datapane reports for visualizing. Here we will create a histogram, box plots, and Regression plots.', '#histogram', 'histogram = df.hist()', '#boxplot', ""boxplot = df.boxplot(column=['TV', 'Radio', 'Newspaper'])"", '#Regression Plots', 'import seaborn as sns', 'scat1 = sns.regplot(x=""Sales"", y=""TV"", data=df)', 'scat2 = sns.regplot(x=""Sales"", y=""Radio"", data=df)', 'scat1 = sns.regplot(x=""Sales"", y=""Newspaper"", data=df)', 'We have created these plots and stored them in a variable so that we can call them in our report for visualization.', 'The next step is to create a report, we will use markdowns so that we can clearly define different sections of our report.', 'report = dp.Report(', '\xa0 \xa0 dp.Markdown(""Advertisement Report With Sales Data""),', '\xa0\xa0\xa0\xa0dp.Table(df),', '\xa0\xa0\xa0\xa0dp.Markdown(""Histogram Of all Attributes""),', '\xa0\xa0\xa0\xa0dp.Plot(histogram),', '\xa0\xa0\xa0\xa0dp.Markdown(""Box Plot of the Feature Variable""),', '\xa0\xa0\xa0\xa0dp.Plot(boxplot),', ""\xa0\xa0\xa0\xa0dp.Markdown('Regression Plots for all features against target\xa0variable'),"", '\xa0\xa0\xa0\xa0dp.Plot(scat1),', '\xa0\xa0\xa0\xa0dp.Plot(scat2),', '\xa0\xa0\xa0\xa0dp.Plot(scat3)', ')', 'Here we have created the report with importing all the plots we have created and the dataframe as a table. Next, we will publish this report to an Html file.', 'The final step is to publish the report we created so that we can share the reports with respective clients/users.', ""report.save(path='adver_data.html')"", 'Now let us open the report we have created and see how it looks. The report would be downloaded in your system where your python setup is installed, go to the directory, and open the file.', 'This is the main page of our report, you can see here that the data is represented in the form of a table and you can search for different values in the particular column. Also, we can arrange the data in ascending or descending order. Let’s look at the visualization we created and how they look in our report.', 'Here, we can see in the report that the data and the visualization along with the markdowns create a highly informative report which can be shared with the respective person.', 'Similarly, we can use different datasets to create different types of reports and visualization and publish them into shareable Html reports.']","'import pandas as pd'
 'import datapane as dp', 'df = pd.read_csv(‘Advertisement’.csv’)', 'df.head()', '#histogram', 'histogram = df.hist()', '#boxplot', ""boxplot = df.boxplot(column=['TV', 'Radio', 'Newspaper'])"", '#Regression Plots', 'import seaborn as sns', 'scat1 = sns.regplot(x=""Sales"", y=""TV"", data=df)', 'scat2 = sns.regplot(x=""Sales"", y=""Radio"", data=df)', 'scat1 = sns.regplot(x=""Sales"", y=""Newspaper"", data=df)', 'report = dp.Report(', 'dp.Markdown(""Advertisement Report With Sales Data""),', '\xa0dp.Table(df),', 'dp.Markdown(""Histogram Of all Attributes""),', 'dp.Plot(histogram),', 'dp.Markdown(""Box Plot of the Feature Variable""),', 'dp.Plot(boxplot),', ""dp.Markdown('Regression Plots for all features against target\xa0variable'),"", 'dp.Plot(scat1),', 'dp.Plot(scat2),', 'dp.Plot(scat3)', ""report.save(path='adver_data.html')"""
30,synthetic_data_generation_libraries/,https://analyticsindiamag.com/guide-to-synthetic-data-vault-an-ecosystem-of-synthetic-data-generation-libraries/,"['Synthetic Data Vault (SDV) is a collection of libraries for generating synthetic data for Machine Learning tasks. It enables modeling of tabular and time-series datasets that can then be used to synthesise new data resembling the original ones in terms of format and statistical properties. SDV was introduced by Neha Patki, Roy Wedge and Kalyan Veeramachaneni – researchers at CSAIL and LIDS, MIT (research paper).', 'A brief introduction of Synthetic Data Vault can also be found in one of our previous articles (weblink). This article talks about the open-source project in a bit more detail, along with its practical example using Python code.', 'Synthetic data generated using SDV can be used as additional information while training Machine Learning models (data augmentation). times, it can even be used in place of the original data since they both remain identical to each other. It also maintains the original data integrity, i.e. the original data does not get disclosed to the user seeing its synthetic version. SDV uses recursive sampling methods and hierarchical models for data generation in order to allow a wide range of structures for storing the synthetic data.\xa0', 'The SDV project is still in its development stage. The functionalities it provides till dates have been summarized in the following section.', 'SDV provides synthetic data generators for creating new data from the following types of data:', 'Here’s a demonstration of implementing the Hierarchical Modelling Algorithm (HMA) for generating new data using Synthetic Data Vault. HMA enables recursive scanning of a relational dataset. It applies tabular model, allowing it in the dataset, allowing the model to learn relationships between different fields of all the tables. The code has been implemented using Google colab with Python 3.7.10 and sdv 0.9.0 versions. Step-wise explanation of the code is as follows:', '!pip install sdv\xa0', 'from sdv import load_demo', 'IMP NOTE: On executing the above import statement, you may get an error as follows:', 'ValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject', 'Uninstalling the NumPy library and reinstalling it resolves this error.', 'Now, import the load_data() method and the execution will be successful.', 'from sdv import load_demo', 'md, tb = load_demo(metadata=True)', 'load_data() with ‘metadata’ parameter set to ‘True’ will be tuple consisting an instance\xa0having metadata of the dataset and a dictionary having data tables loaded in the form of\xa0Pandas DataFrames.', 'md', 'tb', 'md.visualize()', 'The output figure shows the primary key and foreign for each of the three tables. It also shows the parent-child relationships among them i.e. ‘transactions’ is the child of ‘sessions’ table which again is the child of ‘users’ table.', 'from sdv.relational import HMA1', 'hma_model = HMA1(md)', 'hma_model.fit(tb)', 'NOTE: When the fit() method is executed, the SDV scans all the tables in the\xa0order of relationship amazon them. It learns every child table using the GaussianCopula\xa0Model. Before learning the parent table, SDV augments it using copula parameters of its\xa0child table so that the model can learn how the relation between rows of the parent table\xa0and those of its child table(s).', 'synth_data = hma_model.sample()', 'A new dictionary of tables resembling those in the original ‘tb’ dictionary will be created.\xa0The tables will contain new data identical to that of the original relational tables.', ""hma_model.save('hma_mod.pkl')"", 'Load the saved model.', ""load_model = HMA1.load('hma_mod.pkl')"", 'NOTE: The saved model’s file ‘hma_model.pkl’ will be smaller in size than the original data file. The reason being, the HMA model does not have any information of the original data. It only uses the copula parameters for creating a new version of the data. Thus, the saved model’s file can be shared anywhere without revealing any details about the original data.', 'newdt = load_model.sample()', 'newdt', ""hma_model.sample('sessions', num_rows=6)"", 'Since the above line of code specifies ‘sessions’, the synthetic data will only contain the ‘sessions’ table and its child i.e. the ‘transactions’ table.', ""hma_model.sample('users', num_rows=6, sample_children=False)""]","'!pip install sdv\xa0'
 'from sdv import load_demo', 'from sdv import load_demo', 'md, tb = load_demo(metadata=True)', 'md', 'tb', 'md.visualize()', 'from sdv.relational import HMA1', 'hma_model = HMA1(md)', 'hma_model.fit(tb)', 'synth_data = hma_model.sample()', ""hma_model.save('hma_mod.pkl')"", ""load_model = HMA1.load('hma_mod.pkl')"", 'newdt = load_model.sample()', 'newdt', ""hma_model.sample('sessions', num_rows=6)"", ""hma_model.sample('users', num_rows=6, sample_children=False)"""
31,features_set_up_instructions/,https://analyticsindiamag.com/pinterest-open-sources-big-data-analytics-tool-querybook-major-features-set-up-instructions/,"['Recently, Pinterest open-sourced its big data analytics tool Querybook that started its life as an intern project in 2017. Querybook auto analyses executed queries to provide data lineage, example queries, frequent user information, and search/auto-completion ranking. In 2018, the big data tool was released internally and soon became the official solution to query big data at Pinterest.', 'According to its developers, Querybook is built to provide a simple web UI for big data analysis to discover the right data, compose queries, and share findings.\xa0', 'Querybook is a Big Data IDE that allows users to create, discover, and share data analyses, queries, and tables. The tool includes Database, Redis, ElasticSearch and remote storage. The Database is used to store the DataDocs, and MySQL is recommended. Redis is required to send async tasks to workers, maintain multi-server WebSocket connections, and caching live data for collaborative editing. ElasticSearch provides search functionality for database documents such as DataDocs and tables. Lastly, the remote storage stores the query results.', 'Querybook has three key components-', 'According to its developers, there are two main ways to set up the big data analytics tool.', 'Single-Machine Instant Setup (locally or on a server):\xa0', 'The single machine method is a quick way to try out Querybook for less than five users. This method uses docker-compose to bring up all the necessary infrastructure, which is why Docker needs to be installed for quick setup.', 'For installation, open terminal and run the following:\xa0', 'git clone https://github.com/pinterest/querybook.git', 'cd querybook', 'Now run the following:\xa0', 'make', 'Multi-Machine Setup', 'The multi-machine setup is required when someone wants to scale Querybook for thousands of users. The multi-machine set up runs Querybook containers on different machines/pods. This method is more complicated than the single machine instant set up and requires external infrastructure. The set up includes the following steps-', 'Step-1: Requirements', 'Step-2: Choose the instances', 'Step-3: Update your environment variables configuration\xa0', 'Step-4: Start each service', 'You can start each service by the following commands:']",
32,a_velocity_visualization_tool/,https://analyticsindiamag.com/complete-guide-to-dsne-a-velocity-visualization-tool/,"['Handling high-dimensional data is a non-trivial task in Exploratory Data Analysis. Numerous tools ranging from the famous Principal Component Analysis (PCA) to the Stochastic Neighbour Embedding (SNE) variants focus on embedding the high-dimensional data on a low-dimensional visualization, especially in a two-or three-dimensional representation. The vanilla Stochastic Neighbor Embedding converts the high-dimensional Euclidean distance from a point to its K-nearest neighbors into conditional probabilities that represent similarities. Stochastic Neighbour Embedding variants are developed for task-specific applications, including t-SNE, Hierarchical-SNE, RNA-velocity techniques, and UMAP.\xa0', 'Biological science often deals with high-dimensional velocity data such as embryo development, cell fluid transfer, cell-type transition, cell differentiation, and mRNA velocity. Recent work by Volker Bergen, et al., the scVelo has introduced a powerful velocity visualization in a transient cell system. However, the method relies on an intuitive approach rather than a solid mathematical approach. This method yields unreliable results for highly dynamic and unpredictable cell systems such as messenger-RNA (mRNA) velocity visualization.', 'Songting Shi of the Peking University, China, has developed a mathematically sound approach to visualize cell components’ velocity. This method is named as DSNE, the abbreviation for Directional Stochastic Neighbor Embedding. It can be viewed as a variant of the vanilla SNE proposed to handle velocity visualization problems such as cell differentiation and embryo development.', 'DSNE converts the high-dimensional Euclidean distance between the unit-length velocity and the unit-length direction from the point of interest to the nearest neighbor points into a conditional probability distribution. This conditional probability distribution represents the similarity between the velocity and the direction of cell particles.\xa0', 'Mathematically, the conditional probability can be expressed as:', 'Similarly, the conditional probability distribution for the dimensionally-reduced low-dimensional counterparts can be mathematically expressed as:', 'An entropy loss function (the Kullback-Leibler Divergence loss) is incorporated to measure the impurity between the high-dimensional representations and the low-dimensional representations. DSNE is optimized during training by reducing this loss function. The loss function can be expressed mathematically as:', 'Pancreas is one of the sensitive and vital organs in the human body. Pancreas looks after food digestion and blood sugar regulation. Pancreas cells possess highly dynamic functionality inter- and intra-cells. Timely study of Pancreas leads to better medication in case of impairment. Pancreas cell velocity can be analyzed and visualized using DSNE’s python package.\xa0', 'Find the Colab Notebook with these code implementations here.', 'Directional Stochastic Neighbor Embedding (DSNE) outperforms the recent successful methods including scVelo in velocity visualizations. DSNE achieves extraordinary results in cell biology applications such as messenger RNA velocity analysis, embryo development, cell transition and cell differentiation with accuracies close to unity!', 'Further Reading:', 'Original research paper']","'!pip install dsne'
 ' import numpy as np\n import scvelo as scv\n from scipy.sparse import issparse\n from dsne import DSNE, DSNE_approximate ', "" scv.settings.verbosity = 3\xa0 # show errors(0), warnings(1), info(2), hints(3)\n scv.settings.presenter_view = True\xa0 # set max width size for presenter view\n scv.settings.set_figure_params('scvelo')\xa0 # for beautified visualization\n adata = scv.datasets.pancreas()\n scv.pp.filter_and_normalize(adata, min_shared_counts=20, n_top_genes=2000)\n scv.pp.moments(adata, n_pcs=30, n_neighbors=30)\n scv.tl.velocity(adata) "", ' def get_X_V_Y(adata,vkey=""velocity"",\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0xkey=""Ms"",\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0basis=None,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0gene_subset=None,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0):\n \xa0\xa0\xa0\xa0subset = np.ones(adata.n_vars, bool)\n \xa0\xa0\xa0\xa0if gene_subset is not None:\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0var_names_subset = adata.var_names.isin(gene_subset)\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0subset &= var_names_subset if len(var_names_subset) > 0 else gene_subset\n \xa0\xa0\xa0\xa0elif f""{vkey}_genes"" in adata.var.keys():\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0subset &= np.array(adata.var[f""{vkey}_genes""].values, dtype=bool)\n \xa0\xa0\xa0\xa0xkey = xkey if xkey in adata.layers.keys() else ""spliced""\n \xa0\xa0\xa0\xa0basis = \'umap\' if basis is None else basis\n \xa0\xa0\xa0\xa0X = np.array(\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0adata.layers[xkey].A[:, subset]\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if issparse(adata.layers[xkey])\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0else adata.layers[xkey][:, subset]\n \xa0\xa0\xa0\xa0)\n \xa0\xa0\xa0\xa0V = np.array(\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0adata.layers[vkey].A[:, subset]\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if issparse(adata.layers[vkey])\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0else adata.layers[vkey][:, subset]\n \xa0\xa0\xa0\xa0)\n \xa0\xa0\xa0\xa0# V -= np.nanmean(V, axis=1)[:, None]\n \xa0\xa0\xa0\xa0Y =np.array(\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0adata.obsm[f""X_{basis}""]\n \xa0\xa0\xa0\xa0)\n \xa0\xa0\xa0\xa0nans = np.isnan(np.sum(V, axis=0))\n \xa0\xa0\xa0\xa0if np.any(nans):\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0X = X[:, ~nans]\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0V = V[:, ~nans]\n \xa0\xa0\xa0\xa0return X,V,Y ', ' X,V,X_2d = get_X_V_Y(adata,vkey=""velocity"",xkey=""Ms"",basis=""umap"")\n V_2d = DSNE(X, V, Y=X_2d,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0perplexity=3.0,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0K=16,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0threshold_V=1e-8,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0separate_threshold=1e-8,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0max_iter=600,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0mom_switch_iter=250,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0momentum=0.5,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0final_momentum=0.8,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0eta=0.1,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0epsilon_kl=1e-16,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0epsilon_dsne=1e-16,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0seed=6,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0random_state=None,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0copy_data=False,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0with_norm=True,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0verbose=True) ', ' adata.obsm[""X_DSNE""] = X_2d\n adata.obsm[""V_DSNE""] = V_2d\n title =""DSNE""\n scv.pl.velocity_embedding_stream(adata, title=title+\' stream\', basis=\'umap\',V=adata.obsm[""V_DSNE""], smooth=0.5,density=2,)\n scv.pl.velocity_embedding_grid(adata, title=title+\' grid\' , basis=\'umap\',V=adata.obsm[""V_DSNE""], smooth=0.5,density=2,)\n scv.pl.velocity_embedding(adata,\xa0 title=title+\' embedding\',basis=\'umap\',V = adata.obsm[""V_DSNE""]) ', ' title =""DSNE-approximate""\n V_2d = DSNE_approximate(X, V, Y=X_2d,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0perplexity=3.0,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0K=16,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0threshold_V=1e-8,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0separate_threshold=1e-8,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0seed=6,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0random_state=None,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0copy_data=False,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0with_norm=True,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0verbose=True)\n adata.obsm[""X_DSNE_approximate""] = X_2d\n adata.obsm[""V_DSNE_approximate""] = V_2d\n scv.pl.velocity_embedding_stream(adata, basis=\'umap\',V=adata.obsm[""V_DSNE_approximate""],\xa0 title=title+\' stream\', smooth=0.5,density=2,)\n scv.pl.velocity_embedding_grid(adata, basis=\'umap\',V=adata.obsm[""V_DSNE_approximate""],\xa0 title=title+\' grid\', smooth=0.5,density=2,)\n scv.pl.velocity_embedding(adata, basis=\'umap\',V = adata.obsm[""V_DSNE_approximate""], title=title+\' embedding\') ', "" import os\n import numpy as np\n import scvelo as scv\n from anndata import AnnData, read_h5ad\n from dsne import DSNE, DSNE_approximate\n N=500\n D=300\n d=2\n K=16\n perplexity =6\n n_rep=1\n exact = False\n with_norm = True\n basis = 'exact_embeddings'\n verbose = False "", ' def unitLength(V):\n \xa0\xa0\xa0\xa0V_ = V/np.sqrt(np.sum(V*V,axis=1,keepdims=True))\n \xa0\xa0\xa0\xa0return V_\n def velocity_accuracy(V, V_exact):\n \xa0\xa0\xa0\xa0V_unit = unitLength(V)\n \xa0\xa0\xa0\xa0V_exact_unit = unitLength(V_exact)\n \xa0\xa0\xa0\xa0accu = np.sum( V_unit* V_exact_unit )/(V.shape[0]*1.)\n \xa0\xa0\xa0\xa0return accu ', ' def simulate_data(N=50, D=3, d=2, save =True, file_name_prefix =""./data"" ):\n \xa0\xa0\xa0\xa0if not os.path.exists(file_name_prefix):\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0print(""Directory: {} do not exist, create it! \\n"".format(os.path.abspath(file_name_prefix)))\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0os.makedirs(os.path.abspath(file_name_prefix))\n \xa0\xa0\xa0\xa0V_2d = np.random.randn(*(N * 3, d)) * 6\n \xa0\xa0\xa0\xa0err_2d = np.random.randn(*(N * 3, d))*2\n \xa0\xa0\xa0\xa0x_1 = np.asarray([0, ] * d)\n \xa0\xa0\xa0\xa0x_2 = np.asarray([50, ] * d)\n \xa0\xa0\xa0\xa0x_3 = np.asarray([160, ] * d)\n \xa0\xa0\xa0\xa0X_2d = np.zeros_like(V_2d)\n \xa0\xa0\xa0\xa0X_2d[0, :] = x_1\n \xa0\xa0\xa0\xa0X_2d[N, :] = x_2\n \xa0\xa0\xa0\xa0X_2d[N * 2, :] = x_3\n \xa0\xa0\xa0\xa0for i in np.arange(N - 1):\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0X_2d[i + 1, :] = X_2d[i, :] + V_2d[i, :] + err_2d[i,:]\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0X_2d[i + N + 1, :] = X_2d[i + N, :] + V_2d[i + N, :] + err_2d[i + N, :]\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0X_2d[i + N * 2 + 1, :] = X_2d[i + N * 2, :] + V_2d[i + N * 2, :] +\xa0 err_2d[i + N * 2, :]\n \xa0\xa0\xa0\xa0y = np.asarray([0, ] * N + [1, ] * N + [2, ] * N)\n \xa0\xa0\xa0\xa0U = np.array(np.random.randn(*(d, D)))\n \xa0\xa0\xa0\xa0X = X_2d.__matmul__(U)\n \xa0\xa0\xa0\xa0V = V_2d.__matmul__(U)\n \xa0\xa0\xa0\xa0adata = AnnData(X=X, layers={""velocity"": V},obs={""clusters"": y}, obsm={""X_exact_embeddings"":X_2d, ""V_exact_embeddings"":V_2d})\n \xa0\xa0\xa0\xa0if save:\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0file_name = file_name_prefix+""simulated_data_N_{}_D_{}.h5hd"".format(N,D)\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0adata.write_h5ad(file_name)\n \xa0\xa0\xa0\xa0return adata ', ' adata = simulate_data(N=N,D=D,d=d,save=False)\n X = adata.X\n V = adata.layers[""velocity""]\n X_basis = f""X_{basis}""\n X = np.asarray(X, dtype=np.float64)\n V = np.asarray(V, dtype=np.float64)\n Y = None\n if (X_basis in adata.obsm.keys()) and adata.obsm[X_basis] is not None:\n \xa0\xa0Y = adata.obsm[f""X_{basis}""]\n if Y is None:\n \xa0\xa0print(""Do not get the low dimesnional embedding Y! \\n"")\n \xa0\xa0# raise\n Y = np.asarray(Y, dtype=np.float64) ', ' adata_tmp = AnnData(X=X, obsm={""X_umap"": Y}, layers={""velocity"": V, ""spliced"": X})\n scv.tl.velocity_graph(adata_tmp, xkey=\'spliced\')\n scv.tools.velocity_embedding(adata_tmp, basis=""umap"")\n W = adata_tmp.obsm[""velocity_umap""]\n vkey = ""velocity_scvelo_original""\n str_exact = ""exact"" if exact else ""approx""\n method = \'scvelo_velocity_original\'\n adata.obsm[f""{vkey}_{str_exact}_{basis}""] = W\n W_exact = adata.obsm[""V_exact_embeddings""]\n accu = velocity_accuracy(W, W_exact)\n print(f""\xa0 {method}, {str_exact},\xa0 accu: {accu}\\n"")\n method_str = ""scVelo""\n title = ""{} on exact embeddings with accuracy {:5.3f}"".format(method_str, accu)\n scv.pl.velocity_embedding(adata, basis=basis, V=W, title=title,density=2,)\n scv.pl.velocity_embedding_stream(adata, basis=basis, V=W, title=title,density=2,)\n scv.pl.velocity_embedding_grid(adata, basis=basis, V=W, title=title,) ', ' W = DSNE_approximate(X, V, Y=Y,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0perplexity=perplexity,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0pca_d=None,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0threshold_V=1e-8,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0separate_threshold=1e-8,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0seed=16,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0random_state=None,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0copy_data=False,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0verbose=verbose)\n vkey = ""velocity_scvelo""\n str_exact = ""exact"" if exact else ""approx""\n method = ""DSNE_approximate""\n adata.obsm[f""{vkey}_{str_exact}_{basis}""] = W\n W_exact = adata.obsm[""V_exact_embeddings""]\n accu = velocity_accuracy(W, W_exact)\n print(f""\xa0 {method}, {str_exact},\xa0 accu: {accu}\\n"")\n method_str = ""DSNE-approximate""\n title = ""{} on exact embeddings with accuracy {:5.3f}"".format(method_str, accu)\n scv.pl.velocity_embedding(adata, basis=basis, V=W, title=title,density=2,)\n scv.pl.velocity_embedding_stream(adata, basis=basis, V=W, title=title,density=2,)\n scv.pl.velocity_embedding_grid(adata, basis=basis, V=W, title=title,) ', ' W = DSNE(X, V, Y=Y,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0K= K,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0perplexity=perplexity,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0pca_d=None,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0threshold_V=1e-8,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0separate_threshold=1e-8,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0max_iter=1000,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0mom_switch_iter=250,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0momentum=0.5,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0final_momentum=0.8,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0eta=0.1,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0epsilon_kl=1e-16,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0epsilon_dsne=1e-16,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0with_norm=with_norm,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0seed=16,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0random_state=None,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0copy_data=True,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0verbose=verbose)\n vkey = ""velocity_dsne""\n method = \'DSNE\'\n str_exact = ""exact"" if exact else ""approx""\n adata.obsm[f""{vkey}_{str_exact}_{basis}""] = W\n W_exact = adata.obsm[""V_exact_embeddings""]\n accu = velocity_accuracy(W, W_exact)\n print(f""\xa0 {method}, {str_exact},\xa0 accu: {accu}\\n"")\n method_str = ""DSNE""\n title = ""{} on exact embeddings with accuracy {:5.3f}"".format(method_str, accu)\n scv.pl.velocity_embedding(adata, basis=basis, V=W, title=title,density=2,)\n scv.pl.velocity_embedding_stream(adata, basis=basis, V=W, title=title,density=2,)\n scv.pl.velocity_embedding_grid(adata, basis=basis, V=W, title=title,) '"
33,library_with_python_code/,https://analyticsindiamag.com/guide-to-differentiable-digital-signal-processing-ddsp-library-with-python-code/,"['Differentiable Digital Signal Processing (DDSP) is an audio generation library that uses classical interpretable DSP elements (like oscillators, filters, synthesizers) with deep learning models. It was introduced by Jesse Engel, Lamtharn (Hanoi) Hantrakul, Chenjie Gu and Adam Roberts (ICLR paper).\xa0', 'Before going into the library’s details, let us have an overview of the concept of DSP.', 'Digital Signal Processing (DSP) is a process in which digitized signals such as audio, video, pressure, temperature etc., are taken as input to perform mathematical operations on them, e.g. adding, subtracting or multiplying the signals. Visit this page for a detailed understanding of DSP.', 'DDSP library creates complex realistic audio signals by controlling parameters of simple interpretable DSP, e.g. by tuning the frequencies and responses of sinusoidal oscillators and linear filters; it can synthesize the sound of a realistic instrument such as violin, flute etc.\xa0', 'How DDSP works?', 'Image source: Official documentation', 'Neural network models such as WaveNet used for audio generation can generate waveforms taking one sample at one point of time. Unlike those models, DDSP passes parameters through known algorithms for sound synthesis. All the components in the above figure are differentiable,\xa0 the model can be trained end-to-end using stochastic gradient descent and backpropagation.', 'Image source: Official Colab Demo', 'Here’s a demonstration of timbre (pitch/tone quality) transfer using DDSP. The code has been implemented using Google colab with Python 3.7.10 and ddsp 1.2.0 versions. Step-wise explanation of the code is as follows:', '!pip install ddsp', 'sample_rate = DEFAULT_SAMPLE_RATE\xa0\xa0\xa0', 'specplot(audio)', 'Create an HTML5 audio widget using play() method to play the audio file', 'play(audio)', 'Reset CREPE’s global state for re-building the model', 'ddsp.spectral_ops.reset_crepe()', 'start_time = time.time()', 'Compute audio features', 'audio_features = ddsp.training.metrics.compute_audio_features(audio)', 'Store the loudness (in decibels) of the audio\xa0', 'Compute the time taken for calculating audio features by subtracting start time from the current time', ""print('Audio features took %.1f seconds' % (time.time() -\xa0start_time))"", 'The .mp3 audio file that we have used for the demonstration:', ' (Source of the audio file)', 'Define a function to find the selected model', 'model = ddsp.training.models.Autoencoder()', 'Restore the model checkpoints', 'model.restore(ckpt)', '\xa0Sample output: Restoring model took 2.0 seconds', 'Create sliders for model conditioning', 'The sliders to modify conditioning appear in the colab as follows:', 'Quantile shift the parts with ‘on’ section', 'Turn down the parts of audio with ‘off’ notes.', 'Store the computed audio features first', 'af = audio_features if audio_features_mod is None else audio_features_mod', 'Run a batch of predictions', 'Extract audio output from outputs’ dictionary', 'audio_gen = model.get_audio_from_outputs(outputs)', 'Display the time taken for making predictions by computing difference between current time and start time of input audio', ""print('Prediction took %.1f seconds' % (time.time() - start_time))"", 'Original audio:  ', 'Resynthesized audio (using ‘Violin’ model):', 'Google colab notebook of the above implementation is available here.']","'!pip install ddsp'
 'sample_rate = DEFAULT_SAMPLE_RATE\xa0\xa0\xa0', 'specplot(audio)', 'play(audio)', 'ddsp.spectral_ops.reset_crepe()', 'start_time = time.time()', 'audio_features = ddsp.training.metrics.compute_audio_features(audio)', ""print('Audio features took %.1f seconds' % (time.time() -\xa0start_time))"", 'model = ddsp.training.models.Autoencoder()', 'model.restore(ckpt)', 'Restoring model took 2.0 seconds', 'af = audio_features if audio_features_mod is None else audio_features_mod', 'audio_gen = model.get_audio_from_outputs(outputs)', ""print('Prediction took %.1f seconds' % (time.time() - start_time))"""
34,dictionary_module_in_python/,https://analyticsindiamag.com/complete-guide-on-pydictionary-a-real-dictionary-module-in-python/,"['Dictionary is a list of all the words of a particular language generally in alphabetical order with their meanings and other properties like synonyms, antonyms, etc. It helps us learn about different words that are there in a language. Python also allows us to use the dictionary by a module named PyDictionary.', 'A Dictionary is the most important tool to understand the vocabulary of any language because it has all the words that are there in the vocabulary of a language listed in it.', 'PyDictionary is an open-source python library that is used to find the meaning of the words, translation of words and sentences to different languages, and other linguistic properties of different words. PyDictionary uses wordnet for the meanings of the words and search engines for translating the words to different languages.', 'Pydictionary uses beautifulsoup4, requests, etc. in order to fetch the meanings and the linguistic properties of the words and sentences. In this article, we will explore how we can use PyDictionary and what are the different functionalities it provides.', 'Like any other python library, we will install PyDictionary using pip install PyDictionary', 'We will be using PyDictionary for performing different linguistics operations so we will import pydictionary.', 'from PyDictionary import PyDictionary', 'For using PyDictionary we can either create an instance and use it for different words or we can create an instance with passing some words we will use as the argument. We will explore both options.', 'dictionary1 = PyDictionary()', 'dictionary2 = PyDictionary(""exclusive"",""precipitation"")', 'Now we will use different functions and extract some of the linguistics properties of different words.', '#Meaning of Word using dictionary1', ""print(dictionary1.meaning('Archive'))"", '#Meaning of Words in Dictionary2', 'print(dictionary2.printMeanings())\xa0', '#Finding Synonym using Dictionary1', 'print(dictionary1.synonym(""Archive""))', '#Finding Synonyms of word in Dictionary2', 'print (dictionary2.getSynonyms())', '#Finding antonyms using dictionary1', 'dictionary1.antonym(""Archive"")', '#Finding antonyms using dictionary2', 'print (dictionary2.getAntonyms())', '#Translating using Dictionary1', 'dictionary1.translate(""Archive"", ""hi"") #Translating to hindi', '#Translating words in dictionary2', 'print (dictionary2.translateTo(""hi""))', 'Similarly, we can translate words into different languages by giving the desired language in the parameter section.', 'These are some of the functionalities that are provided by PyDictionary and we can use it according to our requirements for different projects.']","'from PyDictionary import PyDictionary'
 'dictionary1 = PyDictionary()', 'dictionary2 = PyDictionary(""exclusive"",""precipitation"")', '#Meaning of Word using dictionary1', ""print(dictionary1.meaning('Archive'))"", '#Meaning of Words in Dictionary2', 'print(dictionary2.printMeanings())\xa0', '#Finding Synonym using Dictionary1', 'print(dictionary1.synonym(""Archive""))', '#Finding Synonyms of word in Dictionary2', 'print (dictionary2.getSynonyms())', '#Finding antonyms using dictionary1', 'dictionary1.antonym(""Archive"")', '#Finding antonyms using dictionary2', 'print (dictionary2.getAntonyms())', '#Translating using Dictionary1', 'dictionary1.translate(""Archive"", ""hi"") #Translating to hindi', '#Translating words in dictionary2', 'print (dictionary2.translateTo(""hi""))'"
35,library_for_automating_visualization/,https://analyticsindiamag.com/complete-guide-to-visualizer-python-library-for-automating-visualization/,"['Data Visualization is considered to be one of the best ways to identify any anomaly, outlier, or if data follows a particular pattern. While looking at graphs and plots with our naked eyes we can clearly see what the data is trying to tell us. Data Visualization is the most important part of Story Telling about telling because it backs the story with visualizations that have a greater impact.', 'Python provides different modules/packages which are used for data visualization. We can create different visualizations like statistical visualizations, 3D visualization, etc. using different python packages and modules like seaborn, matplotlib, bokeh, etc. All python libraries provide us with different processes to create visualizations so each time we use a library we should what syntax to follow and what should be the code for different plots.', 'Visualizer is a python library that automates the process of visualization. It supports a large variety of graphs and plots which can easily be created using a single line of code. It allows creating visualizations of any individual relationship between multiple columns.', 'In this article, we will explore what are some of the plots and graphs which we can create using a visualizer.', 'We will start by installing visualizer using pip install visualizer.', 'In this article, we will be creating visualizations for which we will import visualizer and for the dataset, we will import seaborn to use a default dataset defined in seaborn.', 'from visualizer import Visualizer', 'import seaborn as sns', 'We will use seaborn to load the dataset named tips that contain attributes like total_bill, tip, etc. We will store this dataset into a data frame.', ""df = sns.load_dataset('tips')"", 'df', 'Now let us start creating different visualizations for different attributes.', 'Visualizer.create_count_plot(df=df, cat_col=""day"", annot=True)', 'Visualizer.create_pie_plot(df=df, cat_col=""sex"")', 'Visualizer.create_box_plot(df=df, num_col=""total_bill"", cat_col=\'sex\')', ""Visualizer.create_density_plot(df=df, num_1='tip', num_2='total_bill')"", ""Visualizer.create_scatter_plot(df=df, num_1='tip', num_2='total_bill')"", ""Visualizer.create_kde_plot(df=df, num_col='tip')"", ""Visualizer.create_ridge_plot(df=df, num_col='total_bill', cat_col='day')""]","'from visualizer import Visualizer'
 'import seaborn as sns', ""df = sns.load_dataset('tips')"", 'df', 'Visualizer.create_count_plot(df=df, cat_col=""day"", annot=True)', 'Visualizer.create_pie_plot(df=df, cat_col=""sex"")', 'Visualizer.create_box_plot(df=df, num_col=""total_bill"", cat_col=\'sex\')', ""Visualizer.create_density_plot(df=df, num_1='tip', num_2='total_bill')"", ""Visualizer.create_scatter_plot(df=df, num_1='tip', num_2='total_bill')"", ""Visualizer.create_kde_plot(df=df, num_col='tip')"", ""Visualizer.create_ridge_plot(df=df, num_col='total_bill', cat_col='day')"""
36,for_geographical_data_visualization/,https://analyticsindiamag.com/hands-on-tutorial-on-folium-python-library-for-geographical-data-visualization/,"['Geographical data is defined as the data which is relative to a certain location. As it is a location on earth we can represent it on a map. Representing geographical data on a map is easy using python libraries.\xa0\xa0', 'Python provides different open-source libraries for geographical data visualization. These libraries are easy to use and create highly interactive and visually appealing maps.\xa0', 'In this article, we will explore Folium, a python library which is used to create different types of geographical data visualizations. We will try and create different types of maps and markers on maps.\xa0', 'We will start by installing Folium using pip install folium.', 'We will import folium which contains all the functions that are required for geographical data visualization.', 'import folium', 'We will start exploring folium by creating a world map in just one line of code.', 'world_map = folium.Map()', 'world_map', 'Similarly, we can create maps for different locations by just passing the Longitude and Latitude of the respective location. Let us plot the Bangalore Map using folium. You can google the Latitude and Longitude of your respective location.', 'bang_map = folium.Map(location=[12.9716, 77.5946], zoom_start=12)', 'bang_map', 'Folium allows us to create a marker on the map for a particular location by using the marker function. Let us explore it by marking capitals of some countries on the world map.', 'cap = folium.Map(zoom_start=6)', ""folium.Marker(location=[28.7041, 77.1025 ], popup='Delhi').add_to(cap)"", ""folium.Marker(location=[39.9042, 116.4074], popup='Beijing').add_to(cap)"", ""folium.Marker(location=[55.7558, 37.6173], popup='Moscow').add_to(cap)"", 'Folium supports different types of map formats that clearly display different attributes like Roads, Rivers, etc. Let us explore different types of maps using coordinates of India.', 'First, we will explore the Stamen Toner map which is a high contrast Black & White map which helps in visualizing the river meanders and coastal zones clearly and then Stamen Terrain which shows hill shading and natural vegetation colors.', ""india_map =folium.Map(location=[20.5937, 78.9629 ],zoom_start=5, \xa0\xa0tiles='Stamen Toner')    "", 'india_map', ""india_map =folium.Map(location=[20.5937, 78.9629 ], zoom_start=5,  tiles='Stamen Terrain')"", 'india_map', 'You can see in the output images how clearly these tiles work to show different attributes of the geographical area. Similarly, you can explore many more tile styles that are there in folium.', 'Folium allows us to create interactive maps in just a single line of code and of different types and locations. You can zoom-in or zoom-out of the maps to get a clear picture of the location and the surroundings.', 'In this article, we explored the basics of folium, how we can create different maps with different styles in just a single line of code. We saw how we can create a marker on the map to notify the respective location. We saw how different parameters can change the view and style of the map.\xa0']","'import folium'
 'world_map = folium.Map()', 'world_map', 'bang_map = folium.Map(location=[12.9716, 77.5946], zoom_start=12)', 'bang_map', 'cap = folium.Map(zoom_start=6)', ""folium.Marker(location=[28.7041, 77.1025 ], popup='Delhi').add_to(cap)"", ""folium.Marker(location=[39.9042, 116.4074], popup='Beijing').add_to(cap)"", ""folium.Marker(location=[55.7558, 37.6173], popup='Moscow').add_to(cap)"", ""india_map =folium.Map(location=[20.5937, 78.9629 ],zoom_start=5, \xa0\xa0tiles='Stamen Toner')    "", 'india_map', ""india_map =folium.Map(location=[20.5937, 78.9629 ], zoom_start=5,  tiles='Stamen Terrain')"", 'india_map'"
37,library_for_interactive_visualizations/,https://analyticsindiamag.com/hands-on-tutorial-on-bokeh-open-source-python-library-for-interactive-visualizations/,"['Dashboards are collections of bars, charts, and graphs that help us visualize different attributes of a dataset. A dashboard works as a graphical user interface which helps us identify the key performance indicators relevant to the dataset or the particular business model. Python provides different open-source libraries that can help you create your own dashboard with your dataset. Today we will be talking about Bokeh which is an open-source python library for interactive visualizations for the modern web browsers.\xa0', 'Bokeh provides elegant, concise construction of versatile graphics, and affords high-performance interactivity over large or streaming datasets. It can be used for different purposes like creating interactive plots, dashboards, and even data-driven applications.', 'In this article we will discuss:', 'Like any other library, we need to install Bokeh for exploring it by pip install bokeh', 'We will import pandas for loading the dataset and will import different functions of bokeh as and when required.', 'import pandas as pd', 'from bokeh.plotting import figure, output_file, show', 'We will create a sales dashboard for which we need sales data of a company, here I will use a dataset which contains Sales of a company and different attributes on which it depends.', 'df = pd.read_csv(‘Advertising.csv’)', 'df', 'We will start by creating visualizations and later add them to a single dashboard. Starting with the scatter plots to find the relationship between Sales and the factors governing it.', 'from bokeh.plotting import figure, output_file, show', 'output_file(""p1.html"")', 'p1 = figure(plot_width=400, plot_height=400)', 'p1.circle(df[\'TV\'], df[\'Sales\'], size=2, color=""navy"", alpha=0.5)', 'show(p1)', 'As you can see here Bokeh not only creates visualization but also provides different functions as shown in the image, the visualization is highly interactive and downloadable.', 'Similarly, we will create the visualization for the other two attributes against the Sales.', 'output_file(""p2.html"")', 'p2 = figure(plot_width=400, plot_height=400)', 'p2.circle(df[\'Radio\'], df[\'Sales\'], size=2, color=""navy"", alpha=0.5)', 'show(p2)', 'output_file(""p3.html"")', 'p3 = figure(plot_width=400, plot_height=400)', 'p3.circle(df[\'Newspaper\'], df[\'Sales\'], size=2, color=""navy"", alpha=0.5)', 'show(p3)', 'As you can see here we have created three scatter plots of Sales(Dependent Variable) vs. Newspaper, TV, Radio( Independent Variable)', 'Next, we will create the distribution plots according to the ‘Sales’ of all the numerical attributes.', 'fig1 = figure(plot_width=400, plot_height=400)', ""fig1.vbar(x=df['Sales'], bottom=0, top=df['TV'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 color='blue', width=0.75,\xa0"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 legend='TV')"", '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0show(fig1)', 'fig2 = figure(plot_width=400, plot_height=400)', ""fig2.vbar(x=df['Sales'], bottom=0, top=df['Radio'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0color='red', width=0.75,\xa0"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0legend='Radio')"", 'show(fig2)', 'fig3 = figure(plot_width=400, plot_height=400)', ""fig3.vbar(x=df['Sales'], bottom=0, top=df['Newspaper'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0color='green', width=0.75,\xa0"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0legend='Newspaper')"", 'show(fig3)', 'Similarly, we can create different types of plots and graphs according to our requirements.\xa0', 'Next, we will design the layout and show these images in a single dashboard.', 'For getting all these images in a single layout we need to define the layout of the dashboard.', 'We will use the grid layout and visualize the dashboard in a grid format.', 'from bokeh.layouts import gridplot', 'grid = gridplot([[p1, p2, p3], [fig1, fig2, fig3]], plot_width=250, plot_height=250)', 'show(grid)', 'As you can see in the image we have created a dashboard in a grid layout, it clearly shows the relationship between Sales and the numerical on which it is dependent.', 'This is just a basic layout/Dashboard which is easily created and can help you start designing dashboards in Bokeh if you are a beginner.', 'Conclusion:']","'import pandas as pd'
 'from bokeh.plotting import figure, output_file, show', 'df = pd.read_csv(‘Advertising.csv’)', 'df', 'from bokeh.plotting import figure, output_file, show', 'output_file(""p1.html"")', 'p1 = figure(plot_width=400, plot_height=400)', 'p1.circle(df[\'TV\'], df[\'Sales\'], size=2, color=""navy"", alpha=0.5)', 'show(p1)', 'output_file(""p2.html"")', 'p2 = figure(plot_width=400, plot_height=400)', 'p2.circle(df[\'Radio\'], df[\'Sales\'], size=2, color=""navy"", alpha=0.5)', 'show(p2)', 'output_file(""p3.html"")', 'p3 = figure(plot_width=400, plot_height=400)', 'p3.circle(df[\'Newspaper\'], df[\'Sales\'], size=2, color=""navy"", alpha=0.5)', 'show(p3)', 'fig1 = figure(plot_width=400, plot_height=400)', ""fig1.vbar(x=df['Sales'], bottom=0, top=df['TV'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 color='blue', width=0.75,\xa0"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 legend='TV')"", '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0show(fig1)', 'fig2 = figure(plot_width=400, plot_height=400)', ""fig2.vbar(x=df['Sales'], bottom=0, top=df['Radio'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0color='red', width=0.75,\xa0"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0legend='Radio')"", 'show(fig2)', 'fig3 = figure(plot_width=400, plot_height=400)', ""fig3.vbar(x=df['Sales'], bottom=0, top=df['Newspaper'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0color='green', width=0.75,\xa0"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0legend='Newspaper')"", 'show(fig3)', 'from bokeh.layouts import gridplot', 'grid = gridplot([[p1, p2, p3], [fig1, fig2, fig3]], plot_width=250, plot_height=250)', 'show(grid)'"
38,create_your_own_dataset/,https://analyticsindiamag.com/faker-tutorial-a-python-library-to-create-your-own-dataset/,"['Faker is an open-source python library that allows you to create your own dataset i.e you can generate random data with random attributes like name, age, location, etc. It supports all major locations and languages which is beneficial for generating data based on locality.', 'Faker data can be used to tune machine learning models, for stress testing a model, etc. Depending upon your need you can generate data that best fits your demand. Faker data can also be used for learning purposes like performing different operations on different types of data types.', 'The datasets generated can also be used to tune the machine learning model, validate the model, and to test the model.', 'In this article we will:', 'In order to explore faker we need to install it using pip install faker.', 'We will explore different functions of faker so we will import faker also we will perform some operations on the dataset for which we need to import pandas.', 'Now we will explore different functions that are there in the Faker library, for this, we need to initiate the Faker function using a variable.', 'Now we will use this variable to generate different attributes.', 'We can generate information according to different regions and localities in different languages. We just need to mention the language we want. Let’s generate some data in the Japanese and Hindi language.', 'We can also create our own sentences using the sentence function and text function.', 'We can also create sentences by using our own defined word library which contains words of our choice and the faker will generate fake sentences using those words.', 'Other than generating names and addresses, we can generate whole profiles for different persons that do not exist. We will use the profile function to generate a fake profile of a person.', 'Faker can also generate the random dataset.', 'Now we will use the profile function and generate a dataset that contains profiles of 100 unique people that are fake. For this, we will also use pandas to store these profiles into a data frame. We will create these profiles in the Hindi language.', 'The dataset we have created contains different attributes like residence, location, website, etc. We can use this dataset according to our needs.\xa0', 'We have stored these profiles into a data frame so that we can perform operations on it, like Visualization, Analysis, etc.']","'from faker import Faker\nimport pandas as pd\xa0'
 'exp = Faker()', ""print('Name: ', exp.name())\nprint('Address: ',exp.address())\nprint('DOB: ',exp.date_of_birth())\n\n"", ""exp = Faker(['ja_JP', ‘hi_IN])\nfor i in range(5):\n \xa0\xa0 \xa0 \xa0 \xa0 print(exp.name())\n\n"", 'exp.text()\n\nexp.sentence()\n\n', ""words = ['Hello','Abhishek','all', 'are','where','why',]\nexp.sentence(ext_word_list=words)\n\n"", 'exp.profile()\n\n', ""exp = Faker(['hi_IN'])\ndata = [exp.profile() for i in range(100)]\ndf = pd.DataFrame(data)\ndf\n\n"""
39,for_gui_based_eda/,https://analyticsindiamag.com/autoplotter-tutorial-open-source-python-library-for-gui-based-eda/,"['Autoplotter is an open-source python library built on top of Dash which enables the user to do Exploratory Data Analysis using Graphical User Interface. Dash is a python framework built mainly on top of Flask and Plotly.js and used to create web apps. It is open-source and runs the app on the browser window.\xa0', 'Autoplotter is just like a dashboard in which you can load any dataset and start Exploratory Data Analysis. It supports different types of visualizations, statistical analysis of data, plotting according to your selected features, and creates all the major plots. The best thing about Autoplotter is that you don’t have to waste your time writing code for data analysis and creating different kinds of visual analysis, autoplotter can do all this in just a few lines of code and that too really fast and with a visually appealing Graphical Interface.', 'In this article, we will explore Autoplotter through the following points and see how it saves time in Exploratory Data Analysis.', 'Like any other python library, we will install Autoplotter using pip install autoplotter.', 'a. Importing Autoplotter\xa0', 'b. Loading the Dataset', 'We will be using a Car Design Dataset which contains different attributes of car design companies. We will load this dataset using pandas and then use the autoplotter for further Exploratory Data Analysis.', 'c. Loading the Autoplotter', 'Autolpotter creates the app for Exploratory Data Analysis in just one line of code and that too in just seconds.', 'This is the basic layout of the application created, here you can clearly see that the app contains different sections like Data Exploration and Plots. Also, we can see that every attribute has a filter applied by which we can sort it in ascending and descending order and also see the missing or NaN values.\xa0', 'Exploring different sections of the application:', 'Data Exploration tab allows us to explore the data and mainly the univariate data analysis. It contains three sections.', 'a. Data Distribution', 'In data distribution, we can do univariate or bivariate data analysis to know the distribution of data. It supports different kinds of plots and graphs which are already available and can be selected through different widgets in the application.\xa0', 'It provides us with a clear view of all the attributes which are present in the dataset.', 'b. Statistical Analysis', 'Statistical Analysis is just like the describe function in python, it gives us a clear picture of the statistical properties of the dataset including mean, median, quartiles, outliers, etc.', 'c. Association', 'This third portion allows us to find the association of the different attributes of the dataset. The best thing about autoplotter is the widgets which allow you to select different data points that you want to analyze.', 'In association, we can find the correlation between any two data points as you can see in the below image.', 'These three are the main parts of the Data Exploration tab.', 'Plots, as the name suggests, are used to create different plots here you can select any two data points and plot a variety of graphs in between them.\xa0', 'All the graphs plotted can be manipulated using different parameters that are already loaded in the app. You don’t need to worry about the coding as Autolplotter is fully loaded with all the types of charts and plots and that too highly informative and visually appealing.', 'Other than the different charts and plots, auto plotter also gives you the choice of selecting the theme of your choice as you can see below in the image.', 'These plots also help you in finding if there are any irrelevant or missing data points in the dataset, as shown in the image below.', 'In the histogram above, we can clearly see that the third bar is labelled as ‘?’ which is not a valid data point so we need to remove it in the data cleaning part.', 'All these are the parts of Autoplotter and it makes your EDA process a lot faster and hassle-free']","'from autoplotter import run_app\xa0\nImport pandas as pd\n\n'
 'df = pd.read_csv(‘car_design.csv’)', 'run_app(df)\n\n', '\n\n', '\n\n', '\n\n', '\n\n', '\n\n'"
40,library_data_scientist_toolkit/,https://analyticsindiamag.com/beginners-guide-numpy-must-python-library-data-scientist-toolkit/,"['', 'NumPy, which stands for Numerical Python, is a fundamental library for mathematical computations. This library can be used for different functions in Linear algebra, Matrix computations, Fourier Transforms etc. In Python, array.array\xa0function is limited to only one dimension which can be substituted with NumPy for multi dimensional operations. One can compute the multidimensional array functions like the figure below,', '', 'In this article we will go through some of the important built-in function of NumPy to understand the logic and mathematics behind it. By using NumPy, you can speed up your workflow, and interface with other packages in the Python ecosystem, like scikit-learn, that use NumPy under the hood. Let us dive into computing the math behind these.', 'An array is a set of elements of a data type. The shape of the NumPy array can be defined with an enclosed tuple of positive integers. The order of a NumPy array is given by the dimensions. We can index and slice a particular element from the array, and initialize an array with square brackets like below, ', '', 'Like Python lists, the Numpy array can be indexed and sliced with the right number of parameters. It can be done as described below,', '', 'Index slicing is as it sounds, slicing one or multi dimensional array into different subsets like below example,', '', 'With the use of negative numbers, we can compute the functions in the reverse order like the example below,', '', 'Let us consider a multi-dimensional array and try to slice it. ', '', 'We can implement the following code to slice it, ', '', 'After these steps, matrix multiplication is important, which can be done in two ways. We can either use the dot function, which applies a matrix-matrix, matrix-vector, or inner vector multiplication to its two arguments:', '', 'Functions such as finding the inverse of a matrix and determinant of a matrix can also be done with NumPy like below,', '', 'Most of the times it is useful to store datasets in NumPy arrays. It provides a number of functions to calculate statistics of datasets in arrays. Let us calculate some properties of the matrix B,']",
41,seaborn_pythons_visualization_library/,https://analyticsindiamag.com/a-beginners-guide-to-seaborn-pythons-visualization-library/,"['Data Visualization is an accessible way to represent the patterns, outliers, anomalies, etc. that are available in data by plotting graphs and charts. Data Visualization is a powerful tool because as soon as the human eyes see a chart or plot they try to find out a pattern in it because we get attracted to colours and patterns. Python provides different visualization libraries but Seaborn is the most commonly used library for statistical data visualization.\xa0', 'It can be used to build almost each and every statistical chart. It is built on matplotlib which is also a visualization library. Seaborn provides highly attractive and informative charts/plots. It is easy to use and is blazingly fast. Seaborn is a dataset oriented plotting function that can be used on both data frames and arrays. It enhances the visualization power of matplotlib which is only used for basic plotting like a bar graph, line chart, pie chart, etc.', 'Through this article, we will discuss the following points in detail:\xa0\xa0', 'Before using seaborn we need to install it using pip install seaborn.', 'Here, we will download a dataset named “tips’ from the online repository, or by using Seaborn’s load_dataset() function. This dataset contains different attributes like total_bill, tips, smoker, etc.', 'Let us start by importing the important libraries and the dataset.', 'Plotting different statistical graphs:', '1. lmplot', '‘lmplot’ is the most basic plot which shows a line along a 2-dimensional plane and is used to see the correlation between two attributes plotted against the x-axis and y-axis. Here we will plot Sales against TV.\xa0', 'Seaborn also allows you to set the height, colour palette, etc. properties for the plot generated.', '2. kdeplot', 'A Kernel Density Estimate plot is used to visualize the Probability density distribution of univariate data. In simple terms, we can use it to know the spread/distribution of the data.', '3. Scatterplot', 'Scatterplots are similar to lineplot/lmplot, the difference is that it only shows the scattering of the two attributes without trendline. It is also used for finding the relation between two attributes.', '4. Distplot', 'Distplot is the most convenient way of visualizing the distribution of the dataset and the skewness of the data. It is a combination of kdeplot and histograms.', '5. Barplots', 'Barplots are the most common type of visualization and mostly used for showing the relationship between numeric and categorical data. Barplots can be plotted both horizontally and vertically as required.', '6. FacetGrids\xa0', 'FacetGrids are used to draw multiple instances of the same plot on different subsets of the dataset. In this visualization, we take a data frame as an input and the names of variables for rows and columns.', 'To draw facet grids we need to import matplotlib as well. Let us visualize the dataset using Histogram FacetGrids.', '\n7. Box-Plot', 'We use box-plots to graphically display the data according to its quartiles. With box-plot, we can easily identify the median, any outlier if data has and the range of the data points.', 'Here we will visualize the tip that is paid on different days of a week.', '8. Violin Plots', 'Violin plots are the combination of the KDE plot and box-plot. It is used to visualize the numerical distribution of the data.', '9. Heatmaps', 'Heatmaps are used to display the correlations of different numerical attributes in a dataset. In heatmaps, colour scheme plays an important role in visualizing whether the relationship is positive or negative.\xa0', 'For creating a heatmap we will create a Correlation matrix and pass it to the heatmap parameter. We will also set the annotation to true so the value of the relationship is also visible.', '10. Jointplots', 'Jpintplots are useful when we want to visualize the relationship between two variable as well as their univariate relationship. Jointplots are of many types and we can define the kind we want by passing the value in the “kind” parameter.', 'These are some of the basic plots which we can visualize using Seaborn, and are helpful in data analysis. Seaborn has the advantage of manipulating the graphs and plots by applying different parameters. Some of the important parameters are:', 'Other than these properties all the graphs have some of their internal properties which can be altered accordingly.']","'import pandas as pd\nimport seaborn as sns\ndf = sns.load_dataset(""tips"")\ndf\n\n'
 'sns.lmplot(x=""total_bill"", y=""tip"", data=df, height=4, palette=""dark"")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n', ""sns.kdeplot(df['tip'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"", 'sns.scatterplot(x=""total_bill"", y=""tip"", data=df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n', ""sns.distplot(df['total_bill'])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"", 'sns.barplot(x=""sex"", y=""total_bill"", data=df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n', 'import matplotlib.pyplot as plt\na = sns.FacetGrid(df, col=""time"",\xa0 row=""sex"")\na.map(plt.hist, ""total_bill"")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n', 'sns.boxplot(x=""day"", y=""tip"",\xa0 data=df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n', 'sns.violinplot(x=""day"", y=""total_bill"",\xa0 data=df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n', 'sns.heatmap(df.corr(), annot=True)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n', 'sns.jointplot(x=""total_bill"", y=""tip"", data=df, kind=""reg"")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
42,wrangle_complex_ml_datasets/,https://analyticsindiamag.com/meet-meerkat-a-new-data-library-to-wrangle-complex-ml-datasets/,"['Recently, researchers at Stanford University launched a new data library called Meerkat for working with complex machine learning datasets. The source code of the project is available on GitHub.\xa0', 'Data is the oxygen for machine learning. From training and validation data to future predictions, embeddings and metadata, it drives all parts of the machine learning development process. However, organising and managing data is challenging.', 'To that end, Stanford researchers have proposed a new Python library to help researchers and ML practitioners wrangle data. Data wrangling is a process of cleaning and unifying messy and complex datasets for easy access and analysis.\xa0', 'In a Notion Press blog, ‘Meerkat: Datapanels for machine learning,’ Stanford researchers Sabri Eyuboglu, Arjun Desai and Karan Goel talked about a few areas where Meerkat could solve the data complexity in the machine learning lifecycle.', 'Meerkat provides the DataPanel abstraction. The DataPanel facilitates interactive dataset manipulation, where it can house diverse data modalities and lets you evaluate models carefully with Robustness Gym. “We built DataPanels like DataFrames because they are naturally interactive and work seamlessly across development contexts: Jupiter Notebooks, Python scripts, and Streamlit,” the researchers said.', 'The goal is to make Meerkat DataPanel an interactive data substrate for modern machine learning across the machine learning lifecycle.\xa0', 'Besides Robustness Gym, Meerkat can also be integrated into other popular benchmark datasets and works well with existing libraries and tools like WILDS, Huggingface Datasets, DOSMA, Streamlit.', 'The data structures typically fall into two categories: those supporting complex data types and multiple modalities (PyTorch Dataset, Tensorflow Dataset), and those that support manipulation and interaction (Pandas DataFrame). “With the Meerkat DataPanel, we support all of these desiderata in one data structure,” said the researchers.', 'Advantages:\xa0', 'Comparing Meerkat with other machine learning data structures (Source: Notion Press)\xa0', 'The researchers ran an experiment to detect pneumothorax (a collapsed lung) in chest X-rays. For developing a model for this task, the researchers encountered various types of data — X-ray images to structured metadata to embeddings extracted from a trained model.\xa0', 'Here, Meerkat’s DataPanel (a columnar data structure) could house all these data types under one roof. “Keeping them together enables quicker model iteration, fine-grained error analysis, and easier data exploration and inspection,” said the Stanford researchers. The codes used for running this experiment are available here.\xa0', 'Meerkat addressed the desiderata by facilitating the inspection and manipulation of datasets that combine multiple complex data types. Meerkat provides high-level data abstractions because its data structures are written in Python and have few low-level optimisations, unlike Pandas, NumpY or Apache Arrow.\xa0']",
43,for_automated_feature_engineering/,https://analyticsindiamag.com/introduction-to-featuretools-a-python-framework-for-automated-feature-engineering/,"['Featuretools is an open-source Python library designed for automated feature engineering. It was developed by the Feature Labs. It enables the creation of new features from several related data tables. Feature selection techniques can then be used to choose appropriate features from them and then data scientists can proceed with model creation.', 'Image source: GitHub', 'Before moving to the practical implementation of Featuretools, let us have a quick overview of some essential concepts for performing automatic feature engineering using Featuretools.', 'An entity is just a table of data or a Pandas data frame in Python code. The observations are recorded as rows while its columns denote different features. An entityset is a collection of related entities. The creation of new features becomes easier using an entityset since it shows multiple tables and relationships between them – all in one place. Entities and entitysets are independent of the underlying data so these abstractions can be applied to any dataset.', 'Featuretools can infer the types of variables on its own. However, for cases such as Boolean variable’s values stored as integers(0 or1), we need to explicitly identify and assign the datatype such as variable_types.Boolean. Visit this page to know more about the variable types that Featuretools deals with.', 'The relationship is not a distinguishing feature of Featuretools, it is the same abstract concept used in relational database management systems (RDBMS). A relationship can be of various types such as one-to-one, one-to-many, many-to-one and many-to-many. A parent-child relationship among datasets is an example of a one-to-many relationship in which a parent dataset can be related to multiple other datasets, each of which is called a child dataset.', 'An operation applied to a data frame for creating new features is termed a ‘feature primitive’. It involves simple computations which can be combined for the creation of complex features. The two major feature primitives that we have used in the practical implementation are aggregation and transformation.', 'Aggregation: It groups children of a parent table for statistical computation such as minimum, maximum, mean and standard deviation across them.', 'Transformation: It is an operation performed on one or more columns of one table, e.g. computing the difference between two columns’ values.', 'Refer to this page for detailed information on feature primitives.', 'DFS is a method used by Featuretools for creating new features. To perform DFS, dfs() function of the featuretools library is used. It takes as input an entityset, a target entity, where the new features should be stored, aggregation and transformation primitives to be used for feature creation, and other parameters. Setting the ‘features_only’ parameter of dfs() to True only creates the features’ names but does not compute its actual values (known as feature matrix).\xa0', 'Here’s a demonstration of implementing automated feature engineering using Featuretools for a supervised machine learning classification task that aims to predict whether or not a loan application of a financial institution named ‘Home Credit’ will default on the loan (‘Default’ means the client fails to repay the loan). The ‘Home Credit Default Risk’ dataset used here is available on Kaggle (weblink to download it). The code has been implemented using Python 3.7.10 and featuretools 0.23.2 versions. Step-wise explanation of the code is as follows:', '!pip install featuretools', 'es.plot()', 'Sample condensed output:', '16. Record the feature primitives\xa0', 'Display the records with transformation primitive.', ""primitives[primitives['type'] == 'transform'].head(10)"", '17. Build new features using default primitives of featuretools.', 'Specify the default aggregation and transformation primitives.', 'Create new features from a list of relationships and a dictionary of entities using dfs() method.', '18. Display some of the newly generated features', 'feature_names[1050:1070]']","'!pip install featuretools'
 'There are 32 Boolean variables in the application data.', 'There are 1 Boolean variables in the previous data.', 'es.plot()', ""primitives[primitives['type'] == 'transform'].head(10)"", 'Built 2089 features', 'feature_names[1050:1070]'"
44,2d_3d_scientific_data/,https://analyticsindiamag.com/guide-to-mayavi-a-python-tool-for-visualizing-and-plotting-2d-3d-scientific-data/,"['Mayavi is a cross-platform library and application for 2D and 3D plotting and interactive visualization of scientific data using Python. It leverages the power of Visualization Toolkit (VTK) without requiring the users to have its prior knowledge. It acts as a clean and lucid Python interface that enables plotting complex figures with a few code lines; some of them can even be plotted using one-liners. Besides, it also provides an object-oriented interface for the same purposes.', 'Similar to VTK, Mayavi makes use of a pipeline architecture. Data in Mayavi is stored in a ‘data source’, which is nothing but a data file or a data object describing the data to be used for visualization and plotting. This data can be stored in a VTK-supported file format. Alternatively, it can be generated as sequences like NumPy arrays using simple scripting API such as mlab. It is then processed using Filters and can be visualized using various Modules for visualization. Since there can be different ways of visualizing some given data, ‘data source’ and ‘modules’ have been kept as separate sections of the library to visualize different entities from various perspectives by applying different modules to a common data source. The area where visualization is performed in a Mayavi application is termed as a ‘scene’.', 'Here’s a demonstration of plotting various 3D figures using easy-to-use built-in functions provided by Mayavi library. The following code implementation referenced the official examples of Mayavi and verified in Google Colab with\xa0 Python 3.7.10 and Mayavi 4.7.2 versions. Step-wise explanation of the code is as follows:', '!pip install mayavi', 'mlab.init_notebook()', 'On successful execution of the above line of code, we get the following output:', 'Notebook initialized with ipy backend.', 'mlab.test_points3d()', '\xa0 IMP NOTE: After every plot, remember to clear the current figure using\xa0mayavi.mlam.clf() method, otherwise the next plot will get overlapped with the current\xa0one.', '\xa0 mlab.clf()', 'mlab.test_plot3d()', 'mlab.test_quiver3d()', 'mlab.test_surf()', 'mlab.test_triangular_mesh()', 'mlab.test_contour3d()', 'mlab.test_barchart()', 'mlab.test_flow()', 'mlab.test_mesh()', 'mlab.test_fancy_mesh()', 'mlab.test_mesh_sphere()', 'For more attractive spherical mesh,', 'mlab.test_mesh_mask_custom_colors()', 'NOTE: As mentioned in step (6), if mlab.clf() is used to clear current figure, successive plots will get overlapped. This technique can be used to mix two or more 3D plots as in the following step.', 'IMP NOTE: While executing the Google colab code having multiple Mayavi plots, it is recommended to run the cells manually one-by-one. Allowing the cells to get executed independently by selecting ‘Run all’ or ‘Restart and run all’ option may sometimes result in output plots that do not match the input code since a cell may get executed before the previous figure gets cleared.']","'!pip install mayavi'
 'mlab.init_notebook()', 'Notebook initialized with ipy backend.', 'mlab.test_points3d()', 'mlab.clf()', 'mlab.test_plot3d()', 'mlab.test_quiver3d()', 'mlab.test_surf()', 'mlab.test_triangular_mesh()', 'mlab.test_contour3d()', 'mlab.test_barchart()', 'mlab.test_flow()', 'mlab.test_mesh()', 'mlab.test_fancy_mesh()', 'mlab.test_mesh_sphere()', 'mlab.test_mesh_mask_custom_colors()'"
45,an_interactive_visual_discovery/,https://analyticsindiamag.com/python-guide-to-lux-an-interactive-visual-discovery/,"['A picture is worth a thousand words, even more so when it comes to data-centric projects. Data exploration is the first step in any machine learning project, and it is pivotal to how well the rest of the project turns out. Although libraries like Plotly and Seaborn provide a huge collection of plots and options, they require the user to first think about how the visualization should look like and what to visualize in the first place. This is not conducive to data exploration and just contributes to making it the most time-consuming part of the machine learning life cycle. Well, what if you could get visualizations recommended to you? Lux is a Python package created by the folks at RiseLabs that aims to make data exploration easier and quicker with its simple one-line syntax and visualization recommendations. As the developers put it “Lux is built on the philosophy that users should always be able to visualize anything they want without having to think about how the visualization should look like“.\xa0', 'In Lux, you don’t explicitly create plots; you simply specify your analysis intent, i.e., what attributes/subset interest you; Lux takes care of the rest. Apart from this, Lux is tightly integrated with Pandas and can be used without modifying any code with just one import statement. It preserves the Pandas data frame semantics, so all the commands from the Pandas’s API work in Lux as expected.', 'Install Lux from PyPI', 'pip install lux-api', 'Install and activate the Lux notebook extension (lux-widget) included in the package.', 'Note: Lux does not work in Colab because Colab doesn’t support custom widgets yet.\xa0', 'Check other methods of installation here.', 'Enable Lux by importing it.', 'That’s it. Now every time you print a data frame, you’ll get a toggle option to view the Lux visualizations. Let’s load some data and try this out.\xa0', 'This creates several plots divided into three tabs:', 'In addition to simply visualizing the intermediate steps of data exploration Lux has a simple language for specifying your analysis intent, i.e., attributes and values you’re interested in. There are two ways of specifying intent in Lux:', 'Provides simple string-based description to specify the intent of analysis conveniently.', 'Let’s say value_eur is an attribute of interest:', 'Lux recommends a number of interesting plots in two tabs:', 'Another thing noted here is that Lux doesn’t simply create all possible plots; it determines the channel mappings and plot type based on a set of best practices.', 'If there are multiple attributes of interest, they can be mentioned in the form of a list. Let’s say we have two attributes of interest: overall and value_eur.', 'This creates recommendations depicting the effect other attributes and filters have on the specified attributes.\xa0', 'There is also a new tab called Generalize, it recommends plots with one of the specified attributes removed.\xa0', 'Let’s say we are only interested in midfielders.', 'This creates the same correlation, distribution, and occurrence plot as before but with only midfielder data.', 'Multiple values of interest can be specified by using the | notation. Let’s say we are interested in midfielders and defenders.', 'df.intent = [""Position=Midfielder|Defender""]', 'There’s only so much one can accomplish with string-based intent specifications, lux.Claus offers a more complex and expressive way of specifying intent. Additionally, it allows us to override auto-inferred details about the plots, such as the attribute’s default axis or the aggregation function used for the quantitative attributes.', 'The lux.Clause equivalent for specifying interest in overall would be:\xa0', ""df.intent = [lux.Clause(attribute='overall')]"", 'Let’s say that we want to create plots with overall on the y-axis.', ""df.intent = [lux.Clause(attribute='overall', channel='y')]"", 'Or want to use sum as the aggregation function instead of mean.', 'df.intent = [""value_eur"",lux.Clause(""overall"",aggregation=""sum"")]', 'A Vis object indicates an individual visualization displayed in Lux. To generate a Vis, a source data frame and the intent of analysis are needed as inputs and this intent is expressed using the same intent specification as specified before using either intent or lux.Clause. For example, here, we describe our intent for visualizing the overall attribute on the dataframe df.', 'You can easily replace the Vis‘s data source and the query’s intent without changing its definition. For example, to represent the overall distribution on the subset of data with forwards with a bin size of 50.', 'You can learn more about Vis here.', 'The visualizations can be stored as stand-alone HTML files. The default file name is export.html, you can optionally specify the HTML filename in the input parameter.', ""df.save_as_html('overall_vs_value.html')"", 'Vis objects can also be exported to code in Altair or as Vega-Lite.', 'vis.to_Altair()', 'vis.to_VegaLite()', 'You can find more information about saving and exporting visualizations here.', 'Code for the above implementation is available in this Jupyter notebook.']","'pip install lux-api'
 'intent', 'lux.Clause', 'value_eur', 'value_eur', 'overall', 'value_eur', 'Goals = 1', '|', 'df.intent = [""Position=Midfielder|Defender""]', 'lux.Clause', 'lux.Claus', 'lux.Clause', ""df.intent = [lux.Clause(attribute='overall')]"", 'overall', ""df.intent = [lux.Clause(attribute='overall', channel='y')]"", 'sum', 'mean', 'df.intent = [""value_eur"",lux.Clause(""overall"",aggregation=""sum"")]', 'Vis', 'Vis', 'Vis', 'intent', 'lux.Clause', 'overall', 'df', 'Vis', 'Vis', ""df.save_as_html('overall_vs_value.html')"", 'vis.to_Altair()', 'vis.to_VegaLite()'"
46,toolkit_for_outlier_detection/,https://analyticsindiamag.com/guide-to-pyod-a-python-toolkit-for-outlier-detection/,"['PyOD is a flexible and scalable toolkit designed for detecting outliers or anomalies in multivariate data; hence the name PyOD (Python Outlier Detection). It was introduced by Yue Zhao, Zain Nasrullah and Zeng Li in May 2019 (JMLR (Journal of Machine learning) paper).', 'Before going into the details of PyOD, let us understand in brief what outlier detection means.', 'Outliers in data analysis refer to those data points which differ significantly from the majority of observations or do not conform to the trend/pattern followed by them. The process of identifying such suspicious data points is known as outlier detection. Detecting fraudulent transactions in the banking sector is an example of outlier detection. Following are some of our useful articles for detailed information on outlier detection:', 'PyOD is an open-source Python toolbox that provides over 20 outlier detection algorithms till date – ranging from traditional techniques like local outlier factor to novel neural network architectures such as adversarial models or autoencoders. The complete list of supported algorithms is available here.\xa0', 'Here’s a demonstration of applying eight different outlier detection algorithms using PyOD library and comparing their visualization results. The code demonstrated here is tested with Google Colab having Python 3.7.10 and PyOD 0.8.7 versions.\xa0', 'Following models have been included in the demonstration:', 'Step-wise explanation of the code is as follows:']","' !pip install --upgarde pod\n !pip install combo '
 ' from __future__ import division\n from __future__ import print_function\n import os\n import sys\n from time import time\n import numpy as np\n from numpy import percentile\n import matplotlib.pyplot as plt\n import matplotlib.font_manager ', ' from pyod.models.abod import ABOD\n from pyod.models.cblof import CBLOF\n from pyod.models.iforest import IForest\n from pyod.models.knn import KNN\n from pyod.models.lof import LOF\n from pyod.models.ocsvm import OCSVM\n from pyod.models.pca import PCA ', ' num_samples = 500\n out_frac = 0.30 ', ' clusters_separation = [0]\n x, y = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))\n""""""\n(1 - fraction of outliers) will give the fraction of inliers; multiplying  it with the total number #of samples will give number of inliers\n""""""\n num_inliers = int((1. - outl_frac) * num_samples)\n""""""\nMultiply fraction of outliers with total number of samples to compute number of outliers\n""""""\n num_outliers = int(outl_frac * num_samples)\n""""""\nCreate ground truth array with 0 and 1 representing outliers and inliers respectively\n""""""\n ground_truth = np.zeros(num_samples, dtype=int)\n ground_truth[-num_outliers:] = 1 ', "" print('No. of inliers: %i' % num_inliers)\n print('No. of outliers: %i' % num_outliers)\n print('Ground truth arrayy shape is {shape}. Outlier are 1 and inlier are  \n 0.\\n'.format(shape=ground_truth.shape))\n print(ground_truth) "", "" rs = np.random.RandomState(42)\xa0 #random state\n #dictionary of classifiers\n clf = {\xa0\xa0\xa0\xa0\n \xa0\xa0\xa0\xa0'Angle-based Outlier Detector (ABOD)':\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0ABOD(contamination=out_frac),\n \xa0\xa0\xa0\xa0'Cluster-based Local Outlier Factor (CBLOF)':\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0CBLOF(contamination=out_frac,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0check_estimator=False, random_state=rs),\n \xa0\xa0\xa0\xa0'Isolation Forest': IForest(contamination=out_frac,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0random_state=rs),\n \xa0\xa0\xa0\xa0'K Nearest Neighbors (KNN)': KNN(\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0contamination=out_frac),\n \xa0\xa0\xa0\xa0'Average KNN': KNN(method='mean',\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0contamination=out_frac),\n \xa0\xa0\xa0\xa0'Local Outlier Factor (LOF)':\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0LOF(n_neighbors=35, contamination=out_frac),\n \xa0\xa0\xa0\xa0'One-class SVM (OCSVM)': OCSVM(contamination=out_frac),\n \xa0\xa0\xa0\xa0'Principal Component Analysis (PCA)': PCA(\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0contamination=out_frac, random_state=rs),\n } "", "" for i, classifier in enumerate(clf.keys()):\n \xa0\xa0\xa0\xa0print('Model', i + 1, classifier) "", ' Model 1 Angle-based Outlier Detector (ABOD)\n Model 2 Cluster-based Local Outlier Factor (CBLOF)\n Model 3 Isolation Forest\n Model 4 K Nearest Neighbors (KNN)\n Model 5 Average KNN\n Model 6 Local Outlier Factor (LOF)\n Model 7 One-class SVM (OCSVM)\n Model 8 Principal Component Analysis (PCA) ', ' for i, offset in enumerate(clusters_separation):\n \xa0\xa0\xa0\xa0np.random.seed(42)\n \xa0\xa0\xa0\xa0# Data generation\n \xa0\xa0\xa0\xa0X1 = 0.3 * np.random.randn(num_inliers // 2, 2) - offset\xa0 #inliers data\n \xa0\xa0\xa0\xa0X2 = 0.3 * np.random.randn(num_inliers // 2, 2) + offset\xa0 #outlier data\n #Build an array having X1 and X2 using numpy.r_\n \xa0\xa0\xa0\xa0X = np.r_[X1, X2]\n \xa0\xa0\xa0\xa0# Add outliers to X array\n \xa0\xa0\xa0\xa0X = np.r_[X, np.random.uniform(low=-6, high=6, size=(num_outliers, 2))]\n""""""\nnumpy.random.uniform() draws samples from the uniform distribution of inliers\xa0and outliers\n""""""\n \xa0\xa0\xa0# Fit the models one-by-one\n \xa0\xa0\xa0\xa0plt.figure(figsize=(15, 12))\n #For each classifier to be tested\n \xa0\xa0\xa0\xa0for i, (classifier_name, classifier) in enumerate(clf.items()):\n #fit the classifier to data X\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0classifier.fit(X)\n \xa0\xa0\xa0\xa0\xa0\xa0#compute confidence score\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0scores_pred = classifier.decision_function(X) * -1\n \xa0\xa0\xa0\xa0\xa0#make prediction using the classifier\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0y_pred = classifier.predict(X)\n #compute percentile rank of the confidence score\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0threshold = percentile(scores_pred, 100 * out_frac)\n""""""\ncompute number of errors from difference between predicted and ground \ntruth values\n""""""\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0num_errors = (y_pred != ground_truth).sum()\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0# plot the levels lines and the points\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Z = classifier.decision_function(np.c_[x.ravel(), y.ravel()]) * -1\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Z = Z.reshape(x.shape)\n \xa0\xa0\xa0\xa0\xa0\xa0#2 rows having 4 subplots each\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0subplot = plt.subplot(2, 4, i + 1)\n""""""\nplot filled and unfilled contours using contourf() and contour() respectively\n""""""\n \xa0\xa0\xa0\xa0\xa0\xa0subplot.contourf(x, y, Z, levels=np.linspace(Z.min(), threshold, 7),\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0cmap=plt.cm.Blues_r)\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0a = subplot.contour(x, y, Z, levels=[threshold],\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0linewidths=2, colors=\'red\')\n \xa0\xa0\xa0\xa0\xa0\xa0#for learned decision function\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0subplot.contourf(x, y, Z, levels=[threshold, Z.max()],\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0colors=\'orange\')\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0#for true inliers\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0b = subplot.scatter(X[:-num_outliers, 0], X[:-num_outliers, 1], \n        c=\'white\',s=20, edgecolor=\'k\')\xa0\xa0\xa0\n \xa0\xa0\xa0\xa0\xa0\xa0#for true outliers\n \xa0\xa0\xa0\xa0\xa0\xa0c = subplot.scatter(X[-num_outliers:, 0], X[-num_outliers:, 1], \n       c=\'black\',s=20, edgecolor=\'k\')\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0subplot.axis(\'tight\')\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0#legend of the subplots\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0subplot.legend(\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0[a.collections[0], b, c],\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0[\'learned decision function\', \'true inliers\', \'true outliers\'],\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0prop=matplotlib.font_manager.FontProperties(size=10),\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0loc=\'lower right\')\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0# X-axis label\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0subplot.set_xlabel(""%d. %s (errors: %d)"" % (i + 1, classifier_name, \n         num_errors))\n \xa0\xa0\xa0\xa0\xa0#marking limits of both the axes\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0subplot.set_xlim((-7, 7))\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0subplot.set_ylim((-7, 7))\n \xa0\xa0\xa0#layout parameters\n \xa0\xa0\xa0\xa0plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)\n #centered title to be given to the figure\n \xa0\xa0\xa0\xa0plt.suptitle(""Outlier detection by 8 models"")\n plt.show() \xa0 \xa0 #display the plots '"
47,plotting_and_mesh_analysis/,https://analyticsindiamag.com/hands-on-guide-to-pyvista-a-python-package-for-3d-plotting-and-mesh-analysis/,"['PyVista (formerly known as ‘vtki’) is a flexible helper module and a high-level API for the Visualization Toolkit (VTK). It is a streamlined interface for the VTK, enabling mesh analysis and plotting 3D figures using Python code. It was introduced by C. Bane Sullivan and Alexander A. Kaszynski in May 2019 (research paper).\xa0', 'Before going into the details of PyVista, let us have a brief overview of VTK.', 'VTK is an open-source software developed for image processing, dealing with 3D computer graphics and scientific visualization tasks. It provides a wide range of 2D and 3D plotting capabilities as well as widgets for 3D interaction. This cross-platform toolkit supported by the Kitware team can run on Windows, Mac, Linux and Unix.\xa0', 'Visit the official website for detailed information on VTK.', 'VTK is a state-of-the-art toolkit comprising a C++ class library and interpreted interface layers including Java, Python and Tcl/Tk. It combines the execution speed of C++ with the fast prototyping capabilities of Python. However, applications as simple as creating a mesh object requires a lengthy code when dealt with using VTK while the same task can be done with just a few lines of more Pythonic and straightforward code using PyVista. PyVista wraps the VTK through NumPy library, and direct array access enabled through several classes and methods. It facilitates visual integration of spatial datasets, fast-paced prototyping and mesh analysis.\xa0', 'Here, we demonstrate some use cases of PyVista which references official examples. The implementation is done using Google Colab with Python 3.7.10 and PyVista 0.29.0 versions. For each of the examples, the initial two common steps are as follows:', 'IMP NOTE: Failing to set up a headless display as done in this step may cause your ', 'Colab session to get accidentally crashed due to lack of some visualization\xa0', '\xa0dependencies installed.', 'IMP NOTE: If you miss to specify ‘notebook=True’ parameter in Plotter() method, the code cell may keep on executing for long and your Colab session ma get accidentally crashed.', 'img = examples.download_cow()', 'edge = mesh.extract_feature_edges(20)', 'Where, 20 is the feature angle in degrees (default is 30 degrees)', 'm = examples.download_horse()']","'img = examples.download_cow()'
 'edge = mesh.extract_feature_edges(20)', 'm = examples.download_horse()'"
48,learning_with_3d_data/,https://analyticsindiamag.com/hands-on-guide-to-pytorch3d-a-library-for-deep-learning-with-3d-data/,"['Facebook AI’s PyTorch 3D is a python library to deal with 3D data in deep learning. It is based on PyTorch tensors and highly modular, flexible, efficient and optimized framework, which makes it easier for researchers to experiment with and impart scalability to big 3D data. PyTorch 3D framework contains a set of 3D operators, batching techniques and loss functions(for 3D data) that can be easily integrated with existing deep learning systems through its fast and differentiable API’s. The key features of PyTorch 3D are as follows:', 'You can cover the theoretical aspect of PyTorch 3D through our previous article on PyTorch 3D. In this article, we will cover some Python demos of PyTorch 3D.', 'Core Components in CodeBase', 'Overview of components in the codebase is shown below. The foundation layer consists of data structures for 3D data, data loading utilities and composable transforms. The data structures in particular enable the operators and loss functions in the second layer to efficiently support heterogeneous batching.', 'Installation', 'Install PyTorch 3D through these commands below:', 'Demo – Deform source mesh to target mesh', 'In this demo, we will deform an initial generic shape to fit or convert it to a target. It is divided into four parts mainly:', 'Now, load the target image as an object via load_obj. It will give you tensors of vertices(verts), faces(vertex indices) and aux. Then normalize the tensor of the vertex-indices of each of the corners of the face and then create a mesh with the help of Meshes data structure available in PyTorch 3D.', '\xa0\xa0\xa0 Now, initialize a source shape to be sphere of radius 1.', 'Then, initialize a stochastic gradient descent as an optimizer.', 'Now, we will run a loop to learn the offset to each vertex in the mesh so that the predicted mesh is closer to target mesh at each optimization step. The loss function used here are as follows:', 'However, minimizing only the chamfer distance between the predicted and the target mesh will lead to a non-smooth shape. Hence, we will consider other minimization functions i.e., add shape regularizers to the object for smoothness.', 'Initialize the number of iterations and weight of each loss function and then start a loop.\xa0', 'Now, start the loop by initializing the optimizer and offset the verts of deform_verts, to get a new source mesh. Next, sample 5000 each from both new source and target mesh and calculate all the loss functions and create a final loss by giving weights to each loss function. This process will repeat at each iteration. At last, calculate the loss gradient and update the parameters, as shown below in the code.', 'The output at each 250 iterations is shown below.', 'You can check the full demo here.', 'Bundle Adjustments is state estimation technique used to estimate the location of points in the environment and those points have been estimated from camera images and we do not only want to estimate the location of those points in the world, but we also want to estimate where the camera was, when taking the image and where it was looking. In all, we want to estimate the location of points and camera jointly so the re-projection error where the points are actually projected to, can be minimized. This same problem can be visualized as :', 'The picture below depicts the situation at the beginning of our optimization. The ground truth cameras are plotted in purple while the randomly initialized estimated cameras are plotted in orange:\xa0', 'We seek to align the estimated (orange) cameras with the ground truth (purple) cameras, by minimizing the difference between pairs of relative cameras. Thus, the solution to the problem should look as follows:\xa0', 'Mathematically, the above problem can be defined by minimizing the Sum of Squared Re-projection Errors', 'where,', 'g1, g2, . . ., gN are the extrinsics(location in the world) of N cameras.', 'gij \xa0are the set of relative positions that map between coordinate frames of randomly selected pairs of cameras ( i, j ).', 'd(gi, gj) are is a suitable metric that compares the extrinsics of cameras gi and gj .', 'In this demo, we will learn to initialize a batch of Structure from Motion(SfM), setting up loss functions for bundle adjustments and run an optimization loop using Cameras, transforms and so3 API of PyTorch 3D. The steps are as follows:', 'calc_camera_distance compares a pair of cameras. This function is important as it defines the loss that we are minimizing. The method utilizes the so3_relative_angle function from the SO3 API.', 'get_relative_camera computes the parameters of a relative camera that maps between a pair of absolute cameras. Here we utilize the compose and inverse class methods from the PyTorch3D Transforms API.', 'The code for it is shown below:', 'You can check the full demo, here.', 'In this article, we have talked about PyTorch 3D and its demo for using Mesh data structure – converting deform source mesh to target mesh and also seen the optimized bundle adjustments. The following demo are available at:', 'You can check other libraries dealing with 3D data, here.']","'import os\n!curl -LO https://github.com/NVIDIA/cub/archive/1.10.0.tar.gz\n!tar xzf 1.10.0.tar.gz\n#create a new environement\nos.environ[""CUB_HOME""] = os.getcwd() + ""/cub-1.10.0""\n!pip install \'git+https://github.com/facebookresearch/pytorch3d.git@stable\''
 ""!wget https://dl.fbaipublicfiles.com/pytorch3d/data/dolphin/dolphin.obj\n# Load the dolphin mesh.\ntrg_obj = os.path.join('dolphin.obj')"", '# We read the target 3D model using load_obj\n#which sets verts to be a (V,3)-tensor of vertices and faces.verts_idx to be an (F,3)- tensor of the vertex-indices of each of the corners of \n#the faces. Faces which are not triangles will be split into triangles. aux is an object which may contain normals, \n#uv coordinates, material colors and textures if they are present, and faces may additionally contain indices into these normals, \n#textures and materials in its NamedTuple structure. \nverts, faces, aux = load_obj(trg_obj)\n\n# verts is a FloatTensor of shape (V, 3) where V is the number of vertices in the mesh\n# faces is an object which contains the following LongTensors: verts_idx, normals_idx and textures_idx\n# For this tutorial, normals and textures are ignored.\nfaces_idx = faces.verts_idx.to(device)\nverts = verts.to(device)\n\n# We scale normalize and center the target mesh to fit in a sphere of radius 1 centered at (0,0,0). \n# (scale, center) will be used to bring the predicted mesh to its original center and scale\n# Note that normalizing the target mesh, speeds up the optimization but is not necessary!\ncenter = verts.mean(0)\nverts = verts - center\nscale = max(verts.abs().max(0)[0])\nverts = verts / scale\n\n# We construct a Meshes structure for the target mesh\n#initialize a PyTorch3D datastructure called Meshes, \ntrg_mesh = Meshes(verts=[verts], faces=[faces_idx])', '# We initialize the source shape to be a sphere of radius 1\n\n#ico_sphere creates verts and faces for a unit ico-sphere, with all faces oriented consistently.\n# here, integer specifying the number of iterations for subdivision of the mesh faces. \n#Each additional level will result in four new faces per face.\nsrc_mesh = ico_sphere(4, device)', '# We will learn to deform the source mesh by offsetting its vertices\n# The shape of the deform parameters is equal to the total number of vertices in src_mesh\nverts_shape = src_mesh.verts_packed().shape\n#Creates a tensor of size size filled with fill_value= 0.0\ndeform_verts = torch.full(verts_shape, 0.0, device=device, requires_grad=True)', '# The optimizer\n#create a stochastic gradient optimizer for the deform_verts with\n#learning rate of 1.0\noptimizer = torch.optim.SGD([deform_verts], lr=1.0, momentum=0.9)', '# Number of optimization steps\nNiter = 2000\n# Weight for the chamfer loss\nw_chamfer = 1.0 \n# Weight for mesh edge loss\nw_edge = 1.0 \n# Weight for mesh normal consistency\nw_normal = 0.01 \n# Weight for mesh laplacian smoothing\nw_laplacian = 0.1 \n# Plot period for the losses\nplot_period = 250', 'for i in loop:\n    # Initialize optimizer\n    optimizer.zero_grad()\n    \n    # Deform the mesh\n    new_src_mesh = src_mesh.offset_verts(deform_verts)\n    \n    # We sample 5k points from the surface of each mesh \n    sample_trg = sample_points_from_meshes(trg_mesh, 5000)\n    sample_src = sample_points_from_meshes(new_src_mesh, 5000)\n    \n    # We compare the two sets of pointclouds by computing (a) the chamfer loss\n    loss_chamfer, _ = chamfer_distance(sample_trg, sample_src)\n    \n    # and (b) the edge length of the predicted mesh\n    loss_edge = mesh_edge_loss(new_src_mesh)\n    \n    # mesh normal consistency\n    loss_normal = mesh_normal_consistency(new_src_mesh)\n    \n    # mesh laplacian smoothing\n    loss_laplacian = mesh_laplacian_smoothing(new_src_mesh, method=""uniform"")\n    \n    # Weighted sum of the losses\n    loss = loss_chamfer * w_chamfer + loss_edge * w_edge + loss_normal * w_normal + loss_laplacian * w_laplacian\n    \n    # Print the losses\n    loop.set_description(\'total_loss = %.6f\' % loss)\n    \n    # Save the losses for plotting\n    chamfer_losses.append(loss_chamfer)\n    edge_losses.append(loss_edge)\n    normal_losses.append(loss_normal)\n    laplacian_losses.append(loss_laplacian)\n    \n    # Plot mesh\n    if i % plot_period == 0:\n        plot_pointcloud(new_src_mesh, title=""iter: %d"" % i)\n        \n    # Optimization step\n    loss.backward()\n    optimizer.step()', ""# load the SE3 graph of relative/absolute camera positions\ncamera_graph_file = './data/camera_graph.pth'\n(R_absolute_gt, T_absolute_gt), \\\n    (R_relative, T_relative), \\\n    relative_edges = \\\n        torch.load(camera_graph_file)\n\n# create the relative cameras\ncameras_relative = SfMPerspectiveCameras(\n    R = R_relative.to(device),\n    T = T_relative.to(device),\n    device = device,\n)\n\n# create the absolute ground truth cameras\ncameras_absolute_gt = SfMPerspectiveCameras(\n    R = R_absolute_gt.to(device),\n    T = T_absolute_gt.to(device),\n    device = device,\n)\n\n# the number of absolute camera positions\nN = R_absolute_gt.shape[0]"", 'def calc_camera_distance(cam_1, cam_2):\n    """"""\n    Calculates the divergence of a batch of pairs of cameras cam_1, cam_2.\n    The distance is composed of the cosine of the relative angle between \n    the rotation components of the camera extrinsics and the l2 distance\n    between the translation vectors.\n    """"""\n    # rotation distance\n    R_distance = (1.-so3_relative_angle(cam_1.R, cam_2.R, cos_angle=True)).mean()\n    # translation distance\n    T_distance = ((cam_1.T - cam_2.T)**2).sum(1).mean()\n    # the final distance is the sum\n    return R_distance + T_distance\n\ndef get_relative_camera(cams, edges):\n    """"""\n    For each pair of indices (i,j) in ""edges"" generate a camera\n    that maps from the coordinates of the camera cams[i] to \n    the coordinates of the camera cams[j]\n    """"""\n\n    # first generate the world-to-view Transform3d objects of each \n    # camera pair (i, j) according to the edges argument\n    trans_i, trans_j = [\n        SfMPerspectiveCameras(\n            R = cams.R[edges[:, i]],\n            T = cams.T[edges[:, i]],\n            device = device,\n        ).get_world_to_view_transform()\n         for i in (0, 1)\n    ]\n    \n    # compose the relative transformation as g_i^{-1} g_j\n    trans_rel = trans_i.inverse().compose(trans_j)\n    \n    # generate a camera from the relative transform\n    matrix_rel = trans_rel.get_matrix()\n    cams_relative = SfMPerspectiveCameras(\n                        R = matrix_rel[:, :3, :3],\n                        T = matrix_rel[:, 3, :3],\n                        device = device,\n                    )\n    return cams_relative', ""# init the optimizer\noptimizer = torch.optim.SGD([log_R_absolute, T_absolute], lr=.1, momentum=0.9)\n\n# run the optimization\nn_iter = 2000  # fix the number of iterations\nfor it in range(n_iter):\n    # re-init the optimizer gradients\n    optimizer.zero_grad()\n\n    # compute the absolute camera rotations as \n    # an exponential map of the logarithms (=axis-angles)\n    # of the absolute rotations\n    R_absolute = so3_exponential_map(log_R_absolute * camera_mask)\n\n    # get the current absolute cameras\n    cameras_absolute = SfMPerspectiveCameras(\n        R = R_absolute,\n        T = T_absolute * camera_mask,\n        device = device,\n    )\n\n    # compute the relative cameras as a compositon of the absolute cameras\n    cameras_relative_composed = \\\n        get_relative_camera(cameras_absolute, relative_edges)\n\n    # compare the composed cameras with the ground truth relative cameras\n    # camera_distance corresponds to $d$ from the description\n    camera_distance = \\\n        calc_camera_distance(cameras_relative_composed, cameras_relative)\n\n    # our loss function is the camera_distance\n    camera_distance.backward()\n    \n    # apply the gradients\n    optimizer.step()\n\n    # plot and print status message\n    if it % 200==0 or it==n_iter-1:\n        status = 'iteration=%3d; camera_distance=%1.3e' % (it, camera_distance)\n        plot_camera_scene(cameras_absolute, cameras_absolute_gt, status)\n\nprint('Optimization finished.')"""
49,creating_nicely_formatted_tables/,https://analyticsindiamag.com/beginners-guide-to-tabulate-python-tool-for-creating-nicely-formatted-tables/,"['Visualizing the data in tabular form is easier than visualizing it in a paragraph or comma-separated form. Nicely formatted tables not only provide you with a better way of looking at tables it can also help in understanding each data point clearly with its heading and value.', 'Tabulate is an open-source python package/module which is used to print tabular data in nicely formatted tables. It is easy to use and contains a variety of formatting functions. It has the following functionalities:', 'In this article, we will see what are the different types of table formatting we can perform using Tabulate.\xa0', 'Implementation:', 'We will start by installing tabulate using pip install tabulate.', 'We will be using the tabulate function from the tabulate library so we need to import that. Other than this we do not require to import any python module.', 'from tabulate import tabulate', 'Now we will start by creating different types of formatted tables.', 'data = [[""Himanshu"",1123, 10025], [""Rohit"",1126,10029],\xa0', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0[""Sha"",111178,7355.4]]', 'print(tabulate(data))', 'Here we can see a plain table which is nicely formatted. Now let’s see how we can add a header to this table that we just created.\xa0\xa0', 'print(tabulate(data, headers=[""Name"",""User ID"", ""Roll. No.""]))', 'In order to define the header along with the data, we can set that header=’firstrow’, let us see it through an example.', 'data = [[\'Name\',\'ID\'],[""Himanshu"",1123], [""Rohit"",1126], [""Sha"",111178]]', ""print(tabulate(data, headers='firstrow'))"", 'We can also display the indices of the rows by using the show index parameter.', 'data = [[\'Name\',\'ID\'],[""Himanshu"",1123], [""Rohit"",1126], [""Sha"",111178]]', ""print(tabulate(data, headers='firstrow', showindex='always'))"", 'Now let us see what are some of the formats in which we can print the table.', 'data = [[\'Name\',\'ID\'],[""Himanshu"",1123], [""Rohit"",1126], [""Sha"",111178]]', ""print(tabulate(data, headers='firstrow', showindex='always',\xa0\xa0\xa0\xa0"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0tablefmt='plain'))"", 'data = [[\'Name\',\'ID\'],[""Himanshu"",1123], [""Rohit"",1126], [""Sha"",111178]]', ""print(tabulate(data, headers='firstrow', showindex='always',\xa0\xa0"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0tablefmt='fancy_grid'))"", 'data = [[\'Name\',\'ID\'],[""Himanshu"",1123], [""Rohit"",1126], [""Sha"",111178]]', ""print(tabulate(data, headers='firstrow', showindex='always', tablefmt='jira'))"", 'data = [[\'Name\',\'ID\'],[""Himanshu"",1123], [""Rohit"",1126], [""Sha"",111178]]', ""print(tabulate(data, headers='firstrow', showindex='always',\xa0"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0tablefmt='html'))"", 'data = [[\'Name\',\'ID\'],[""Himanshu"",1123], [""Rohit"",1126], [""Sha"",111178]]', ""print(tabulate(data, headers='firstrow', showindex='always',\xa0"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0tablefmt='textile'))"", 'Similarly, there are many more table formats that we can use to design or format our table.', 'Conclusion:']","'from tabulate import tabulate'
 'data = [[""Himanshu"",1123, 10025], [""Rohit"",1126,10029],\xa0', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0[""Sha"",111178,7355.4]]', 'print(tabulate(data))', 'print(tabulate(data, headers=[""Name"",""User ID"", ""Roll. No.""]))', 'data = [[\'Name\',\'ID\'],[""Himanshu"",1123], [""Rohit"",1126], [""Sha"",111178]]', ""print(tabulate(data, headers='firstrow'))"", 'data = [[\'Name\',\'ID\'],[""Himanshu"",1123], [""Rohit"",1126], [""Sha"",111178]]', ""print(tabulate(data, headers='firstrow', showindex='always'))"", 'data = [[\'Name\',\'ID\'],[""Himanshu"",1123], [""Rohit"",1126], [""Sha"",111178]]', ""print(tabulate(data, headers='firstrow', showindex='always',\xa0\xa0\xa0\xa0"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0tablefmt='plain'))"", 'data = [[\'Name\',\'ID\'],[""Himanshu"",1123], [""Rohit"",1126], [""Sha"",111178]]', ""print(tabulate(data, headers='firstrow', showindex='always',\xa0\xa0"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0tablefmt='fancy_grid'))"", 'data = [[\'Name\',\'ID\'],[""Himanshu"",1123], [""Rohit"",1126], [""Sha"",111178]]', ""print(tabulate(data, headers='firstrow', showindex='always', tablefmt='jira'))"", 'data = [[\'Name\',\'ID\'],[""Himanshu"",1123], [""Rohit"",1126], [""Sha"",111178]]', ""print(tabulate(data, headers='firstrow', showindex='always',\xa0"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0tablefmt='html'))"", 'data = [[\'Name\',\'ID\'],[""Himanshu"",1123], [""Rohit"",1126], [""Sha"",111178]]', ""print(tabulate(data, headers='firstrow', showindex='always',\xa0"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0tablefmt='textile'))"""
50,library_for_faster_eda/,https://analyticsindiamag.com/hands-on-guide-to-datatable-library-for-faster-eda/,"['Working with tabular data in data science we always use the Pandas library in Python. This is widely used for data exploration, analysis, munging and manipulation. These are the primary steps for understanding the data well and making it ready for the model to fit. The only disadvantage of using pandas is its time consuming when there’s a large amount of data(big data).', 'Datatable overcomes the limitations of pandas and speeds up the process of EDA(exploratory data analysis). Datatable has been built by H20.ai, one of the leading AI ML companies in the world. Datatable is pretty similar to pandas and R data.table libraries. Datatable has proper documentation. Works with Python version 3.6+.', 'In this article, I’ll be discussing the implementation of the datatable library with a large dataset.', 'pip install datatable', 'Dataset – Credit Card Fraud Detection', 'The dataset contains transactions that have been made by credit cards in September 2013 by European cardholders. This dataset shows transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. A total of 31 features are time, class, amount and\xa0 V1 to V28.', 'Reading files', 'With pandas:', '3.0183897018432617', 'With datatable:', '0.30858349800109863', 'Clearly, datatable performs much better than pandas. Datatable takes 30 milliseconds to fetch the data whereas pandas take more than 3 seconds.\xa0', 'fread() -> datatable parser for text(csv)\xa0', 'Dataset size:', 'print(df.shape)\xa0', '(284807, 31)', 'Feature Column names', 'print(df.names)', '(‘Time’, ‘V1’, ‘V2’, ‘V3’, ‘V4’, ‘V5’, ‘V6’, ‘V7’, ‘V8’, ‘V9’, ‘V10’, ‘V11’, ‘V12’, ‘V13’, ‘V14’, ‘V15’, ‘V16’, ‘V17’, ‘V18’, ‘V19’, ‘V20’, ‘V21’, ‘V22’, ‘V23’, ‘V24’, ‘V25’, ‘V26’, ‘V27’, ‘V28’, ‘Amount’, ‘Class’)', 'df.head() -> displays the first 10 rows of the dataset in a compact mode as shown below.', 'Column Types', 'print(df.stypes)\xa0', '(stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.float64, stype.bool8)', 'Convert to numpy array', 'np_arr = df.to_numpy()', 'Convert to pandas\xa0', 'df_pd = df.to_pandas()', 'Convert to python list object', 'py_obj = df.to_list()', 'Statistical functions', 'Sorting Frame -> sort() function sorts the row by the column defined in ascending order.', 'With pandas:', '0.14470458030700684', 'With datatable:', '0.03646421432495117', 'GroupBy', 'Let us get the mean amount for each V1 feature. In datatable, operations of a Frame can be represented as dt[i,j,…] where i is row selector, j is column selector and .. are other modifiers. Derived from matrix notations.', '48.4233283996582', '16.062947988510132', '.f in dt.f means frame proxy referring to currently calling frame.', 'Here Datatable takes 1/4th the time of pandas.', 'Appending rows/columns to Frames:', 'cbind() for binding columns and rbind() for binding rows to existing Frame.', 'Deleting a column', ""del df[:, 'V27']"", 'Saving Frames', 'Saving Frame in disk as binary format and opening it later instantly', 'Write the Frame', 'df.to_csv(“out.csv”)', 'Datatable has a similar syntax to pandas and provides predominantly better performance speed for all operations. Datatable covers almost all types of text and tabular data formats.', 'Highly efficient when working with large amounts of data(having 100GB in RAM). Active contributions and development is going onto datatable as in some advanced functionalities it lacks as compared to pandas.\xa0']","'print(df.shape)\xa0'
 'print(df.names)', 'print(df.stypes)\xa0', 'np_arr = df.to_numpy()', 'df_pd = df.to_pandas()', 'py_obj = df.to_list()', ""del df[:, 'V27']"", 'df.to_csv(“out.csv”)'"
51,define_and_visualize_graphs/,https://analyticsindiamag.com/hands-on-guide-to-graphviz-python-tool-to-define-and-visualize-graphs/,"['Python provides different visualization libraries that allow us to create different graphs and plots. These graphs and plots help us in visualizing the data patterns, anomalies in the data, or if data has missing values. Visualization is an important part of data discovery.\xa0', 'Modules like seaborn, matplotlib, bokeh, etc. are all used to create visualizations that are highly interactive, scalable, and visually attractive. But these libraries don’t allow us to create nodes and edges to connect different diagrams or flowcharts or a graph. For creating graphs and connecting them using nodes and edges we can use Graphviz.', 'Graphviz is an open-source python module that is used to create graph objects which can be completed using different nodes and edges. It is based on the DOT language of the Graphviz software and in python it allows us to download the source code of the graph in DOT language.', 'In this article, we will see how we can create a graph using Graphviz and how to download the source code of the graph in the DOT language.', 'We will start by installing Graphviz using pip install graphviz.', 'The digraph is defined under graphviz which we will use for creating graphs object, nodes, and edges. We will create different sample graphs. We will import digraph.', 'from graphviz import Digraph', 'After importing the digraph the next step is to initialize digraph by creating a graph object. Let us create a graph object.', 'gra = Digraph()', 'For creating graphs we will use the dot and edges function and create different types of graphs. Initially, we will start by creating a node for the graph.', ' gra.node(‘a’, ‘Machine Learning Errors’)', 'gra.node(‘b’, ‘RMSE’)', 'gra.node(‘c’, ‘MAE’)', 'This will create different graph nodes, now we need to connect these nodes with edges and then visualize them. Let’s create the edges for these graph objects.', 'gra.edges([‘ab’, ‘ac’])', 'This will create the Edge between Graph objects, now let us visualize what we have created.\xa0', 'gra', 'Here You can see how we created the graph objects(nodes) and then connect them using the edges. Now let us see how we can see the source code of the graph we created.', 'print(gra.source)', 'This is the source code that can be used in DOT language to render the graph using graphviz graph drawing software.\xa0 We can also save and render the source code using render function. Let us see how we can save it.', ""gra.render('Machine.gv.pdf', view=True)"", 'This will create a pdf with the graph which we created and the name which we have assigned.', 'If we open the pdf which we have created in the above step we will have the output given below.', 'Now let us see one more example and create a new graph. Let us create a family tree and see how we can visualize it.', ""gra = Digraph(filename='Family_Tree.gv') #Filename"", '#Creating the Subgtraph with same rank', 'with gra.subgraph() as i:', ""\xa0\xa0\xa0\xa0i.attr(rank='same')"", ""\xa0\xa0\xa0\xa0i.node('A')"", ""\xa0\xa0\xa0\xa0i.node('X')"", ""gra.node('C')"", '#Creating SubGraph with same level', 'with gra.subgraph() as i:', ""\xa0\xa0\xa0\xa0i.attr(rank='same')"", ""\xa0\xa0\xa0\xa0i.node('B')"", ""\xa0\xa0\xa0\xa0i.node('D')"", ""\xa0\xa0\xa0\xa0i.node('Y')"", '#Connecting the edges of the graph', ""gra.edges(['XC', 'AC', 'CD', 'XY', 'XD', 'XB'])"", 'This is how we have created the family tree now let us visualize it.', 'gra', 'Here you can see the graph objects we created linked to each other using edges. Now let us see the source code for this graph.', 'print(gra.source)']","'from graphviz import Digraph'
 'gra = Digraph()', 'gra.node(‘a’, ‘Machine Learning Errors’)', 'gra.node(‘b’, ‘RMSE’)', 'gra.node(‘c’, ‘MAE’)', 'gra.edges([‘ab’, ‘ac’])', 'gra', 'print(gra.source)', ""gra.render('Machine.gv.pdf', view=True)"", ""gra = Digraph(filename='Family_Tree.gv') #Filename"", '#Creating the Subgtraph with same rank', 'with gra.subgraph() as i:', ""i.attr(rank='same')"", ""\xa0i.node('A')"", ""i.node('X')"", ""gra.node('C')"", '#Creating SubGraph with same level', 'with gra.subgraph() as i:', ""i.attr(rank='same')"", ""\xa0i.node('B')"", ""i.node('D')"", ""i.node('Y')"", '#Connecting the edges of the graph', ""gra.edges(['XC', 'AC', 'CD', 'XY', 'XD', 'XB'])"", 'gra', 'print(gra.source)'"
52,to_visualize_missing_values/,https://analyticsindiamag.com/tutorial-on-missingno-python-tool-to-visualize-missing-values/,"['Individuals working in the field of Data Science understand the importance of data. Data is the resource to fuel a machine learning model. But raw data in the real world cannot be used without pre-processing them to a usable format. One of the most common problems faced with real-time data is missing values. There are some values in rows and columns that simply do not exist. But, for a good model training, we need the data to be as clean as possible.', 'Missing values are generally represented with NaN which stands for Not a Number. Although Pandas library provides methods to impute values to these missing rows and columns, we need to be able to understand how, where and how many points of NaN are distributed in the dataset. For this, python introduced a new library called Missingno.', 'The purpose of this article is to get a better understanding of missing data by visualizing them using Missingno.\xa0', 'Missingno is a Python library that provides the ability to understand the distribution of missing values through informative visualizations. The visualizations can be in the form of heat maps or bar charts. With this library, it is possible to observe where the missing values have occurred and to check the correlation of the columns containing the missing with the target column. Missing values are better handled once the dataset is fully explored. Let us now implement this and find out how it helps us pre-process the data better.\xa0', 'The first step in implementing this is to install the library using the pip command as follows:', 'pip install missingno', 'Once this is installed, let us select a dataset that contains missing values. I have selected a dataset from Kaggle called Life expectancy dataset. This dataset is used to estimate the average human life expectancy based on the geographical location, health expenditure, disease etc. To download this dataset click here.\xa0\xa0', 'Let us now import some of the libraries and load our dataset.\xa0', 'Now, let us identify the sum of the missing values using the isnull method of pandas.\xa0', 'life_expectancy.isna().sum()', 'Now, we can identify that there are values which are missing. It is time to now visualize this using the library.\xa0', 'import missingno as msno', 'msno.matrix(life_expectancy)', 'The dataset is distributed from 1 to 2938 data points. The white lines indicate the missing values in each column. The Hepatitis B, population and GDP columns seem to have the highest number of missing values. Other than this, on closer observation, you can notice that there are few trends in the missing rows and columns. For example, if a row value is missing from the BMI column there is also the same rows missing from the thinness 1-19 years column. Another trend is that if there are values missing from the GDP column, then the income column is also missing those rows. These trends give an idea about how the features are correlated with one another. But to get a better idea about correlations we need to use heatmaps.', 'msno.heatmap(life_expectancy)', 'The heatmap shows a positive correlation with blue. The darker the shade of blue, the more the correlation. The map shows that the total expenditure and alcohol have the highest correlation of 0.9. It also shows that the GDP and income column are positively correlated as per our initial intuition which means these two columns can affect the target.\xa0', 'Another way to visualize the data for missing values is by using bar plots.\xa0', 'msno.bar(life_expectancy)', 'These bars show the values that are proportional to the non-missing data in the dataset. Along with that, the number of values missing is also shown. Since the total number of datapoints is 2938, the columns with lesser than these contain missing values.\xa0']","'pip install missingno'
 'life_expectancy.isna().sum()', 'import missingno as msno', 'msno.matrix(life_expectancy)', 'msno.heatmap(life_expectancy)', 'msno.bar(life_expectancy)'"
53,for_swift_statistical_analysis/,https://analyticsindiamag.com/hands-on-tutorial-on-lens-python-tool-for-swift-statistical-analysis/,"['Whenever we are working with datasets the first step is generally understanding what is the data all about. So for exploring the data we start with Exploratory Data Analysis which is analyzing the data with certain techniques and visualization in order to get a clear idea of the data we are dealing with. In EDA we analyze different attributes and their statistical properties also we visualize the data using different graphs and plots.', 'EDA is a necessary step so we cannot neglect it, but performing EDA generally is a pretty time-consuming task because we need to write different types of code for statistical properties as well as codes for different types of visualizations. There are different python libraries and modules which can help in reducing the efforts and time taken in EDA by simple and easy to use codes. The lens is one such library.', 'The lens is an open-source python library which is used for fast calculation of summary statistics and the correlation in the dataset. It helps us explore the properties of different attributes of the dataset in just a single line of code. It creates different types of visualizations of all the attributes in the data. It works on both numerical and categorical data. It is blazingly fast and easy to use.\xa0', 'In this article, we will explore how we can perform EDA using Lens and save time and effort.', 'We will start by installing lens using pip install lens', 'We would load the dataset we will use using pandas so we will import pandas and we will import lens for data analysis and visualizations.', 'import pandas as pd', 'import lens', 'The dataset we will use here is an advertising dataset of an MNC which contains different attributes like ‘Sales’, ‘TV’, etc. We will load this dataset using pandas.', 'df = pd.read_csv(‘Advertising.csv’)', 'df', 'Now as we have loaded the dataset we will work on displaying the statistical properties of this dataset. We will use the summarise and explore function to display the statistical properties of the dataset.', 'data = lens.summarise(df)', 'exp = lens.explore(data)', 'exp.describe()', 'Similarly, we can use these functions to display the properties of a single column also.', 'exp.column_details(‘Sales’)', 'Analyzing and visualizing is easy in the lens, we just need to write a single line of code.', 'exp.correlation()', 'exp.correlation_plot()', 'We can easily visualize different attributes of the dataset using different plots which are already defined in Lens. Let us look at some of the visualizations.', 'exp.distribution_plot(‘Sales’)', 'exp.cdf_plot(‘Newspaper’)', 'Lens has an attractive function named ‘interactive’ which creates a user interface where users can select different attributes and different type of attributes. Let us visualize this interface.', 'lens.interactive_explore(data)', 'Here you can clearly see that we can select different attributes and visualize the different type of plots and graphs of those attributes. Let us see some other plots also.']","'import pandas as pd'
 'import lens', 'df = pd.read_csv(‘Advertising.csv’)', 'df', 'data = lens.summarise(df)', 'exp = lens.explore(data)', 'exp.describe()', 'exp.column_details(‘Sales’)', 'exp.correlation()', 'exp.correlation_plot()', 'exp.distribution_plot(‘Sales’)', 'exp.cdf_plot(‘Newspaper’)', 'lens.interactive_explore(data)'"
54,for_attractive_data_visualizations/,https://analyticsindiamag.com/complete-guide-on-altair-python-tool-for-attractive-data-visualizations/,"['Visualization is one of the best ways to identify any pattern anomalies or trends in the dataset. Data visualization is considered to be a scientific method in which we visualize the data using different charts, bars, graphs, etc. in order to gain some useful and actionable insights. Visualization makes it easier for the human eyes to analyze the trend in the dataset which is not so prominent in tabular datasets.\xa0', 'Python provides different modules/packages/libraries which are used for data visualization. Altair is an open-source python library used for declarative statistical visualization and is based on Vega and Vega-Lite. Altair creates highly interactive and informative visualizations so that we can spend more time in understanding the data we are using and it’s meaning.', 'Altair’s is simple, easy to use, and consistent because it is built on top of the powerful Vega-Lite visualization grammar. It produces beautiful and effective visualizations with a minimal amount of code. The visualizations produced can be downloaded in different formats and can be manipulated using different parameters.', 'In this article, we will explore different types of visualizations that can be produced using Altair and how we can customize these visualizations according to our requirements.', 'Like any other python library, we can install Altair using pip install altair. Also, we can download the sample datasets available by altair by pip install altair vega_datasets.', 'a. Importing required libraries', 'We will be using Pandas for storing and loading our dataset, Seaborn for downloading the dataset other than this we will be using the Altair library for creating beautiful visualizations.', 'import pandas as pd', 'import seaborn as sns', 'import altair as alt', 'b. Loading the Dataset\xa0', 'We will be using a sample dataset named ‘Tips’ which we will download using the seaborn library and store it into a dataframe. This dataset contains different attributes like ‘tips’, ‘total bill’, etc. of different restaurant customers.', 'df = sns.load_datasets(‘tips’)', 'df.head()', 'c. Creating Visualization', 'Altair supports a large variety of visualizations that can be manipulated using different parameters. Lets us create some of the most used statistical visualizations using different parameters.\xa0', 'A bar chart or graph represents categorical data with rectangular bars with heights or lengths proportional to the values that they represent. Altair supports different types of bar graphs.', '# Simple Bar Graph', ""alt.Chart(df).mark_bar().encode(x= 'total_bill',y = 'day')"", '#Stack Bar Graph', ""alt.Chart(df).mark_bar().encode(x= 'total_bill',y = 'day', color='sex')"", 'A scatter plot is a type of plot which displays the values for two variables for a set of data. It is generally used to visualize the relationship between two variables.', '\xa0 \xa0 alt.Chart(df).mark_point().encode(', ""\xa0\xa0\xa0\xa0alt.X('total_bill'),"", ""\xa0\xa0\xa0\xa0alt.Y('tip'),"", ""\xa0\xa0\xa0\xa0alt.Color('sex')"", '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0)', '#Scatter Plot with tool tip', 'alt.Chart(df).mark_point().encode(', ""\xa0\xa0\xa0\xa0 alt.X('total_bill'),"", ""\xa0\xa0\xa0\xa0alt.Y('tip'),"", ""\xa0\xa0\xa0\xa0alt.Color('sex'),"", ""\xa0\xa0\xa0\xa0tooltip=['total_bill', 'tip', 'sex', 'day']"", ')', 'A line chart is a graphical representation of price action that connects a series of data points with a continuous line. Here I will plot it using the data I am using but generally line charts are used to display the time series data like historic stock price over a time period.', 'alt.Chart(df).mark_line().encode(', ""\xa0\xa0\xa0\xa0x='total_bill',"", ""\xa0\xa0\xa0\xa0y = 'tip'"", ')', 'A histogram is a graphical display of data using bars of different heights. In a histogram, each bar groups numbers into ranges. Taller bars show that more data falls in that range.', 'alt.Chart(df).mark_bar().encode(', '\xa0\xa0\xa0\xa0alt.X(""total_bill"", bin=True),', ""\xa0\xa0\xa0\xa0y='count()',"", ')', 'A box plot is a type of chart often used in explanatory data analysis to visually show the distribution of numerical data and skewness through displaying the data quartiles (or percentiles) and averages.', 'alt.Chart(df).mark_line().encode(', ""\xa0\xa0\xa0\xa0x='total_bill',"", ""\xa0\xa0\xa0\xa0y = 'tip'"", ')', 'A heat map is a data visualization technique that shows magnitude of a phenomenon as color in two dimensions. Heat maps make it easy to visualize complex data and understand it at a glance.', 'alt.Chart(df).mark_rect().encode(', ""\xa0\xa0\xa0\xa0alt.X('total_bill', bin=alt.Bin(maxbins=5)),"", ""\xa0\xa0\xa0\xa0alt.Y('tip', bin=alt.Bin(maxbins=40)),"", ""\xa0\xa0\xa0\xa0alt.Color('day') #scale=alt.Scale()"", ')', '', 'Here we have some of the statistical charts using Altair, similarly, we can create different other charts that are there in Altair. All the graphs and plots can be downloaded in several formats which makes them more scalable and usable.', 'Now let us look at some of the parameters which can be used to enhance the visualizations created by Altair.', 'd. Parameters/Function for Enhancing Visualization', 'There are certain parameters which when passed along with the graphs can enhance the visualization and make it more informative and insightful.', 'The interactive function is used to make the graphs interactive i.e. by using the interactive function you can zoom in and zoom out of the graph easily. It is called at the end of the code for visualization.', 'alt.Chart(df).mark_bar().encode(x= ‘total_bill’,y = ‘day’).interactive()', 'The scale parameter is by default active in altair which makes the axis starting point as zero for numerical variables, we can change it according to our requirements.', 'Altair provides different color schemes which can be used for different types of visualization. We can easily use the color scheme we want for our visualization.', 'By using properties parameters we can set the height and width of the visualization we have generated/created.']","'import pandas as pd'
 'import seaborn as sns', 'import altair as alt', 'df = sns.load_datasets(‘tips’)', 'df.head()', '# Simple Bar Graph', ""alt.Chart(df).mark_bar().encode(x= 'total_bill',y = 'day')"", '#Stack Bar Graph', ""alt.Chart(df).mark_bar().encode(x= 'total_bill',y = 'day', color='sex')"", ' alt.Chart(df).mark_point().encode(', ""alt.X('total_bill'),"", ""alt.Y('tip'),"", ""alt.Color('sex')"", '\xa0\xa0)', '#Scatter Plot with tool tip', 'alt.Chart(df).mark_point().encode(', ""alt.X('total_bill'),"", ""alt.Y('tip'),"", ""alt.Color('sex'),"", ""tooltip=['total_bill', 'tip', 'sex', 'day']"", ')', 'alt.Chart(df).mark_line().encode(', ""x='total_bill',"", ""y = 'tip'"", ')', 'alt.Chart(df).mark_bar().encode(', '\xa0alt.X(""total_bill"", bin=True),', ""y='count()',"", ')', 'alt.Chart(df).mark_line().encode(', ""x='total_bill',"", ""y = 'tip'"", ')', 'alt.Chart(df).mark_rect().encode(', ""alt.X('total_bill', bin=alt.Bin(maxbins=5)),"", ""alt.Y('tip', bin=alt.Bin(maxbins=40)),"", ""alt.Color('day') #scale=alt.Scale()"""
55,accelerated_python_gui_framework/,https://analyticsindiamag.com/complete-tutorial-on-dearpygui-gpu-accelerated-python-gui-framework/,"['Graphical User Interface provides interaction between the user and the application. With the help of different widgets and functions, we can create a GUI of an application. Different indicators like buttons, text boxes, checkboxes can be used to build the GUI of an application.\xa0\xa0\xa0\xa0', 'DearPyGUI is a powerful python GUI framework which is easy to use and is a wrapper for Dearlmgui. Dearpygui is not an ordinary python GUI framework as it does not use the native widgets but instead draws the widgets using the system’s GPU. It is an all-included GUI framework which means that it can perform and create each and every widget that is used to make GUI interfaces.', 'In this article, we will explore what are the different types of widgets that we can create using Dearpygui also we will see how we can leverage visualizations using Dearpygui.', 'Like any other python library, we will install Dearpygui using pip install dearpygui.', 'We will start by importing dearpygui to look at the creation of the different widgets. We will import any other library required as and when required.', 'from dearpygui.dearpygui import *', 'As dearpygui supports a large variety of we will now explore some of the most used widgets and how we can create them. While creating an interface we need to end the python script with start_dearpygui() to launch the interface.\xa0', 'We can create different types of widgets using different parameters. Let us start by creating a text box and a button. We will also define and fix the Main Window Size.', 'set_main_window_size(400, 400)', ""add_text('DearPyGUI Generated This Text')"", ""add_text('Below You Will See two buttons')"", 'add_button(""Button 1"")', 'add_same_line(spacing=30) #Adding a space on the same line', 'add_button(""Button 2"")', 'start_dearpygui()', 'Next, we will see how we can create radio buttons and checkboxes. We will add all the widgets in the same window.', 'add_spacing(count=4)', 'add_text(""Below You will See a Check Box and a Radio Button"")', 'add_checkbox(""Checkbox"")', 'start_dearpygui()', 'Dearpygui gives us the option of creating a menu bar and menu items to it. Let us see how we can create a menu bar. We will add this menu bar to the top of the window and some items to it.', 'add_menu_bar(""Main Menu Bar"")', 'add_menu(""Format"")', 'add_menu_item(""form1"")', 'add_menu_item(""form2"")', 'add_menu_item(""form3"")', 'add_spacing(count=5)', 'start_dearpygui()', 'Here we have created a dropdown menu that opens up as soon as we click it. Next, we will see how we can plot visualizations on our main window using Dearpygui.', 'We will create a new window with a new size and visualize different plots on it.', 'set_main_window_size(800, 800)', 'add_simple_plot(""Lineplot"", [1, 4, 2, 8, 12], height=180)', 'add_simple_plot(""Histogram"", [1,4,2,8,12], height=180, histogram=True)', 'start_dearpygui()', 'Visualizations created using Dearpygui are highly interactive and informative.', 'DearPyGui has a drawing API that is well suited for primitive drawing, custom widgets, or even dynamic drawings. Let’s see how we can use the drawing feature.', 'add_drawing(""First_Drawing"", width=300, height=300)', 'draw_circle(""First_Drawing"", [150, 150], radius=50, color=[255, 255, 255,\xa0255], segments=0)', 'start_dearpygui()', 'Other than this dearpygui also allows you to select a theme of your own choice or even create your own theme.', 'This is an introduction to the basic functionalities of dearpygui, it contains a lot more functions that you can explore according to your requirements.']","'from dearpygui.dearpygui import *'
 'set_main_window_size(400, 400)', ""add_text('DearPyGUI Generated This Text')"", ""add_text('Below You Will See two buttons')"", 'add_button(""Button 1"")', 'add_same_line(spacing=30) #Adding a space on the same line', 'add_button(""Button 2"")', 'start_dearpygui()', 'add_spacing(count=4)', 'add_text(""Below You will See a Check Box and a Radio Button"")', 'add_checkbox(""Checkbox"")', 'start_dearpygui()', 'add_menu_bar(""Main Menu Bar"")', 'add_menu(""Format"")', 'add_menu_item(""form1"")', 'add_menu_item(""form2"")', 'add_menu_item(""form3"")', 'add_spacing(count=5)', 'start_dearpygui()', 'set_main_window_size(800, 800)', 'add_simple_plot(""Lineplot"", [1, 4, 2, 8, 12], height=180)', 'add_simple_plot(""Histogram"", [1,4,2,8,12], height=180, histogram=True)', 'start_dearpygui()', 'add_drawing(""First_Drawing"", width=300, height=300)', 'draw_circle(""First_Drawing"", [150, 150], radius=50, color=[255, 255, 255,\xa0255], segments=0)', 'start_dearpygui()'"
56,interactive_and_scalable_visualization/,https://analyticsindiamag.com/complete-tutorial-on-pygal-a-python-tool-for-interactive-and-scalable-visualization/,"['Data visualization is a scientific method of finding out the patterns, anomalies, and outliers in the data. It is a method that is used to create the graphs and plots which helps in finding out the trend in the data because it makes data simpler in terms of understanding by the human brain and we identify trends with naked eyes.\xa0', 'Python provides different visualization packages that help in creating different types of charts, graphs, and plots. Pygal is an open-source python library that not only creates highly interactive plots but also creates SVG images of the graphs/plots so that we can use it and customize it accordingly. Pygal is highly customizable and creates graphs with a few lines of code.\xa0', 'Pygal creates a variety of plots like a bar, area, histogram, line, etc and due to the high scalability of the images of the plot downloaded as SVG, we will have good quality images that can be embedded into different projects, websites, etc.', 'In this article, we will go through different charts that we can create using pygal and also see how we can download them in the SVG format.', 'We will start exploring pygal but before that, we need to install pygal using pip install pygal.', 'We will be importing pygal for visualization and pandas for importing the dataset for which we will be creating the visualizations.', 'import pandas as pd', 'import pygal', 'We will be working with an Advertisement dataset that contains different attributes like ‘TV’, ‘Radio’, etc. and a target variable ‘Sales’.', 'df = pd.read_csv(‘Advertising.csv’)', 'df.head()', 'Now we will use pygal to create different bars and graphs using the dataset we have loaded. We will start by creating some basic charts and then move to some advance charts and downloading all the graphs and charts in SVG formats.', 'Bar charts represent the data in the form of horizontal or vertical bars with the values they represent. Pygal supports different types of Bar charts.', 'bar_graphs = pygal.Bar()\xa0\xa0\xa0', ""bar_graphs.add('Sales', df['Sales'][:5])\xa0"", ""bar_graphs.render_to_file('bar1.svg')"", ""bar_graphs.add('Sales', df['Sales'][:5])"", ""bar_graphs.add('TV', df['TV'][:5])"", ""bar_graphs.render_to_file('bar2.svg')"", 'A line chart displays the data or information as a series of data points on a line.\xa0', 'line_chart = pygal.Line()', ""line_chart.add('Sales', df['Sales'][:15])"", ""line_chart.render_to_file('line1.svg')"", ""line_chart.add('Sales', df['Sales'][:15])"", ""line_chart.add('TV', df['TV'][:15])"", ""line_chart.add('Newspaper', df['Newspaper'][:15])"", ""line_chart.add('Radio', df['Radio'][:15])"", ""line_chart.render_to_file('line2.svg')"", 'Box Plots are considered as the statistical graphs which are used to display the numerical data with their quartile information. They are generally used to visualize the outliers in the data.', 'box_plot = pygal.Box()', ""box_plot.title = 'Advertising'"", ""box_plot.add('TV', df['TV'])"", ""box_plot.add('Radio', df['Radio'])"", ""box_plot.add('Newspaper', df['Newspaper'])"", ""box_plot.render_to_file('box1.svg')"", 'A dot plot is a statistical plot that is used to display the data in forms of the dot, the size of the dot represents the values i.e higher the value bigger is the dot.\xa0', 'dot_chart = pygal.Dot()', ""dot_chart.title = 'Advertising'"", ""dot_chart.add('Sales', df['Sales'][:5])"", ""dot_chart.add('TV', df['TV'][:5])"", ""dot_chart.add('Radio', df['Radio'][:5])"", ""dot_chart.add('Newspaper', df['Newspaper'][:5])"", ""dot_chart.render_to_file('dot1.svg')"", 'Funnel charts are used to visualize the progressive reduction or expansion of data when it passes from different phases. It is generally used to represent the Sales data.', 'funnel_chart = pygal.Funnel()', ""funnel_chart.title = 'Advrtising'"", ""funnel_chart.add('TV', df['TV'][:15])"", ""funnel_chart.add('Radio', df['Radio'][:15])"", ""funnel_chart.add('Newspaper', df['Newspaper'][:15])"", ""funnel_chart.render_to_file('funnel1.svg')"", 'A gauge chart combines the doughnut chart and pie chart, it is used to visualize the data at a particular value.\xa0', ' gauge_chart = pygal.Gauge(human_readable=True)', ""gauge_chart.title = 'Advertising'"", 'gauge_chart.range = [0, 500]', ""gauge_chart.add('Sales', df['Sales'][:1])"", ""gauge_chart.add('TV', df['TV'][:1])"", ""gauge_chart.add('Radio', df['Radio'][:1])"", ""gauge_chart.add('Newspaper', df['Newspaper'][:1])"", ""gauge_chart.render_to_file('gauge.svg')"", 'Treemaps are used to represent data in nested forms usually in rectangles shape. The size of the map resent the value i.e higher the value greater is the size of the treemap.', ' treemap = pygal.Treemap()', ""treemap.title = 'Advertisement TreeMap'"", ""treemap.add('Sales', df['Sales'][:10])"", ""treemap.add('TV', df['TV'][:10])"", ""treemap.add('Radio', df['Radio'][:10])"", ""treemap.add('Newspaper', df['Newspaper'][:10])"", ""treemap.render_to_file('treemap1.svg')"", 'Pygal provides different types of parameters for different graphs, we can manipulate different designs, features, and render them accordingly.']","'import pandas as pd'
 'import pygal', 'df = pd.read_csv(‘Advertising.csv’)', 'df.head()', 'bar_graphs = pygal.Bar()\xa0', ""bar_graphs.add('Sales', df['Sales'][:5])"", ""bar_graphs.render_to_file('bar1.svg')"", ""bar_graphs.add('Sales', df['Sales'][:5])"", ""bar_graphs.add('TV', df['TV'][:5])"", ""bar_graphs.render_to_file('bar2.svg')"", 'line_chart = pygal.Line()', ""line_chart.add('Sales', df['Sales'][:15])"", ""line_chart.render_to_file('line1.svg')"", ""line_chart.add('Sales', df['Sales'][:15])"", ""line_chart.add('TV', df['TV'][:15])"", ""line_chart.add('Newspaper', df['Newspaper'][:15])"", ""line_chart.add('Radio', df['Radio'][:15])"", ""line_chart.render_to_file('line2.svg')"", 'box_plot = pygal.Box()', ""box_plot.title = 'Advertising'"", ""box_plot.add('TV', df['TV'])"", ""box_plot.add('Radio', df['Radio'])"", ""box_plot.add('Newspaper', df['Newspaper'])"", ""box_plot.render_to_file('box1.svg')"", 'dot_chart = pygal.Dot()', ""dot_chart.title = 'Advertising'"", ""dot_chart.add('Sales', df['Sales'][:5])"", ""dot_chart.add('TV', df['TV'][:5])"", ""dot_chart.add('Radio', df['Radio'][:5])"", ""dot_chart.add('Newspaper', df['Newspaper'][:5])"", ""dot_chart.render_to_file('dot1.svg')"", 'funnel_chart = pygal.Funnel()', ""funnel_chart.title = 'Advrtising'"", ""funnel_chart.add('TV', df['TV'][:15])"", ""funnel_chart.add('Radio', df['Radio'][:15])"", ""funnel_chart.add('Newspaper', df['Newspaper'][:15])"", ""funnel_chart.render_to_file('funnel1.svg')"", 'gauge_chart = pygal.Gauge(human_readable=True)', ""gauge_chart.title = 'Advertising'"", 'gauge_chart.range = [0, 500]', ""gauge_chart.add('Sales', df['Sales'][:1])"", ""gauge_chart.add('TV', df['TV'][:1])"", ""gauge_chart.add('Radio', df['Radio'][:1])"", ""gauge_chart.add('Newspaper', df['Newspaper'][:1])"", ""gauge_chart.render_to_file('gauge.svg')"", 'treemap = pygal.Treemap()', ""treemap.title = 'Advertisement TreeMap'"", ""treemap.add('Sales', df['Sales'][:10])"", ""treemap.add('TV', df['TV'][:10])"", ""treemap.add('Radio', df['Radio'][:10])"", ""treemap.add('Newspaper', df['Newspaper'][:10])"", ""treemap.render_to_file('treemap1.svg')"""
57,tool_for_data_cleaning/,https://analyticsindiamag.com/beginners-guide-to-pyjanitor-a-python-tool-for-data-cleaning/,"['As a data scientist, you are more or less going to spend 60-70% of your time cleaning and preparing your data. The process of cleaning, encoding and transforming your raw data in order to bring them into a format that the machine learning model can understand is called Data Pre-processing. This process is often long and cumbersome and most developers consider it to be the least favourite part of a project. Despite being tedious, it is one of the most important techniques that need to be implemented. To simplify the overall process and make it a bit more interesting, python introduces a package called PyJanitor- A Python Tool for Data Cleaning.', 'This article deals with an overview of what pyjanitor is, how it works and a demonstration of using this package to clean dirty data.\xa0', 'Initially developed in R as a Janitor library, it was developed in Python due to its convenience. Pyjanitor is an API that is written on top of the popular python library Pandas. Data pre-processing can be thought of as a directed acyclic graph where the starting node is raw data and we implement a series of techniques on this raw data to get usable data. Pandas has been a huge part of the data science ecosystem and pyjanitor API is implemented on pandas using a concept called method chaining.\xa0', 'Method chaining can be understood as something similar to parallel processing. Instead of having an imperative style of programming as with pandas, method chaining combines multiple processes and allows the user to decide the order of the actions taken. Here is an example of method chaining.', 'If we were using Pandas to clean our data it would look something like this', 'With pyjanitor, the functions are just verbs that help you perform the actions.\xa0', 'Using these functions, we pipeline them together. This method chaining helps in writing cleaner code and the function names are easier to remember, making the data cleaning much simpler. There are two advantages to using pyjanitor. One, it extends pandas with convenient data cleaning routines. Two, it provides a cleaner, method-chaining, verb-based API for common pandas routines.', 'The functions are written in the pandas-flavour package and are registered. These functions are registered without making any changes to pandas. You can add your own data processing functions depending on the data and use these methods whenever needed. Here is an example.', 'Once you import the janitor library, the above function is automatically registered and can be used. The fact that each DataFrame method pyjanitor registers return the DataFrame is what gives it the capability to method chain.', 'In this demonstration, we will implement various data frame manipulation techniques on raw data. Before we start with cleaning, let us install the requirements.\xa0', 'pip install pyjanitor', 'I have used Melbourne housing dataset that can be found here. Once you have installed the package and downloaded the data we can get started with data pre-processing.\xa0', 'Here is what the raw data looks like.\xa0', 'As you can see there are missing values, whitespaces in column names and categorical columns. Using pyjanitor I will now pipeline the most obvious data processing into just one block of code.\xa0', 'I have listed the methods to clean the column names. This removes the white spaces, capital letters and special characters from column names making it easy to access them. Then, a function to remove empty rows if any.\xa0', 'To make it easy for identifying the features and target I renamed the price column to target. Next, I converted three columns namely ‘method’,’ suburb’ and ‘regionname’ to categorical values. The fill_empty method is used to fill your columns that contain NaN with any value of your choice. Finally, I decided to drop the Date column just for the purposes of this demonstration.', 'Once you have pipelined and finished the first and obvious parts of processing your data, it becomes easier for you to identify the more subtle changes needed and where the changes need to be made. I will now show you how to register your custom function to the janitor.\xa0', 'Let’s say you want to remove spaces in a column of your dataset. I have selected a column named council area. This is how the column looks like.\xa0', 'To remove the space, I will write my custom function as follows and register it to the janitor library.', 'To register it, simply import janitor again and you can use it on your dataset.\xa0', 'Just like that, we wrote a custom function to remove white spaces and this can be used for any dataset by anybody. There are 1000s of methods you can choose from for making the process of transforming the data easier. Here is a list of various methods that can be used.']",'pip install pyjanitor'
58,processing_and_data_mining/,https://analyticsindiamag.com/hands-on-guide-to-pattern-a-python-tool-for-effective-text-processing-and-data-mining/,"['Text Processing mainly requires Natural Language Processing( NLP), which is processing the data in a useful way so that the machine can understand the Human Language with the help of an application or product. Using NLP we can derive some information from the textual data such as sentiment, polarity, etc. which are useful in creating text processing based applications.', 'Python provides different open-source libraries or modules which are built on top of NLTK and helps in text processing using NLP functions. Different libraries have different functionalities that are used on data to gain meaningful results. One such Library is Pattern.\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'Pattern is an open-source python library and performs different NLP tasks. It is mostly used for text processing due to various functionalities it provides. Other than text processing Pattern is used for Data Mining i.e we can extract data from various sources such as Twitter, Google, etc. using the data mining functions provided by Pattern.\xa0', 'In this article, we will try and cover the following points:', 'We will start by installing Pattern using the pip install pattern.', 'Different functionalities are defined under different functions we will import them as and when required as we move ahead in this article. We will be working in the English language so we will be using ‘en’ for the English module. Let us start with some basic functionalities of Pattern for NLP operations\xa0', 'We will go through some of the most used and most important functionalities which are provided by Pattern. Starting with parsing a sentence.', 'a. Parsing', 'from pattern.en import parse', ""parse('Hello Everyone and Welcome to Analytics India Magazine')"", 'Here we can see the output of the parse function differentiate the words in the sentence as a noun, verb, subject, or subject. We can also use the ‘pprint’ function defined in the pattern library to display the parsed sentence in a clear manner. Also, we can set different parameters for parses such as lemmata, tokenize, encoding, etc.\xa0 All these parameters can be used in parsing only so that we do not have to use a separate function for different properties.', 'from pattern.en import pprint', ""pprint(parse('Hello Everyone and Welcome to Analytics India Magazine',\xa0relations\xa0 = True,tokenize= True, lemmata= True))"", 'b. N-Grams', 'N-Gram function is used to find all the n-grams in a given text string.', 'from pattern.en import ngrams', 'print(ngrams(""Hello Everyone and Welcome to Analytics India Magazine"", n=3))', 'Sentiment function tries to identify the opinion or view that is held by the particular text string. Sentiment function returns both polarity and the subjectivity of the given text. The Polarity value ranges between 1(Highly Positive) to -1(Highly Negative) and subjectivity value ranges between 0(Objective) to 1(Subjective).', 'from pattern.en import sentiment', 'print(sentiment(""He is a good boy but sometimes he behaves miserably""))', 'We can see that the sentiment analysis says that the sentence is negative with high subjectivity.', 'Modality is one such function that makes it different from other python libraries based on NLP. The modality function is used to find the degree of certainty in a particular sentence. Its value ranges from -1 to 1. As defined in the Pattern library we can state that a sentence with a modality of 0.5 and above can be stated as a fact.', 'from pattern.en import modality', ""text = parse('He is a good boy but sometimes he behaves miserably')"", 'text= Sentence(text)', 'print(modality(text))', 'The modality comes out to be zero which means that the sentence is neutral.', 'Suggest function is used for spelling corrections but it is more than that. It not only checks the spelling it also gives you suggestions of what might be the correct word with their probabilities. This function also distinguishes pattern from other libraries.\xa0', 'from pattern.en import suggest', 'print(suggest(""Heroi""))', 'Quantify function is used to provide a word count estimation of the words given.', 'from pattern.en import quantify', ""a = quantify(['Pencil', 'Pencil', 'Eraser', 'Sharpener', 'Sharpener', 'Sharpener',\xa0'Scale', 'Compass'])"", 'print(a)', 'One of the most important features of Pattern is that it can be used for data mining through different platforms like Google, Twitter, Wikipedia, etc. Let us explore the data mining operations of the pattern library and extract some data using it.', 'We will start by mining data using Google by entering a keyword that we want to search for and display the text along with the URL that is there in the search result.', 'from pattern.web import Google', 'google = Google()', ""for results in google.search('Analytics India Magazine'):"", '\xa0\xa0\xa0\xa0print(results.url)', '\xa0\xa0\xa0\xa0print(results.text)', 'We can also use twitter for mining data which we require. Let us explore it through an example.', 'from pattern.web import Twitter', 'twitter = Twitter()', ""for results in twitter.search('Analytics India Magazine'):"", '\xa0\xa0\xa0\xa0print(results.url)', '\xa0\xa0\xa0\xa0print(results.text)', 'Flickr is an American image hosting and video hosting service, as well as an online community. Pattern can be used to extract data from Flickr.', 'from pattern.web import Flickr', 'flickr = Flickr(license=None)', ""for result in flickr.search('Analytics India Magazine'):"", '\xa0\xa0\xa0\xa0print(result.url)', '\xa0\xa0\xa0\xa0print(result.text)', 'Similarly, Pattern provides a large number of online data mining using different platforms and we can use them accordingly.']","'from pattern.en import parse'
 ""parse('Hello Everyone and Welcome to Analytics India Magazine')"", 'from pattern.en import pprint', ""pprint(parse('Hello Everyone and Welcome to Analytics India Magazine',\xa0relations\xa0 = True,tokenize= True, lemmata= True))"", 'from pattern.en import ngrams', 'print(ngrams(""Hello Everyone and Welcome to Analytics India Magazine"", n=3))', 'from pattern.en import sentiment', 'print(sentiment(""He is a good boy but sometimes he behaves miserably""))', 'from pattern.en import modality', ""text = parse('He is a good boy but sometimes he behaves miserably')"", 'text= Sentence(text)', 'print(modality(text))', 'from pattern.en import suggest', 'print(suggest(""Heroi""))', 'from pattern.en import quantify', ""a = quantify(['Pencil', 'Pencil', 'Eraser', 'Sharpener', 'Sharpener', 'Sharpener',\xa0'Scale', 'Compass'])"", 'print(a)', 'from pattern.web import Google', 'google = Google()', ""for results in google.search('Analytics India Magazine'):"", 'print(results.url)', 'print(results.text)', 'from pattern.web import Twitter', 'twitter = Twitter()', ""for results in twitter.search('Analytics India Magazine'):"", 'print(results.url)', 'print(results.text)', 'from pattern.web import Flickr', 'flickr = Flickr(license=None)', ""for result in flickr.search('Analytics India Magazine'):"", 'print(result.url)', 'print(result.text)'"
59,feature_engineering_using_autofeat/,https://analyticsindiamag.com/guide-to-automatic-feature-engineering-using-autofeat/,"['In machine learning after acquiring the data, we enquire what the data is trying to tell us and how we can make its features into a simpler form to fit in the model. Feature engineering is the most crucial and critical phase in building a good machine learning model. Manipulating the required features and selecting accurate ones to give us the best results is quite a challenging task, especially for large datasets. This needs proper knowledge and continuous experimenting, turning out to be memory intensive and time-consuming.', 'AutoFeat is a python library that provides automated feature engineering and feature selection along with models such as AutoFeatRegressor and AutoFeatClassifier. These are built with many scientific calculations and need good computational power.', 'In this article, I’ll be discussing the aspects of using AutoFeat, steps involved and its implementation with a real-world dataset.\xa0\xa0', 'Feature Engineering -The input feature vector is transformed to non-linear transformations(logarithmic, exponential, reciprocal, square root, etc) then combining those transformed or normal features to create more complex features and again apply non-linear transformations. This could be repeated as many times as wanted but the feature space grows dynamically so conventionally it is preferred to stop after two or three iterations by which much information about the features is gathered.', 'For generating new features SymPy library from python is used which simplifies and removes redundant features. To operate and manipulate physical quantities, a Pint library from python is used which also computes the dimensionless features using Buckingham Pi theorem.', 'Feature Selection – Out of the large feature vectors, the optimal features are to be picked. There are two types of feature selection approaches – Univariate(one feature at a time) and multivariate(multiple features at a time). Multivariate is preferred over univariate as univariate feature selection could add redundant features and also miss out on important features. For this purpose, AutoFeat has integrated the FeatureSelector class. The LassolarsCV model is used to choose features based on sparse weights. It uses a ‘noise filtering approach’ by training the model on original features.', 'Installation – pip install autofeat', 'For regression, AutoFeatRegressor module is used.', 'Parameters:', 'The dataset used for demonstration is Boston housing price dataset from scikit learn library.', 'Performance metrics used for regression is R-squared error present within model.score().', 'OUTPUT - Final R^2: 0.9074', 'Originally X.shape = (506,13) and after transformation df.shape = (506,32)', 'The column names show the newly formed features.', 'For classification, AutoFeatClassifier module is used. Parameters are the same as a regressor. The dataset used for demonstration is wine classification dataset from scikit learn library. Performance metrics used for classification is Accuracy present within model.score().', 'OUTPUT - Final Accuracy: 0.9944', 'AutoFeatModel\xa0', 'Instead of using Regressor or Classifier separately, they could be used as an argument to AutoFeatModel class. By default, it’s set to regression.', 'OUTPUT - Final R^2: 0.8908', 'FeatureSelector class provides automatic feature selection. The selected features are returned as a dataframe.', 'Parameters', 'Training the prediction model with additional noisy features and selecting only those which have a higher coefficient than other noisy features.', 'As we progress towards Auto ML so these ready to use libraries can prove to be very handy in the production of business decision making. Though AutoFeat does not achieve the complete state-of-art and has certain limitations, it does perform a fairly good job.\xa0\xa0']","'pip install autofeat'
 'OUTPUT - Final R^2: 0.9074', 'OUTPUT - Final Accuracy: 0.9944', 'OUTPUT - Final R^2: 0.8908'"
60,speed_up_data_visualization/,https://analyticsindiamag.com/hands-on-guide-to-pandas-visual-analysis-way-to-speed-up-data-visualization/,"['Exploratory Data Analysis is the process of inspecting the data in order to understand what the data is all about. It is generally a visual method where we create different plots and graphs to understand what patterns, anomalies and outliers do data have. It is an important step because it helps us analyze the relationship between different attributes within themselves, also it is helpful in analyzing the properties of different attributes of the dataset.', 'Being an important step in analyzing what data is all about Exploratory Data Analysis generally takes a lot of time because we need to write code for analyzing and visualizing data. What if we can automate this process of visualizing and analyzing data?\xa0', 'Pandas Visual Analysis is an open-source python library which is used to visually analyze the data and that too in just a single line of code. It creates a user interface that can be used to create different plots and graphs taking different attributes. It supports a large variety of graphs and plots, also all the graphs are created using Plotly so that they are highly interactive, visually appealing, and easily downloadable.', 'In this article, we will see how easily and effortlessly we can automate the process of visual analysis using Pandas Visual Analysis.', 'We will start by installing pandas visual analysis using pip install pandas-visual-analysis.', 'For data analysis, we will be importing pandas visual analysis and we will import pandas for loading the dataset we will use. Other than this we will import seaborn to load a dataset defined in seaborn named tips.', 'import pandas as pd', 'from pandas_visual_analysis import VisualAnalysis', 'import seaborn as sns', 'We will explore pandas visual analysis using two different datasets. One dataset we will load from seaborn named tips is a dataset of a restaurant data which contains attributes like ‘total bill’, ‘tip’, etc. and the second dataset is a sales data of an MNC which contains attributes like ‘Sales’, ‘TV’, etc.', 'df1= sns.load_dataset(‘tips’)', 'df1', 'df2= pd.read_csv(‘Advertising.csv’)', 'df2', 'This is the final step that will load our data in the form of a Graphical User Interface where we have a variety of graphs and plots defined and we can select different attributes to visualize.', 'VisualAnalysis(df1)', 'Here you can see that we have created an interface with different sections to analyze and visualize the dataset we are working on. It is a multivariate dataset still pandas visual analysis created it so easily and effortlessly. Let us see what are the different sections.', 'The first section helps us analyze the statistical properties, we can analyze different metrics like mean, quartiles, median, etc. for all the numerical attributes.', 'Using this,\xa0 we can analyze the distribution and relationship between two attributes using a scatter plot.', 'In this way, we will analyze the distribution of an attribute using the histogram.', 'These are the different approaches that we can use to analyze a dataset using pandas visual analysis.', 'Similarly, we can create this interface and analyze different sections of the second dataset using the same VisualAnalysis command.', 'VisualAnalysis(df2)']","'import pandas as pd'
 'from pandas_visual_analysis import VisualAnalysis', 'import seaborn as sns', 'df1= sns.load_dataset(‘tips’)', 'df1', 'df2= pd.read_csv(‘Advertising.csv’)', 'df2', 'VisualAnalysis(df1)', 'VisualAnalysis(df2)'"
61,on_short_data_annotations/,https://analyticsindiamag.com/hands-on-tutorial-on-holoviews-automated-visualization-based-on-short-data-annotations/,"['Data Visualization is a scientific study of the data in order to find out the anomalies, patterns, or trends in a particular dataset. It can be done using a variety of plots and graphs which we can use to visualize different properties of the attributes of the dataset. Visualization is one of the easiest ways of understanding the data as we can clearly visualize the data with our naked eyes and our brain processes the data to give us a clear picture of what the data is trying to say.', 'Visualization can be of many types like Bar Charts, Histograms, Scatter Plots, etc. which can be used on different types of data to gain useful insights about the data. Python has a large number of libraries/modules which can be used for data visualization and creating highly informative and attractive graphs and plots. Holoviews is one such library that makes the process of visualization easier such that we can create highly informative and insightful visualizations in a few lines of code.', 'Holoviews is an open-source python library that makes data visualization easier. Holoviews works on conveying the message that data is trying to tell rather than focusing on how to plot visualizations. Holoviews works on Numpy and Params, and for visualization, it supports ‘Bokeh’ and\xa0 ‘Matplotlib’.', 'In this article, we will see how we can create different types of visualizations using Holoviews and how we can manipulate them according to our requirements.', 'Like any other python library, we will install Holoviews and all its dependencies using pip install holoviews.\xa0\xa0\xa0', 'In this article, we will use two different datasets for different visualization. For loading the dataset we will import Pandas and Seaborn. For visualization purposes, we will import holoviews.', 'import pandas as pd', 'import holoviews as hv\xa0\xa0', 'from holoviews import opts', 'import seaborn as sns', ""hv.extension('bokeh', 'matplotlib')\xa0#extensions used for visualization"", 'For visualization using Holoviews, we will use an Advertising dataset of an MNC which contains different attributes like Sales, Newspaper, etc. and the second dataset we will be using is a sample dataset defined under seaborn namely the ‘Tips’ Dataset which contains different attributes of restaurant billing history like ‘total bill, ‘tip’, etc.', ""df = pd.read_csv('Advertising.csv')"", 'df.head()', ""df1 = sns.load_dataset('tips')"", 'df1.head()', 'We will start by creating visualizations for the advertising dataset and after that, we will create some advanced visualization using the tips dataset.', 'We will plot a scatter plot between our target variable i.e’ ‘Sales’ and all other feature variables. For creating the visualizations we will start by defining our feature variables and then creating a dataset using Holoviews.', ""vdims = [('Newspaper'), ('Radio'), ('TV')]"", ""ds = hv.Dataset(df, ['Sales'], vdims)"", 'ds', 'Now will use this dataset and create the visualization.', ""layout1= (ds.to(hv.Scatter, 'Sales', 'Newspaper') + ds.to(hv.Scatter, 'Sales', 'TV') +\xa0\xa0ds.to(hv.Scatter, 'Sales', 'Radio')).cols(2)"", 'layout1.opts(opts.Scatter(width=400, height=250))', 'For creating this visualization we used “cols” functions which define the number of columns in which visualization is created that is why we can see the graphs in two rows. This graph is plotted using bokeh as we can see the symbol on the right side top corner, so these graphs are highly interactive and visually appealing.', 'We will create a Bar Plot of ‘Sales’ and ‘Radio’.\xa0', ""layout2 = (ds.to(hv.Bars, 'Sales', 'Radio'))"", 'layout2.opts(opts.Bars(width=900, height=400))', 'We will create distribution plots of all the attributes so that we can visualize how data is distributed among all these attributes.', ""distribution = (hv.Distribution(ds, ['Sales']) + hv.Distribution(ds, ['TV']) +\xa0hv.Distribution(ds, ['Newspaper'])+hv.Distribution(ds,\xa0['Radio'])).cols(2)"", 'distribution.opts( width=400, height=250)', 'Now we will the second dataset i.e tips dataset and create some advanced statistical charts like boxplot.', ""title = 'Total Bill according to Gender'"", ""box = hv.BoxWhisker(df1, ['sex'], 'total_bill', label=title)"", ""box.opts( width=600,\xa0 cmap='Set1')"", 'The opts function as seen in the code is used for defining the height and width of the plots and can be used to manipulate other features also.', 'We will create violin plots among different attributes of the tips dataset and visualize them.', ""violin= (hv.Violin(df1, ['day'], 'tip', label='Tip according to Day') + hv.Violin(df1, ['smoker'], 'tip', label='Tip according to\xa0Smokers')).cols(2)"", 'violin.opts( width=600)']","'import pandas as pd'
 'import holoviews as hv\xa0\xa0', 'from holoviews import opts', 'import seaborn as sns', ""hv.extension('bokeh', 'matplotlib')\xa0#extensions used for visualization"", ""df = pd.read_csv('Advertising.csv')"", 'df.head()', ""df1 = sns.load_dataset('tips')"", 'df1.head()', ""vdims = [('Newspaper'), ('Radio'), ('TV')]"", ""ds = hv.Dataset(df, ['Sales'], vdims)"", 'ds', ""layout1= (ds.to(hv.Scatter, 'Sales', 'Newspaper') + ds.to(hv.Scatter, 'Sales', 'TV') +\xa0\xa0ds.to(hv.Scatter, 'Sales', 'Radio')).cols(2)"", 'layout1.opts(opts.Scatter(width=400, height=250))', ""layout2 = (ds.to(hv.Bars, 'Sales', 'Radio'))"", 'layout2.opts(opts.Bars(width=900, height=400))', ""distribution = (hv.Distribution(ds, ['Sales']) + hv.Distribution(ds, ['TV']) +\xa0hv.Distribution(ds, ['Newspaper'])+hv.Distribution(ds,\xa0['Radio'])).cols(2)"", 'distribution.opts( width=400, height=250)', ""title = 'Total Bill according to Gender'"", ""box = hv.BoxWhisker(df1, ['sex'], 'total_bill', label=title)"", ""box.opts( width=600,\xa0 cmap='Set1')"", ""violin= (hv.Violin(df1, ['day'], 'tip', label='Tip according to Day') + hv.Violin(df1, ['smoker'], 'tip', label='Tip according to\xa0Smokers')).cols(2)"", 'violin.opts( width=600)'"
62,overcome_drawbacks_of_pandas/,https://analyticsindiamag.com/hands-on-guide-to-vaex-tool-to-overcome-drawbacks-of-pandas/,"['Pandas is an open-source data analysis and manipulation tool built on python. It is generally used for manipulating numerical and time-series data. It is used to create data structures like a data frame. Pandas is one of the most used python libraries but it has certain drawbacks like it uses a slow function which is not very suitable for bigger datasets, also pandas only handle results that fit in the memory which can be easily filled.', 'To overcome these drawbacks of Pandas, let us explore a high-performance python library for lazy Out-of-Core Dataframes named Vaex which is used to visualize and manipulate big tabular datasets. It performs different statistical functions and visualizations on very large datasets within seconds. Vaex in python uses Lazy computation and Memory mapping in which no memory is wasted. It loads a dataset with billions of rows in a few seconds.\xa0', 'In this article we will explore:', 'We will start exploring vaex but before to that, we need to install it using pip install vaex', 'We will import both pandas and vaex library as we need to compare the performance of both.\xa0', 'import vaex', 'import pandas as pd', 'We will explore how to load a dataset in vaex and perform different operations on it. The dataset we are using here is of NYC Motor Vehicle Collision which is of around 350 MB. We will load this dataset using vaex.', 'df = vaex.open(‘motor_nyc.csv’)', 'df.head(5)', 'Basic Operations on the dataset:', '%%time', ""df['NUMBER OF PEDESTRIANS KILLED'].mean()"", '%%time', ""df['CONTRIBUTING FACTOR VEHICLE 1'].value_counts()"", '%%time', ""df['CROSS STREET NAME'].count()"", 'Now we will visualize some of the plots using the data frame loaded using vaex and note the time using ‘%%time’', '%%time', ""df.plot1d(df['COLLISION_ID']);"", '%%time', ""df.plot(df['NUMBER OF PERSONS INJURED'], df['NUMBER OF PERSONS KILLED']);"", 'Here we can see that despite the dataset being large in size Vaex in python did not take much time to create the plots.', 'Similarly, we can also create several other plots and note the time taken by Vaex as very less compared to pandas.\xa0', 'Vaex has several other features like:', 'd. Vaex V/s Pandas', 'Now let’s compare the time taken by pandas and vaex for different operations.', '#Using Pandas', '%%time', ""df = pd.read_csv('motor_nyc.csv')"", '#Using Vaex', '%%time', ""df1 = vaex.open('motor_nyc..csv')"", 'Here we can see that while pandas took 18 seconds Vaex loaded the same dataset in 23 milliseconds.', '#Using Pandas', '%%time', ""print(df['NUMBER OF PEDESTRIANS KILLED'].mean())"", ""print(df['NUMBER OF PEDESTRIANS KILLED'].value_counts())"", '#Using Vaex', '%%timeit', ""print(df1['NUMBER OF PEDESTRIANS KILLED'].mean())"", ""print(df1['NUMBER OF PEDESTRIANS KILLED'].value_counts())"", 'Here also we can see that vaex is incredibly faster than pandas.', 'Similarly, we can try different operations using both pandas and vaex to find out that Vaex is faster than pandas.\xa0\xa0', 'Conclusion:\xa0']","'import vaex'
 'import pandas as pd', 'df = vaex.open(‘motor_nyc.csv’)', 'df.head(5)', '%%time', ""df['NUMBER OF PEDESTRIANS KILLED'].mean()"", '%%time', ""df['CONTRIBUTING FACTOR VEHICLE 1'].value_counts()"", '%%time', ""df['CROSS STREET NAME'].count()"", '%%time', ""df.plot1d(df['COLLISION_ID']);"", '%%time', ""df.plot(df['NUMBER OF PERSONS INJURED'], df['NUMBER OF PERSONS KILLED']);"", '#Using Pandas', '%%time', ""df = pd.read_csv('motor_nyc.csv')"", '#Using Vaex', '%%time', ""df1 = vaex.open('motor_nyc..csv')"", '#Using Pandas', '%%time', ""print(df['NUMBER OF PEDESTRIANS KILLED'].mean())"", ""print(df['NUMBER OF PEDESTRIANS KILLED'].value_counts())"", '#Using Vaex', '%%timeit', ""print(df1['NUMBER OF PEDESTRIANS KILLED'].mean())"", ""print(df1['NUMBER OF PEDESTRIANS KILLED'].value_counts())"""
63,target_based_eda_tool/,https://analyticsindiamag.com/hands-on-tutorial-on-exploripy-effortless-target-based-eda-tool/,"['Exploratory Data Analysis is the initial step that should be performed on a dataset in order to know about the properties of the different attributes of the dataset. EDA gives us an idea of what all columns do data have, what are the values in these columns, what are the datatypes, etc. Other than that EDA also helps in visualizing the relationships between different columns/ attributes in the dataset.', 'Exploratory data analysis is an approach to analyze the datasets in order to summarize their main characteristics with both statistical and visual methods. In order to perform EDA in python, we generally use different libraries like pandas, NumPy, matplotlib, etc. to know about different properties of the dataset. Using all these libraries for EDA takes a lot of time and effort.', 'ExploriPy is an open-source python library that can be used for EDA and make the whole process a lot easier and effortless. It automates the whole process of EDA and saves a lot of time which can be used in other tasks. It works in just a few lines of code so no prior hardcore coding experience is required to use ExploriPy.', 'In this article, we will explore ExploriPy to perform EDA on a dataset and derive useful insights.', 'Like any other library, we will start by installing ExploriPy using pip install ExploriPy.', 'For loading the dataset we will use pandas so we need to import that and for EDA we will use ExploriPy so we will also import that.', 'from ExploriPy import EDA', 'import pandas as pd', 'In this article, we will use a dataset of Advertisement Dataset of an MNC in which Sales is the Target Variable which is dependent on certain features like ‘TV’, ‘Radio’, etc.es of automobiles like ‘price’, ‘height’, ‘length’, etc.\xa0', 'df = pd.read_csv(‘Advertisement.csv’)', 'df.head()', 'For Exploratory data analysis, we will use Exploripy. As we are using the advertising dataset and we know that sales are the target variable so we will pass it as the target variable.', 'ContinuousFeatures = [‘Radio’,’Newspaper’,’TV’]', 'analysis = EDA(df,title=’EDA for Sales Data’)', 'analysis.TargetAnalysis(‘Sales’)', 'After running the above-said commands we will be generating an EDA report with Sales as our target Variable.', 'Now let us explore different sections of the EDA report.', 'This page shows what are the different attributes of the dataset along with their datatypes. Also, we can see here a pie-chart which clearly shows the distribution of the attributes in categorical or continuous variables.\xa0', 'This segment of the report shows which attributes have missing data or null data and also represent it in the form of bar charts. The dataset we used has no missing data so it is showing as Null Percentage = 0%.', 'Here we can see the statistical properties of the target variable along with the number of records that it contains. Statistical properties contain Skewness and Kurtosis also.', 'In this section, we can clearly see how our Sales data is distributed with the help of different visualizations namely Boxplot, KDE Plot, and histogram.', 'This is the end of the report which shows the distribution of the feature variables.', 'Similarly, we can generate Target Specific EDA reports for different datasets.']",
64,visualize_pandas_data_structure/,https://analyticsindiamag.com/dtale-tutorial-guide-to-visualize-pandas-data-structure/,"['Analyzing a data structure helps us gather information about the data like how it is stored, what are the different attributes and their properties. Data Analysis can be performed using different python libraries like pandas, etc.', 'DTale is a Flask and React-based powerful tool which is used to analyze and visualize pandas data structure seamlessly. It supports different objects like Data Frame, Series, etc. It works beautifully on both the Jupyter notebook and the command-line interface.\xa0', 'DTale is a Graphical Interface where we can select the data we want to analyze and how to analyze using different graphs and plots.\xa0', 'In this article, we will explore Dtale with all its functionalities.', 'Like any other python library, we need to install DTale before using it by pip install dtale.', 'We will import Dtale before using it, also we will be downloading our dataset from plotly so we also need to import plotly. You can use any dataset you have.', 'import dtale', 'Import plotly.express as px\xa0', 'import pandas as pd', 'For this article, we will be downloading the dataset named ‘tips’ using Plotly. The dataset contains different attributes of restaurant data.\xa0', 'df = px.data.tips()', 'df.head()', 'd = dtale.show(df)', '#open it in a new window in browser', 'd.open_browser()', 'This is the data frame visualized using Dtale. Here we can change the value of any attribute to our desired value. In this image, you can see the highlighted play button.', 'When we click on this button we see different functions that are displayed in the image below.', 'Here we can see that it provides us with different options like:', 'These different functions are used to analyze the dataset and how we want to want to manipulate the data according to our demands. We can visualize the dataset by adding new attributes to our dataset and also export the dataset with the required changes.', 'Now let us analyze the column-wise functions and see what all operations we can perform on the columns of the dataset.', 'By clicking on the column name we can see a dropdown menu as shown in the image above. The functions defined here are:', 'We can click on different options/functions to see how it works.', 'Describe function is used to display the statistical properties of the dataset. Lets us analyze the statistical properties of this dataset by clicking the describe option.', 'Here we can see that we can analyze the statistical properties of different attributes by selecting them using the checkbox provided. Also, you can visualize the histogram of the data by clicking on the Histogram option.\xa0', 'Not only this you can also copy the code which is running in the backend to show the current visualization by clicking on the code export option.\xa0', '2. Visualization', 'Here we can plot correlations between different attributes. We can select these attributes using widgets. It not only allows you to visualize but also displays the value of the correlation.', 'Charts help us visualize all different kinds of charts between different attributes. It also supports 3d Charts and animations with the group by and aggregation functions also. It is really useful as you can visualize any type of relationship using different charts.', '3. Manipulating Data', 'Other functions like Highlight Range, Highlight dtypes, Highlight Missing, etc. are used to manipulate the data according to our wish and what we want to visualize or analyze. It helps us take out the outliers, ambiguous values, any irregularity in the data, etc.\xa0', 'Dtale helps us to do an Exploratory Data Analysis of the dataset and that too without even writing more than one line of code. It is a GUI Based Exploratory Data Analysis tool that analyzes and visualizes each and every aspect of the dataset. It is easy to use and blazingly fast.', 'Dtale can be used to create detailed EDA reports and that too with minimum effort and maximum output. If we go by the conventional way of creating the EDA report it will take a lot of time while with Dtale we can create it in an hour.']","'import dtale'
 'Import plotly.express as px\xa0', 'import pandas as pd', 'df = px.data.tips()', 'df.head()', 'd = dtale.show(df)', '#open it in a new window in browser', 'd.open_browser()'"
65,time_series_with_tsfresh/,https://analyticsindiamag.com/a-guide-to-feature-engineering-in-time-series-with-tsfresh/,"['Feature engineering plays a crucial role in many of the data modelling tasks. This is simply a process that defines important features of the data using which a model can enhance its performance. In time series modelling, feature engineering works in a different way because it is sequential data and it gets formed using the changes in any values according to the time. In this article, we are going to discuss feature engineering in time series and also we will cover an implementation of feature engineering in time series using a package called tsfresh. The major points to be discussed in the article are listed below.', 'Table of contents', 'Let’s start with understanding what features engineering in time series.', 'Feature engineering in time series', 'In supervised learning, feature engineering aims to scale strong relationships between the new input and output features. Talking about the time series modelling or sequential modelling we don’t feed any input variable to the model or we don’t expect any output variable (input and outputs are in the same variable ). Since the features of data and methods that we know about the time series modelling work in a different nature. This data consists of time features in the data with some values that are changing with the time feature.\xa0', 'By looking at such data we can say that features of any time series data are the time or the main feature we use in modelling is time and that is also responsible for predicting good results. In time series we are required to perform feature engineering with the time variable. For example, we may require engineering dates from the year where we are finding the value of our sales is increasing. Time series data gets created using the independent variable where time is the most common variable. The below image can be an example of a time series plot.', 'Image source', 'In the plot, we can see that we have changed the values as time moves on. We may want to find some of the characteristics from this data. Examples of characteristics can be the maximum or minimum value, average value, or temporary peak in the data. Take an example from the below image.', 'Image source', 'Here we can see what the feature can consist of by time-series data. We can perform time-series feature engineering using the tsfresh package. Let’s understand what is tsfresh.', 'What is tsfresh?', 'tsfresh is an open-source python package that can help us in feature engineering of time series data. As we have discussed before, the time series is sequential data so this package can also be used with any kind of sequential data. One thing that is mandatory about the data it should have generated using an independent variable. For example, in time-series data, we find the time variable is an independent variable.\xa0', 'Utilizing this tool we can extract features and we can perform analysis based on the new insights. Feature extraction is also helpful in making clusters from the time series or we can also perform classification and regression tasks using feature extraction.', 'One more thing that can be very useful for us in this package is that it is compatible with pandas library for data manipulation and also it is compatible with the sklearn library that helps us in providing various machine learning models. Let’s discuss how this package can be used.', 'Implementation\xa0', 'We can install this package using the following lines of codes:', '!pip install tsfresh', 'After the installation, we are ready to use the package. To understand the nature of working of tsfresh we are going to perform a classification task using tsfresh provided dataset that consists of information about robot failure. In the data, we find that each robot has collected time series from different sensors. Let’s load the data.', 'Defining x and y', 'x, y = load_robot_execution_failures()', 'Here we have features of the data in x instance and the target feature is in the y instance.', 'x.head()', 'y.head()', 'In the head of x we can see that we have 6 columns in the data name as F_x, F_y, F_z, T_x, T_y, T_z are the time series and the id column gives the different robots. Let’s visualize some of the data using the matplotlib.', 'In the above output, we can see the time series for sample id 4 that is not representing any failure. Let’s check for another sample.', 'Here we can see that we have got some failures. Let’s check whether our model can capture these details or not.\xa0\xa0\xa0', 'Extracting features', 'Here the process of feature extraction from time series is completed. Let’s see how many features we have from these different time series.', 'features', 'Here we can see 88 rows and 4734 columns in extracted features. There can be many non-values in extracted features that can be removed using the following lines of codes.', 'Now we can select relevant features using the following lines of codes.', 'In the above output, we can see that there are only 682 columns presented after the filtration of the features. Here we have performed two things with the data: first, we extracted the features from the data and second, we have filtered the extracted features.\xa0', 'Now we can compare the results of any model from sklearn using the data with all features and the data with filtered features. Let’s split the data into tests and train.', 'Let’s fit the data with all the features in a decision tree model.', 'Here we have got some good results with all the features. Let’s check a model using data that has been filtered.', 'Here we can see that a similar model has improved its performance with the filtered model.', 'Final words', 'Here in this article, we have discussed feature engineering in time series. Along with that, we have discussed a python package named tsfresh, that can be used in feature engineering of time series. Using some of the modules we have performed feature engineering and after feature engineering, we find some improvements in the model performance.']","'!pip install tsfresh'
 'from tsfresh.examples.robot_execution_failures import download_robot_execution_failures, \\\n    load_robot_execution_failures\ndownload_robot_execution_failures()', 'x, y = load_robot_execution_failures()', 'x.head()', 'y.head()', ""import matplotlib.pyplot as plt\nx[x['id'] == 4].plot(subplots=True, sharex=True, figsize=(12,12))\nplt.show()"", ""x[x['id'] == 14].plot(subplots=True, sharex=True, figsize=(12,12))\nplt.show()"", 'from tsfresh import extract_features\nfeatures = extract_features(x, column_id=""id"", column_sort=""time"")', 'features', 'from tsfresh.utilities.dataframe_functions import impute\nimpute(features)', 'from tsfresh import select_features\nfiltered_features = select_features(features, y)\nfiltered_features\n', 'from sklearn.model_selection import train_test_split\nX_feature_train, X_feature_test, y_train, y_test = train_test_split(features, y, test_size=.4)\nX_filtered_train, X_filtered_test = X_feature_train[filtered_features.columns], X_feature_test[filtered_features.columns]', 'from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\nclassifier_feature = DecisionTreeClassifier()\nclassifier_feature.fit(X_feature_train, y_train)\nprint(classification_report(y_test, classifier_feature.predict(X_feature_test)))', 'classifier_filtered = DecisionTreeClassifier()\nclassifier_filtered.fit(X_filtered_train, y_train)\nprint(classification_report(y_test, classifier_filtered.predict(X_filtered_test)))'"
66,effective_time_series_analysis/,https://analyticsindiamag.com/a-guide-to-kats-python-tool-by-meta-for-effective-time-series-analysis/,"['Time series analysis and forecasting are the tasks in machine learning that require complex, time-consuming, and huge efforts for analyzing the data. Keeping these efforts in mind, researchers at Facebook (now Meta) have created a library called Kats. This library can be very useful and easy to use in the context of performance and high-level codes. In this article, we will discuss the functionality of the Kats library along with its implementation in some useful time series analysis tasks. The major points to be discussed in this article are listed below.', 'Table of contents', 'Let’s start with understanding what Kats is.', 'What is Kats?', 'Kats stands for Kits to Analyze Time Series, which was developed by the researchers at Facebook, now Meta. One of the most important things about Kats is that it is very easy to use. Also, it is a very light weighted library of generic time series analysis in a very generalized nature. That means we can easily use it without spending so much time processing time series and calculations in different models.\xa0', 'This library provides us with models from traditional to advanced timings. Using the set of algorithms from Kats we can perform the following things in the time series analysis field:', 'By looking at the above points, we can say that Kats can be a very important part of our time series analysis project. In the next sections, we will look at how easy it is to use this library. Before going to perform any operation we are required to install this library in our environment. In the Google Colab environment, we can easily install it using the following lines of code.\xa0', '!pip install kats', 'In the above output, we can see the list of packages that are required to install this library.', 'Time series analysis with Kats', 'Now we are ready to use this library. Let’s start our time series analysis using Kits.', 'Loading the data', 'In various libraries, we find that the style of data is required to be set according to the module of the library. This library is different in that we can perform our analysis in time series using the normal Pandas data frame. In this article, we are going to use the air passenger data set, and using this data we are going to perform some analysis using the modules and functionality of Kats. We can find a copy of the airpassenger.csv dataset here.', 'Here in the dataset, we can see that we have dates in one column and the number of passengers in other columns. After loading the data we will use our first module from the Kats library. The name of the module is TimeSeriesData. This module helps us in creating the object for time series in the required form of the library. We can find this module in the kat.consts part.', 'Here we can see that the time series is a TimeSeriesData object.\xa0 Since we are working on time series data there is always a requirement for one column to be time and the other to be different values. In any case of confusion where we have multiple variables, we can define them by name in this module, like following lines of code.', 'df_from_series = TimeSeriesData(time=df.time, value=df.value)', 'Using this module we can also perform various changes on the data type of time series. For example, we can change the standard datetime to pandas timestamp, str or int.\xa0 With all these, we get facilities of different operations that can be used in time series analysis such as,', 'Let’s cross-check it by slicing and plotting our data.', 'Let’s plot our data', ""df.plot(cols=['value'])"", 'Here we have plotted our time series and it took only one line of code. Now after analyzing our dataset we are ready to make forecasts using any of the models.', 'Forecasting with Kats', 'In the above section, we have discussed the module TimeSeriesData of Kats. in this section we will discuss the modelling procedure which we are required to follow to forecast some values.\xa0', 'Normally, in time series forecasting we find the usage of different models like ARIMA family models, VAR models, etc. using the Kits library we can perform the following time series modelling:\xa0', 'In this library, we have special modules for all of the above-given models that follow the sklearn model API pattern. That means we perform modelling using the following steps:', 'For example, we can perform the time series modelling with the prophet model. Let’s start with importing the classes for the prophet model.', 'Creating the instance of parameters of the model:', 'Creating an instance of prophet model', 'm = ProphetModel(df, params)', 'Fitting the model using fit function', 'm.fit()', 'Here in the output, we can see some of the information regarding our modelling that depends on our parameter setting.\xa0', 'Let’s Forecast some of the values using the fitted model using the predict function.', 'In the above output, we can see our prediction, where we have some upper and lower values of the prediction. This means that we have a range in between our prediction of what lies ahead. That is a good way of making predictions. Let’s visualize our prediction.', 'm.plot()', 'Here in the visualization, we can see our predicted value. Or we can say the predicted values from the library because we haven’t put so much effort into making predictions.', 'In the above section, we have seen how easily we can perform time series modelling using the Kats library. Now we will discuss the functionality of detection methods.', 'Detection with Kats', 'In this section, we will discuss the functions and functionality that Kats provides us to perform on the time series data. We mainly use these methods to find out the patterns of the time series. In summarization we can utilize the following functionality from Kats for detection:', 'In the above, we have discussed different detection functionality that can be used with the Kat library. In this article, we will look at how we can use OutlierDetector for generating a generalized understanding.', 'Using the below lines of code we can add outliers in our air passenger data set.', 'Here in the output, we can see that we have added some outliers in our dataset.\xa0 After converting the data into a TimeSeriesData object we will be ready to detect outliers.', 'Calling and fitting OutlierDetector', 'Here we can see our outlier and to delete it from data we can use the remover function of the same module.', 'In the output, we can see missing values in the data. We can also convert them to linear values using the parameter interpolate = true.', 'Here we have seen how we can use detection functionality from the Kats library. Now in the next section, we will see how we can extract features from data using the Kats library.\xa0', 'Feature extraction with Kats', '\xa0\xa0Features of any time series play an important role in time series analysis and the accuracy of time series forecasting. Using the TsFeature of the Kats library we can calculate various features of any time series. Some of the examples of features are as follows:', 'Let’s see how many others are there that we can extract.', 'Instantiating feature extractor instance', 'Extracting features\xa0', 'df_features= TsFeatures().transform(df)', 'Displaying the list of features\xa0', 'df_features', 'Here in the output, we can see extracted features from the TsFeatures module. In time series modelling these features can also be extracted using the features of the R language.\xa0', 'Final words', 'In this article, we have seen some of the basic steps that are required to follow in time series analysis. For taking those steps, we used the Kats library and saw how easily we have performed all of them. I encourage readers to use this library in their time series analysis projects and find what else we can do with it.\xa0\xa0']","'!pip install kats'
 'import pandas as pd\nimport numpy as np\ndf = pd.read_csv(""/content/air_passengers.csv"")\ndf.columns = [""time"", ""value""]\ndf.head()\n', 'from kats.consts import TimeSeriesData\ndf = TimeSeriesData(df)\nprint(type(df))', 'df_from_series = TimeSeriesData(time=df.time, value=df.value)', '# Creating two slices\nts_1 = df[0:3]\nts_2 = df[3:7]\n \nts_1.extend(ts_2)\nts_1\n', ""df.plot(cols=['value'])"", 'from kats.models.prophet import ProphetModel\nfrom kats.models.prophet import ProphetParams', ""params = ProphetParams(seasonality_mode='multiplicative')"", 'm = ProphetModel(df, params)', 'm.fit()', 'forecast = m.predict(steps=30, freq=""MS"")\nforecasting.head()\n', 'm.plot()', ""outlier_df = df.copy(deep=True)\noutlier_df.loc[outlier_df.time == '1950-12-01','value']*=5\noutlier_df.loc[outlier_df.time == '1959-12-01', 'value']*=4\noutlier_df.plot()\n"", 'outlier_ts = TimeSeriesData(outlier_df)\nprint(type(outlier_ts))', ""from kats.detectors.outlier import OutlierDetector\n \nts_outlier = OutlierDetector(outlier_ts, 'additive') \nts_outlier.detector() \nts_outlier.outliers\n"", ""outliers_removed = ts_outlier.remover(interpolate = False)\noutliers_removed.plot(cols=['y_0'])"", 'from kats.tsfeatures.tsfeatures import TsFeatures\ntsFeatures = TsFeatures()\n', 'df_features= TsFeatures().transform(df)', 'df_features'"
67,it_with_python_code/,https://analyticsindiamag.com/merlion-salesforces-latest-time-series-library-how-to-use-it-with-python-code/,"['Cloud-based software company, Salesforce released Merlion this month, an open-source Python library for time series intelligence. It is used for time series analysis and provides an end-to-end machine learning framework that includes loading and transforming data, building and training models, post-processing model outputs, and evaluating model performance. Along with this, we will also learn to implement anomaly detection in time series using Merlion. The major points to be discussed in this article are listed below.\xa0\xa0', '\xa0Let’s start with understanding the Merlion package.', 'It is an open-source time-series machine learning library that has a uniform interface for various commonly used models and datasets for anomaly detection and forecasting on univariate and multivariate time series, as well as conventional pre/post-processing layers. It includes numerous modules to increase use, such as visualization, anomaly score calibration to improve interoperability, AutoML for hyperparameter tuning and model selection, and model assembly.\xa0', 'Merlion also offers a one-of-a-kind evaluation system that replicates live model deployment and re-training in production. This library intends to provide engineers and researchers with a one-stop solution for fast developing and benchmarking models for their specific time series needs across numerous time-series datasets.', 'It provides an end-to-end machine learning framework that covers data loading and transformation, model development and training, model output post-processing, and model performance evaluation. Apart from these Merlion is:', 'Merlion’s module architecture is divided into five layers:-', 'Merlion employs a wide range of models for forecasting and anomaly detection. Among these are statistical methods, tree-based models, and deep learning approaches. To transparently expose all of these possibilities to an end-user, the engineering team has unified all Merlion models under two common APIs, one for forecasting and the other for anomaly detection. All models start with a config object containing implementation-specific hyperparameters and support a model. method train(time series). Now let’s move to the implementation part where we implement anomaly detection and Forecasting a series.\xa0\xa0\xa0', 'Merlion includes a number of models that are optimized for detecting univariate time series anomalies. These are classified into two types: forecasting-based and statistical. Forecasters in Merlion are simple to modify for anomaly identification because they predict the value of a specified univariate in a generic time series. The anomaly score is just the difference between the expected and true-time series values, optionally normalized by the predicted standard error of the underlying forecaster (if it produces one).', 'To start using merlion first we need to install it, we can install it either by using the PIP command or by cloning the repository. Check here for the instructions for installing the package.\xa0', 'Merlion comes with a data loader package called ts_dataset it basically implements certain python-based Classes which help to manipulate numerous time-series datasets into standardized pandas data frames. The submodules of it like ts_dataset.anomaly and ts_dataset.forecast are used to load the dataset for anomaly detection and forecasting a series respectively.\xa0', 'For anomaly detection, we are using the NAB(Numenta Anomaly Benchmark) dataset. NAB is a new benchmark for evaluating algorithms in streaming, real-time applications for anomaly detection. It consists of more than 50 labeled real-world and artificial time series data files. We are using Merlion’s standard data class called TimeSeries from the subpackage utils which can handle both univariate and multivariate time series data. This class wraps a collection of Univariate time series in a single class.\xa0', 'The below code shows the use case of both ts_dataset and TimeSeries class, and we are splitting the NAB train and test set and will take a glimpse of the obtained time series.', 'Merlion’s DefaultDetector, which is an anomaly detection model that balances performance and efficiency, may now be initialized and trained. On the test split, we also get its predictions.', 'Now visualize the prediction, for visualization merlion comes with a visualization package that gives us a very interactive and informative visualization of our predictions.', 'Finally, we may assess the model quantitatively by using the evaluate package. Merlion’s evaluation implements utility and metrics by which we can evaluate the performance of our time series task.', 'As we can see in the plot, the model fired three alarms, with three true positives,\xa0 and one false negative, resulting in precision and recall. By using the evaluation package we can also look at the average time it took the model to accurately detect each abnormality as shown below.']","""from merlion.utils import TimeSeries\nfrom ts_datasets.anomaly import NAB\n \ntime_series
 metadata = NAB(subset='realTweets')[5]\ntrain_data = TimeSeries.from_pd(time_series[metadata.trainval])\ntest_data = TimeSeries.from_pd(time_series[~metadata.trainval])\ntest_labels = TimeSeries.from_pd(metadata.anomaly[~metadata.trainval])\n"", 'from merlion.models.defaults import DefaultDetectorConfig, DefaultDetector\n# initialize,train, and test the detector\nmodel = DefaultDetector(DefaultDetectorConfig())\nmodel.train(train_data=train_data)\ntest_pred = model.get_anomaly_label(time_series=test_data)\n', 'from merlion.plot import plot_anoms\nimport matplotlib.pyplot as plt\nfig, ax = model.plot_anomaly(time_series=test_data)\nplot_anoms(ax=ax, anomaly_labels=test_labels)\nplt.show()\n', 'from merlion.evaluate.anomaly import TSADMetric\n#Precision Score\np = TSADMetric.Precision.value(ground_truth=test_labels, predict=test_pred)\n# Recall Score\nr = TSADMetric.Recall.value(ground_truth=test_labels, predict=test_pred)\n# F1 Score\nf1 = TSADMetric.F1.value(ground_truth=test_labels, predict=test_pred)\n# returns mean time taken to detect anomaly\nmttd = TSADMetric.MeanTimeToDetect.value(ground_truth=test_labels, predict=test_pred)\nprint(f""Precision: {p:.4f}, Recall: {r:.4f}, F1: {f1:.4f}\\n""\n      f""Mean Time To Detect: {mttd}"")\n'"
68,an_open_source_library/,https://analyticsindiamag.com/merlion-an-open-source-library/,"['Cloud-based software company, Salesforce has released Merlion, an open-source Python library for time series intelligence.', 'Merlion provides an end-to-end machine learning platform that includes data loading, data transformation, and model construction. After training is complete, post-processing might encompass various tasks, such as evaluating model performance or making accurate predictions about what will happen in the future based on historical trends.', 'The library provides a uniform interface for models and datasets and conventional pre/post-processing layers to detect abnormalities in both univariate and multivariate time series. Merlion is easy to use, especially for visualisation, anomaly score calibration for better interpretability, AutoML for hyperparameter tuning and model selection, and model ensembling. Merlion’s evaluation system simulates a model’s live deployment and retraining in a production environment. It can offer engineers a one-stop-shop for creating models with unique time series requirements.', 'Merlion’s distinguishing characteristics include the following:']",
69,to_recbole_recommendation_algorithms/,https://analyticsindiamag.com/hands-on-guide-to-recbole-recommendation-algorithms/,"['Have you ever noticed that many of the product selling, service providing, and content providing companies like amazon, Oyo, youtube, Netflix, and Flipkart provide suggestions or recommendations about their products, services, and content us? Many of the recommendations we use because we know the quality of those products and many of the products we use because the platform where we are surfing for products or services knows about the product that hopefully we may like or require to use in future. How do those recommendations come out in front of us while using the platforms? The companies we are talking about are some leading companies in the market in today’s scenario. One of the factors that adds to their current stature is their recommendation system.\xa0', 'Recommendation systems are designed to give recommendations about the user’s services, products, or items based on different factors. These systems predict probabilities about the products which are used by users most likely.\xa0 Using those recommendation systems, a user can figure out their requirements. These systems deal with the information provided by the user about the user or about the product they want to use. It finds out the match between users information and product requirements and imputes the similarity between users and items for recommendation.', 'Recommendation systems are beneficial for both sides, for the user side as well as the provider. It benefits the user to make it easy and save time for searching the things he is interested in and for the provider to place his services in the right place and improve the user experience.\xa0 Many things can be recommended like movies, books, news articles, videos, etc., and many platforms are using it to grow their business.', 'There are many types of recommendation systems. Some of them are-', 'These all systems are somewhere having different algorithms for implementation in a platform. And to assign recommendation systems to a platform we all need to imply all recommendation systems differently. Because, commonly, every business needs higher user engagement in their platform to grow more. If a user finds the platform useful for him, he uses the platform frequently, and a recommendation system helps provide a better user experience to platform users.\xa0', 'Choosing recommendation systems to embed with a platform requires a lot of knowledge. Also, it is the subject of research because a single failure can cause the loss of several customers. And that can also harm the business too. And this subject is too deep; what we have discussed earlier is just the basic things about the recommendation system. Implementation of the recommendation system is needed to be very accurate, and also, by the time of implementation, it should give a high level of accuracy and proper recommendations. Here RecBole takes part in, which can make these all tasks very easy to perform. Next, in the article, we will discuss the RecBole framework designed for research purposes in recommendation systems.', 'RecBole,’s library consists of 72 Algorithms of recommendation systems. It is developed in Python on PyTorch framework. It produces recommendation algorithms in a unified and comprehensive way by covering four major categories of recommendation algorithms.', 'Also, with all these algorithms in its library, we have 28 recommendation data sets available, which are designed to be flexible with the algorithms. There are some scripts also available to preprocess the datasets to fit in those algorithms. The following image can show the overall architecture of the RecBole.', 'Image source', 'Let’s see how we can work with RecBole.\xa0', 'Requirements – python 3.6 or above version,\xa0 torch version 1.6.0 or above , CUDA version 9.2 or above, NVIDIA driver version >= 396.26 (for Linux) or >= 397.44 (for Windows10).', 'Here in this article, we are using google colab, so we need to start with mounting the drive in our colaboratory; we can use the following codes for mounting the drive.', 'We can access the authorization code from the provided link after copying the code, paste it in the widget, and press enter. Our google drive will get mounted in the drive.', 'We can install the Recbole using the following command.', 'Or we can directly clone the package from GitHub using the following command.', 'After cloning the package, we just need to direct the colaboratory to the package folder.', 'After directing the colaboratory, we can install the Recbole framework using the following command.', '!pip install -e . --verbose', 'After installation, we can run the provided script by the package for initial usage of the library. This script will run the BPR model in ml- 100k dataset. The data sets consist of movie title \xa0 release year and their class like a thriller, action, adventure etc., with their unique id.', '!python run_recbole.py', 'That will provide different outputs, which will consist of details of the system, model, data set, and final result.', 'In the above output, we can see the system configuration of the whole process.', 'In the above output, we can see the details of the model in training.', 'In the above output, we can see the evaluation metrics of the script.', 'In the above output, we can see the details of the data.', 'And at the end, we can see the best result of the algorithm.', 'We can also set the different parameters using the following command.', 'In this command, the hyper.test file consists of those parameters.', 'Where loguniform indicates, the parameter will follow the randomly taken values between\xa0 e^{-8}- e^{0} in a uniform distribution. And choice allows you to take discrete values from the setting list.', 'Basically, this file is providing the hyperparameter tuning of the model. We can use different numbers in the hyper.test file available in RecBole/hyper.test location.', 'For example, I am setting the parameters in the following codes.', '!python run_hyper.py --model=BPR --dataset=ml-100k --params_file=hyper.test', 'The output will start like this, and it will take some time to get completed.', 'Here is the final result of the model after hyperparameter tuning.', 'Validation result.', 'Test result:', 'Here we can see the results are quite satisfying, and also, we can just make a recommendation system in only a few steps. The library is tested on different datasets like Ml-1m, Netflix, yelp data. We can check the whole details of the models and performance in this link.\xa0']","""from google.colab import drive\n\ndrive.mount('/content/drive')""
 '!pip install recbole', '!git clone https://github.com/RUCAIBox/RecBole.git', 'cd RecBole', '!pip install -e . --verbose', '!python run_recbole.py', 'python run_hyper.py --model=[model_name] --dataset=[data_name] --config_files=xxxx.yaml --params_file=hyper.test', 'python run_hyper.py --model=[model_name] --dataset=[data_name] --config_files=xxxx.yaml --params_file=hyper.test', ""learning_rate loguniform -8, 0embedding_size choice [64, 96 , 128]\ntrain_batch_size choice [512, 1024, 2048]\nmlp_hidden_size choice ['[64, 64, 64]','[128, 128]']"", 'learning_rate loguniform -8, 0', '!python run_hyper.py --model=BPR --dataset=ml-100k --params_file=hyper.test'"
70,for_time_series_forecasting/,https://analyticsindiamag.com/linkedin-open-sources-this-new-library-for-time-series-forecasting/,"['Recently, LinkedIn introduced a new open-sourced Python library, Greykite, to provide flexible, intuitive and fast time series forecasts. The library was developed to support the forecasting needs of the online professional networking platform.\xa0', 'Greykite library provides a framework to develop a robust forecast model using outlier/anomaly preprocessing, grid search, exploratory data analysis, benchmarking, feature extraction and engineering, evaluation, and plotting.', 'For a few years now, forecasting business metrics and quantifying their volatility has become a go-to technique for most organisations. Through the long-term forecast, organisations can have an outlook and expectations about future growth or future resource requirements. In contrast, short-term forecasts can be used to detect anomalies in the system.\xa0', 'Greykite contains a simple modelling interface that facilitates data exploration and model tuning. The library provides intuitive forecasts through its flagship algorithm, known as Silverkite. Silverkite is highly customisable and includes tuning parameters to capture diverse time series characteristics.\xa0', 'In an official blogpost, LinkedIn said, “The Silverkite algorithm works well on time series with (potentially time-varying) trends and seasonality, repeated events/holidays, and/or short-range effects. At LinkedIn, we’ve successfully applied it to a wide variety of metrics in different time frequencies (hourly, daily, weekly, etc.), as well as various forecast horizons, e.g., 1 day ahead (short-term) or 1 year ahead (long-term).”', '', 'The professional networking platform uses time series forecasts for various processes, such as to provision sufficient infrastructure to handle peak traffic, to optimize budget decisions by forecasting growth of various markets, to set business metric targets and track progress for operational success, among others. The developers at LinkedIn designed this library in order to solve these types of problems.\xa0', 'Besides Silverkite, Greykite also supports Prophet, which is an open-source algorithm for forecasting time series data. The Prophet algorithm is developed by Facebook and is robust to missing data and shifts in the trend as well as handles outliers well.', 'The Greykite library comes with various capabilities for model assessment, validation, and testing that are appropriate for the time series context. The library offers components that could be used within other forecasting libraries or even outside the forecasting context. The library includes a number of features-', 'The Greykite library provides a highly customizable algorithm Silverkite for time series forecasting. The library also provides intuitive tuning options and diagnostics for model interpretation.\xa0']",
71,prophet_with_python_code/,https://analyticsindiamag.com/comprehensive-guide-to-facebooks-prophet-with-python-code/,"['Prophet, a Facebook Research’s project, has marked its place among the tools used by ML and Data Science enthusiasts for time-series forecasting. Open-sourced on February 23, 2017 (blog), it uses an additive model to forecast time-series data. This article aims at providing an overview of the extensively used tool along with its Pythonic demonstration.', 'NOTE: ‘Trend’ in time-series refers to an overall change in the data with time. While the term ‘seasonality’ means the way the data changes over a specific period e.g. week, month, year etc.', 'Image source: Facebook blog', 'Prophet employs an additive regression model having four constituents at its core:', 'Here’s a demonstration of using Python API for forecasting avocados’ prices using Prophet. The dataset used is available on Kaggle. The code implementation has been done using Google Colab and fbprophet 0.7.1 library. Step-wise implementation of the code is as follows:', '!pip install fbprophet', ""df = pd.read_csv('avocado.csv')"", 'df.head()', 'df.info()', 'df1 = df.sort_values(""Date"")', 'Display some initial records of the sorted data.', 'df1.head()', 'First, get the minimum and maximum dates in the historical data.', 'df1[‘Date’].min()', 'df2[‘Date’].max()', 'These outputs show that we have records from January 2015 to March 2018.Plot the prices of that period.', 'The plot shows that the data is balanced i.e. equally distributed region-wise.', ""sns.countplot(x='year',data=df1)"", 'Extract the two required columns', 'Rename the columns', 'Create a Prophet instance', 'm = Prophet()', 'Fit the historical data', 'm.fit(df1)', 'Create a DataFrame with future dates for forecast.\xa0', 'df1 has dates till 25/3/2018 so ‘future’ will be till 25/3/2019. Predict the prices for this new data having future dates as well', 'forecast = m.predict(future)', 'Get information on the ‘forecast’ DataFrame created by Prophet.', 'forecast.info()', 'Display a few initial records of ‘forecast’.', 'forecast.head()', 'Condensed output:', '11)\xa0 Plot the data with recorded as well as forecasted prices.\xa0\xa0\xa0 \xa0', 'Our original data had monthly records till February 2019. The blue-shaded portion of the\xa0\xa0above plot shows the prices predicted for the next one year’s span, i.e. till February 2019.', 'Actual recorded prices have been marked with black dots in the above plot, while the\xa0The blue non-linear line shows the average predicted prices.', 'figure = m.plot_components(forecast)', 'Extract data of the required region from the original data.', ""df2 = df[df['region']=='West']"", 'Display initial records.', 'df.head()', ""df2 =\xa0 df2.sort_values('Date')"", 'Plot the recorded prices for that specific region.', 'Forecast prices for the next one year for that specific region.', '(Black dots: actual price values, Blue curve: predicted prices)', 'figure = m.plot_components(forecast)']","'!pip install fbprophet'
 ""df = pd.read_csv('avocado.csv')"", 'df.head()', 'df.info()', 'df1 = df.sort_values(""Date"")', 'df1.head()', 'df1[‘Date’].min()', '2015-01-04', 'df2[‘Date’].max()', '2018-03-25', ""sns.countplot(x='year',data=df1)"", 'm = Prophet()', 'm.fit(df1)', 'forecast = m.predict(future)', 'forecast.info()', 'forecast.head()', 'figure = m.plot_components(forecast)', ""df2 = df[df['region']=='West']"", 'df.head()', ""df2 =\xa0 df2.sort_values('Date')"", 'figure = m.plot_components(forecast)'"
72,series_forecasting_python_library/,https://analyticsindiamag.com/guide-to-giotto-time-a-time-series-forecasting-python-library/,"['Giotto-Time is an open-source Python library to perform time-series forecasting in machine learning. It is built on top of SciKit-Learn with a few modifications and wrappings to do end-to-end time-series analysis in a single go. Giotto-Time gives importance to every task associated with the time-series analysis.\xa0With Giotto-Time library, Giotto spans its list of powerful open source tools to perform various machine learning tasks.', 'Time-series forecasting is a centuries-old method of predicting the future with past data in hand. It finds applications in a variety of domains, including e-commerce, finance, space science, weather forecasting and medical sciences. Unlike time-insensitive structured data, time-series data do need more care in every stage of problem solving. Preprocessing time-series data is one of the difficult tasks that needs field expertise to perform.\xa0', 'Giotto-Time is introduced to make the time-series modeling tasks simple. This library presents data preprocessing, data cleaning, data extraction, data analysis, forecast modeling and causality testing with very few lines of code. Data analysis in Giotto-Time is associated with data visualization, for which the library introduces special plots. The data visualization module is built on top of the MatPlotLib library.\xa0', 'We explore the Giotto-Time library in the sequel with some examples and hands-on codes. Giotto-Time is available as a PyPi package. We can simply pip install it.', 'Import the necessary libraries and modules.\xa0', 'Define a function to generate some synthetic time-series data using Pandas’ testing module.', 'Generate synthetic time-series data.', 'The time-series data should be in PeriodIndex format to proceed further. The Giotto-Time library offers a time-series preprocessing module using which we can transform the data from DatetimeIndex to PeriodIndex.', 'Let’s visualize the time-series data.', 'Extract features and generate new features using the FeatureCreation API of Giotto-Time. Here, moving average of time period is determined and appended as a feature. In addition, a temporal shift is performed to generate two new features.', 'Fit the time-series data into the feature generation pipeline.', 'Generate the ground truth (output variable) using horizon_shift method.\xa0', 'Next, split the data into train and test sets using the FeatureSplitter method. Sample some data from each split part.', 'Develop a simple linear regression model from the SciKit-Learn library. Build a Generalized Auto-Regressive (GAR) model on top of the linear regression model to perform a simple time-series forecasting, and train the model with the training dataset.', 'Once the model is trained, infer the future by predicting it.', 'Find the Colab Notebook here with above code implementation.', 'Import the necessary libraries and modules.\xa0', 'Load the Kansas Wheat Index data from the Giotta-Time’s official google cloud storage.', 'Transform the data into PeriodIndex format and fill the missing values.', 'Calculate logarithmic value of sales returns and generate a returns data.', 'Plot the Wheat price and returns to visualize the data.', 'Seasonal plots are powerful tools in Giotto-Time library that give an overall picture of how the time-series data vary over seasons such as yearly, monthly, weekly, etc. The following codes generate seasonal plots for price index data.', 'Plot monthly returns with seasonal plot in polar form.', 'Seasonal plots can also be realized through Whisker’s box plot. This plot gives the basic statistical summary such as mean, mode, quartiles, minimum and maximum entries.', 'Lag plots have a prominent place in time-series analysis. It compares the data with its own temporal lags. Giotto-Time’s lag plots are simple to execute. Let’s visualize the price index data in a lag plot with three different lags, one day, one month and one year.', 'Let’s visualize the lag plot for the returns data.\xa0', 'Autocorrelation of price index seems good even up to a lag of one month. But, in the case of returns, the plot is random irrespective of the lag.', 'Find the Colab Notebook here with the above code implementation.', 'We discussed the open-source time-series forecasting Python library, Giotto-Time. We went through hands-on practice with Python codes on two tasks.']","'PeriodIndex'
 'DatetimeIndex', 'PeriodIndex', 'FeatureCreation', 'horizon_shift', 'FeatureSplitter', 'PeriodIndex'"
73,for_time_series_analysis/,https://analyticsindiamag.com/top-10-python-tools-for-time-series-analysis/,"['Time series is a sequence of numerical data points in successive order and time series analysis is the technique of analysing the available data to predict the future outcome of an application. At present, time series analysis has been utilised in a number of applications, including stock market analysis, economic forecasting, pattern recognition, and sales forecasting.', 'Here is a list of top ten Python tools, in no particular order, for Time Series Analysis.', 'About: Arrow is a Python library that offers a human-friendly approach to creating, manipulating, formatting and converting dates, times and timestamps. The library implements and updates the datetime type, plugging gaps in functionality and providing an intelligent module API that supports many common creation scenarios. Features include:', 'Know more here.', 'About: Cesium is an open source library that allows users to extract features from raw time series data, build machine learning models from these features, as well as generate predictions for new data. The cesium library also powers computations within the cesium web interface, which allows similar time series analyses to be performed entirely within the browser.', 'Know more here.', 'About: Featuretools is an open source Python library for automated feature engineering. The framework excels at transforming temporal and relational datasets into feature matrices for machine learning. Featuretools references generated features through the feature name. In order to make features easier to understand, Featuretools offers two additional tools, featuretools.graph_feature() and featuretools.describe_feature(), to help explain what a feature is and the steps Featuretools took to generate it.', 'Know more here.', 'About: TSFRESH or Time Series Feature extraction based on scalable hypothesis tests is a Python package with various feature extraction methods and a robust feature selection algorithm. The package automatically calculates a large number of time series characteristics and contains methods to evaluate the explaining power and importance of such characteristics for regression or classification tasks. Advantages include:', 'Know more here.', 'About: Pastas is an open-source Python framework designed for processing, simulation and analysis of hydrogeological time series models. Introduced by Raoul A. Collenteur, Mark Bakker, Ruben Calje, Stijn A. Klop and Frans Schaars, this framework has built-in tools for statistically analysing, visualising and optimising time series models. The two major objectives of this library are:', 'Know more here.', 'About: PyFlux is an open source library for time series analysis and prediction. In this library, users can choose from a flexible range of modelling and inference options, and use the output for forecasting and retrospection. The library allows for a probabilistic approach to time series modelling. The latest release version of PyFlux is available on PyPi. Python 2.7 and Python 3.5 are supported, but development occurs primarily on 3.5.\xa0', 'Know more here.', 'About: TimeSynth is an open source library for generating synthetic time series for model testing. The library can generate regular and irregular time series. The architecture of this library allows the user to match different signals with different architectures allowing a vast array of signals to be generated. At present, the library supports only Python 3.6+ versions.\xa0', 'Know more here.', 'About: Sktime is a unified python framework that provides API for machine learning with time series data. The framework also provides scikit-learn compatible tools to build, tune and validate time series models for multiple learning problems, including time series classification, time series regression and forecasting.', 'Know more here.', 'About: Darts is a python library for easy manipulation and forecasting of time series. It contains a variety of models, from classics such as ARIMA to neural networks.\xa0 Darts supports both univariate and multivariate time series and models, and the neural networks can be trained multiple time series.\xa0', 'Know more here.', 'About: Orbit is a Python framework created by Uber for Bayesian time series forecasting and inference. The framework is built on probabilistic programming packages like PyStan and Uber’s own Pyro. At present, Orbit supports the implementations of a few forecasting models, such as Damped Local Trend (DLT), Exponential Smoothing (ETS) and Local Global Trend (LGT).\xa0\xa0']",
74,synthetic_time_series_generation/,https://analyticsindiamag.com/guide-to-timesynth-a-python-library-for-synthetic-time-series-generation/,"['TimeSynth is a powerful open-source Python library for synthetic time series generation, so is its name (Time series Synthesis). It was introduced by J. R. Maat, A. Malali and P. Protopapas as “TimeSynth: A Multipurpose Library for Synthetic Time Series Generation in Python” (available here) in 2017.', 'Before going into the details of the library, since it is used for generating time series, have a quick overview of what the term ‘time series’ means.', 'Time series is a sequence of data points recorded at successive uniform intervals of time. It is thus a sequence of discrete-time data. It can be taken on any variable that changes over time. As the data is collected at adjacent time periods, the observations are correlated. This is in contrast with cross-sectional data wherein an entity is observed at a single point in time. Some examples of time series data include population records, monthly sunspot observations, heights of ocean tides, stock prices and so on. Visit this page to learn more about time series.', 'TimeSynth can generate regular and irregular time series. It enables users to match different signals with various architectures allowing a wide variety of signals to be generated.', 'NOTE: TimeSynth supports Python 3.6+ versions. It is supported by PyPI.\xa0', 'The types of signals that can be generated using TimeSynth are as follows:', 'Following two types of noise can be generated using TimeSynth:', 'The classes associated with the TimeSynth library are as follows:', 'General classes', 'Classes for noise generation', 'Classes for signal generation', 'Here’s a demo of using TimeSynth library in Google Colab with Python version 3.6.9.', '\xa0Clone the GitHub repository', '\xa0!pip install git+https://github.com/TimeSynth/TimeSynth.git', 'Import the TimeSynth library', 'import timesynth as ts', 'Import Matplotlib library for visualizing the outputs', 'In the above lines of code,\xa0', 'The above steps are common. Steps to generate and plot time series using specific types of signals have been explained below:', 'time_sampler = ts.TimeSampler(stop_time=20)', 'Setting stop_time=20 stops the sampling of time series after 20s. (Default value of the parameter is 10).', 'sample_irregular_time() samples irregularly spaced time using num_points (number of points in time series) or resolution (resolution of time series) parameter. Only one of these parameters need to be specified. Here, the time series will have 500 data points. keep_percentage=50 means 50% of the data points (i.e. 250 out of 500) will be retained in the irregular series.', 'sinusoid = ts.signals.Sinusoidal(frequency=0.25)', '‘frequency’ parameter represents frequency (in Hertz) of the harmonic signal formed. Its default value is 1.0', 'white_noise = ts.noise.GaussianNoise(std=0.3)', '‘std’ is a float-value parameter representing the standard deviation of the noise. You can optionally specify its mean using the ‘mean’ parameter.', '‘signal_generator’ and ‘noise_generator’ are signal object and noise object for the time series respectively. You must specify at least one signal generator while the noise generator is optional.', 'sample() method samples time from the specified time series (here ‘irregular_time_samples’)', 'red_noise = ts.noise.RedNoise(std=0.5, tau=0.8)', '‘std’ is the standard devastation for the noise and ‘tau’ signifies pulse interval in\xa0\xa0seconds.', 'timeseries_corr = ts.TimeSeries(sinusoid, noise_generator=red_noise)', 'Again, as explained in step (8), ‘signal_generator’ and ‘noise_generator’ are the signal and noise objects for time series creation respectively.', 'samples, signals, errors = timeseries_corr.sample(irregular_time_samples)', 'sample() method samples the ‘irregular_time_samples’ time series. It returns the samples as well as signals and errors produced from them, which are stored in the respective arrays on the LHS of the above line of code.', 'The first parameter represents the resolution of the time series while keep_percentage=50 retains 50% of the total generated data points in the time series.\xa0', '‘frequency’ denotes the frequency of the signal (default 1.0Hz), ‘freqSD’ denotes frequency standard deviation (default 0.1) and ‘ampSD’ is amplitude standard deviation (default 0.1).', 'timeseries_pp = ts.TimeSeries(pseudo_periodic)', '\xa0samples_pp, signals_pp and errors_pp are the arrays which will store the samples, signals generated from the samples and resulting errors respectively.', ""\xa0gp = ts.signals.GaussianProcess(kernel='Matern', nu=3./2)"", 'GaussianProcess() method samples the time series with the specified covariance\xa0function (kernel). Here, ‘Matern’ kernel has been used which requires the ‘nu’ parameter to\xa0be specified, representing the mean of the variables used in the Gaussian process. Other possible kernel types and required parameter to be specified for each can be found\xa0here.', 'gp_series = ts.TimeSeries(signal_generator=gp)', 'samples = gp_series.sample(irregular_time_samples)[0]', 'sample() returns a tuple of three arrays (samples, signals and errors – in this order). [0] in the above line of code means the first array in the tuple i.e. that of the samples.', 'car = ts.signals.CAR(ar_param=0.9, sigma=0.01)', '‘ar_param’ is a parameter of the AR(1) process. ’sigma’ denotes the standard deviation of the signal. Both the parameters have default value 1.0', 'car_series = ts.TimeSeries(signal_generator=car)', 'samples = car_series.sample(irregular_time_samples)', 'regular_time_samples = time_sampler.sample_regular_time(num_points=500)', '500 of the total data points will be retained in the time series.', 'ar_p = ts.signals.AutoRegressive(ar_param=[1.5, -0.75])', '‘ar_param’ is the list containing phi_1 and phi_2 (parameters of the AR(2) process (understand them here))', 'ar_p_series = ts.TimeSeries(signal_generator=ar_p)', 'samples = ar_p_series.sample(regular_time_samples)', '1500 data points will be there in the time series out of which 75% i.e. 1125 points will be retained.', 'mg = ts.signals.MackeyGlass()\xa0', '’tau’ can be specified as a delay parameter; omitting it will assume its default value of 17', 'noise = ts.noise.GaussianNoise(std=0.1)', '‘std’ is the standard deviation for the noise.', 'mg_series = ts.TimeSeries(signal_generator=mg, noise_generator=noise)', 'mg_samples, mg_signals, mg_errors = mg_series.sample(irregular_time_samples)', '‘mg_samples’ , ‘mg_signals’ and ‘mg_errors’ are errors which store the samples, generated signals and resulting errors respectively.', 'times = time_sampler.sample_regular_time(resolution=1.)', 'narma_signal = ts.signals.NARMA(order=10)', '‘order’ specifies the order of nonlinear interactions in the NARMA formula given here.', 'series = ts.TimeSeries(narma_signal)', 'samples, signals, errors = series.sample(times)', 'The samples, signals and errors will be stored in the respective LHS arrays on executing the above line of code.']","'\xa0!pip install git+https://github.com/TimeSynth/TimeSynth.git'
 'import timesynth as ts', 'time_sampler = ts.TimeSampler(stop_time=20)', 'sinusoid = ts.signals.Sinusoidal(frequency=0.25)', 'white_noise = ts.noise.GaussianNoise(std=0.3)', 'red_noise = ts.noise.RedNoise(std=0.5, tau=0.8)', 'timeseries_corr = ts.TimeSeries(sinusoid, noise_generator=red_noise)', 'samples, signals, errors = timeseries_corr.sample(irregular_time_samples)', 'timeseries_pp = ts.TimeSeries(pseudo_periodic)', ""\xa0gp = ts.signals.GaussianProcess(kernel='Matern', nu=3./2)"", 'gp_series = ts.TimeSeries(signal_generator=gp)', 'samples = gp_series.sample(irregular_time_samples)[0]', 'car = ts.signals.CAR(ar_param=0.9, sigma=0.01)', 'car_series = ts.TimeSeries(signal_generator=car)', 'samples = car_series.sample(irregular_time_samples)', 'regular_time_samples = time_sampler.sample_regular_time(num_points=500)', 'ar_p = ts.signals.AutoRegressive(ar_param=[1.5, -0.75])', 'ar_p_series = ts.TimeSeries(signal_generator=ar_p)', 'samples = ar_p_series.sample(regular_time_samples)', 'mg = ts.signals.MackeyGlass()\xa0', 'noise = ts.noise.GaussianNoise(std=0.1)', 'mg_series = ts.TimeSeries(signal_generator=mg, noise_generator=noise)', 'mg_samples, mg_signals, mg_errors = mg_series.sample(irregular_time_samples)', 'times = time_sampler.sample_regular_time(resolution=1.)', 'narma_signal = ts.signals.NARMA(order=10)', 'series = ts.TimeSeries(narma_signal)', 'samples, signals, errors = series.sample(times)'"
75,dates_time_and_timestamps/,https://analyticsindiamag.com/complete-guide-to-arrow-a-python-library-for-user-friendly-handling-of-dates-time-and-timestamps/,"['Arrow is a flexible Python library designed to create, format, manipulate, and convert dates, time, and timestamps in a sensible and human-friendly manner. It provides an intelligent module API that allows dealing with dates and times with a few code lines and imports. The library is named after the “time’s arrow” concept suggesting the one-directional progress of time. It is inspired by Requests (an HTTP library) and Moment.js (a JavaScript date library).', 'Standard Python library and low-level conventional modules are available for the various date and time functionalities but have the following limitations from a user’s point of view:', 'Install Arrow using pip command as follows:', 'pip install -U arrow', '(where -U option is used for installing the updated version)', 'Here’s a demonstration of performing various operations using Arrow. The experiments have been done in Google colab with Python version3.7.10. Explanation of each operation along with its output is as follows:', 'First, import the Arrow library', 'import arrow as arw', 'arw.now()', 'With no timezone specified, an Arrow object representing current UTC time will be created. Same output results on executing', 'arw.utcnow()', 'To get current time in say US/Pacific timezone:', 'arw.now(‘US/Pacific’)', 'arw.get(1567900664)', 'Verify the converted output here.', ""arw.get('12-03-2020 22:30:25', 'MM-DD-YYYY HH:mm:ss')"", 'Specified date and time will be represented as MM-DD-YYYY HH:mm:ss\xa0', 'format', ""arw.get('I was born on March 12 1990','MMMM DD YYYY')"", '‘MMMM DD YYYY’ formatted date will be searched in the string', 'arw.Arrow(2020,12,26)', 'x = arw.now()', 'Suppose, x is <Arrow [2021-03-16T11:49:17.908895+00:00]>', 'x.datetime', 'The components can then be separated as:', 'x.month\xa0\xa0\xa0\xa0', 'x.year', 'x.day', 'x.hour', 'Datetime functions can be called to get properties of the Arrow object', 'x.time()', 'a = arw.now()', 'Suppose, ‘a’ is <Arrow [2021-03-16T12:00:13.500164+00:00]>', 'a.replace(year=2019, second=45, month=11)', 'will give the output: <Arrow [2019-11-16T12:00:45.500164+00:00]>', 'present = arw.now()', 'Suppose, ‘present’ object is <Arrow [2021-03-16T12:08:34.530495+00:00]>', 'Go forward in time by 2 years', 'present.shift(years=+2)', 'Go backward by 4 hours', 'present.shift(hours=-4)', ""arw.now().format('HH:MM:SS MM:DD:YYYY')"", '(i.e. 12 hrs 3 mins 00 sec, 16 March 2021)', 'utc = arw.utcnow()', 'Suppose, utc is <Arrow [2021-03-16T12:20:43.301159+00:00]>', 'It can be converted to US/Pacific time zone as follows:', ""utc.to('US/Pacific')"", 'Humanize relative to current time:', 'Humanize relative to another datetime or Arrow object', 'To get only the distance:', 'future.humanize(present, only_distance=True)\xa0', 'Indicate time difference in terms of one or more specific time granularities like hours,\xa0\xa0', '\xa0minutes etc.', 'future.humanize(present, granularity=""minute"")', 'Multiple granularities can be specified as:', 'future.humanize(present, granularity=[""minute"",""second""])', 'arw.now().span(‘minute’)\xa0\xa0', ""arw.now().span('day')\xa0 #get the span of time for a whole day"", ""arw.now().floor('minute')"", ""arw.now().ceil('minute')"", 'We want time from 2:30 to 4:50 at the interval of an hour so output will contain 2:30, 3:30 and 4:30 times.', 'Suppose, we need to find difference in terms of days between today and the Christmas day. We can define a custom class as:', 'Suppose, x is <Custom [2021-03-16T13:21:09.741500+00:00]>', 'Call the till_christmas() method to compute the difference\xa0', 'x.till_christmas()', 'i.e. Christmas of the year 2021 is 283 days away from 16 March 2021.']","'pip install -U arrow'
 'import arrow as arw', 'arw.now()', 'arw.utcnow()', '<Arrow [2021-03-16T10:39:26.746385+00:00]>', 'arw.now(‘US/Pacific’)', '<Arrow [2021-03-16T03:48:10.893440-07:00]>', 'arw.get(1567900664)', '<Arrow [2019-09-07T23:57:44+00:00]>', '<Arrow [1970-07-27T04:24:24.546388+00:00]>', ""arw.get('12-03-2020 22:30:25', 'MM-DD-YYYY HH:mm:ss')"", '<Arrow [2020-12-03T22:30:25+00:00]>', ""arw.get('I was born on March 12 1990','MMMM DD YYYY')"", '<Arrow [1990-03-12T00:00:00+00:00]>', 'arw.Arrow(2020,12,26)', '<Arrow [2020-12-26T00:00:00+00:00]>', 'x = arw.now()', '<Arrow [2021-03-16T11:49:17.908895+00:00]>', 'x.datetime', 'datetime.datetime(2021, 3, 16, 11, 49, 17, 908895, tzinfo=tzlocal())', 'x.month\xa0', '3', 'x.year', '2021', 'x.day', '16', 'x.hour', '11', 'x.time()', 'datetime.time(11, 49, 17, 908895)', 'a = arw.now()', '<Arrow [2021-03-16T12:00:13.500164+00:00]>', 'a.replace(year=2019, second=45, month=11)', '<Arrow [2019-11-16T12:00:45.500164+00:00]>', 'present = arw.now()', '<Arrow [2021-03-16T12:08:34.530495+00:00]>', 'present.shift(years=+2)', '<Arrow [2023-03-16T12:08:34.530495+00:00]>', 'present.shift(hours=-4)', '<Arrow [2021-03-16T08:08:34.530495+00:00]>', ""arw.now().format('HH:MM:SS MM:DD:YYYY')"", '12:03:00 03:16:2021', 'utc = arw.utcnow()', '<Arrow [2021-03-16T12:20:43.301159+00:00]>', ""utc.to('US/Pacific')"", '<Arrow [2021-03-16T05:20:43.301159-07:00]>', '‘in 2 minutes’', '‘3 days ago’', '‘in 3 days’', 'future.humanize(present, only_distance=True)\xa0', '‘3 days’', 'future.humanize(present, granularity=""minute"")', '‘4320 minutes ago’', 'future.humanize(present, granularity=[""minute"",""second""])', '‘in 4320 minutes and 0 seconds’', 'arw.now().span(‘minute’)\xa0', ""arw.now().span('day')\xa0 #get the span of time for a whole day"", ""arw.now().floor('minute')"", '<Arrow [2021-03-16T13:04:00+00:00]>', ""arw.now().ceil('minute')"", '<Arrow [2021-03-16T13:04:59.999999+00:00]>', '<Custom [2021-03-16T13:21:09.741500+00:00]>', 'x.till_christmas()', '283'"
76,https://analyticsindiamag.com/sktime_library/,https://analyticsindiamag.com/sktime-library/,"['Time series data is widely used to analyse different trends and seasonalities of products over time by various industries. Sktime is a unified python framework/library providing API for machine learning with time series data and sklearn compatible tools to analyse, visualize, tune and validate multiple time series learning models such as time series forecasting, time series regression and classification. Sktime was represented in a research paper named ‘sktime: A Unified Interface for Machine Learning with Time Series’ to NeurIPS by a group of researchers at Alan Turing Institute – Markus Loning, Franz J Kiraly, from University of East Anglia – Anthony Bagnall, Jason Lines and from University College London – Sajaysurya Ganesh, Viktor Kazakov.', 'Sktime explores a blend of both features of popular time series algorithms, and the sci-kit learn library. It uses sklearn algorithms in the reduction of vast tabular data. Other features include time series regression, classification(multivariate and univariate), time series clustering, time-series annotations, forecasting, estimation, transformation, datasets, feature tools and utility functions (preprocessing and plotting).\xa0', 'Under time-series transformations comes Panel transformers and Series transformers. For Panel transformers there is Shapelet, Segment, Reduce, Rocket, PCA, Matrix profile, Compose, Summarize, tsfresh. For Series transformers there is DTrend, Adapt, box-cox, AutoCorrelation, Cosine.\xa0 The popular time series libraries available in sktime are ARIMA, AutoARIMA, fbprophet. The input data for sktime is expected to be in pandas dataframe. For more information, check the documentation.', 'The main aim of the library is to provide:', 'pip install sktime', '0.08661468139978168', '0.8679245283018868', 'A single time series variable and a corresponding label for multiple instances. The aim is to find a suitable classifier model that can be used to learn the relationship between time-series data and label and predict likewise the new series’s label.', 'In this notebook, we use the arrowhead problem.', 'The arrowhead dataset is a time-series dataset containing outlines of the images of arrowheads. In anthropology, the classification of projectile points is an important topic. The classes are categorized based on shape distinctions eg. – the presence and location of a notch in the arrow.', 'The shapes of the projectile points are to be converted into sequences using the angle-based method. For more details check this blog post about converting images into time-series data for data mining.', '# binary target variables', ""['0' '1' '2'] [60 54 44]"", ""[Text(0.5, 1.0, 'Example time series'), Text(0.5, 0, 'Time')]"", 'Time series forest is a modification of the random forest algorithm to the time series setting:', 'We can directly fit and evaluate the single time series tree (which is simply a pipeline).', '0.8113207547169812', '# For time series forest classifier, we can simply use the single tree as the base estimator in the forest ensemble.', '# Fitting and obtaining the out-of-bag score:', '0.8867924528301887', '# algorithms for plotting feature importance graph over time to obtain feature importances for the different features and intervals.', 'For more examples visit Binder to directly try out the interactive Jupyter Notebook without any other dependencies to be installed, from here.\xa0', 'Sktime-dl is an extension library to sktime in the form of applying deep learning algorithms to time-series data. This repository aims to include Keras networks to be used with sktime and it’s making a machine learning pipeline and strategy tools along with it also having an extension to sci-kit learn, for use in applications and research. The interface provides an implementation of neural networks for time series analysis.', 'Neural Networks for time-series Classification', 'The current toolkit provides an interface of dl-4-tsc and implements the following network architectures: Multilayer perceptron (MLP), Fully convolutional neural network (FCNN), Time convolutional neural network (CNN), Time Le-Net (TLeNet), Encoder (Encoder), Residual network (ResNet), Multi-scale convolutional neural network (MCNN), Multi-channel deep convolutional neural network (MCDCNN), Time warping invariant echo state network (TWIESN). There is one more interface with InceptionTime.\xa0', 'Regression', 'Most classifier architectures act to serve as regressors. These are – Time convolutional neural network (CNN), Fully convolutional neural network (FCNN), Multilayer perceptron (MLP), Encoder (Encoder), Time Le-Net (TLeNet), Residual network (ResNet), InceptionTime (Inception).', 'Forecasting', 'The regression networks can be adapted to work as time series forecasting through sktime’s reduction strategies. In future, RNNs/LSTMs networks can be seen as functional within sktime.', 'Hyper-parameter tuning is done through sci-kit learn’s RandomizedSearch and GridSerach tools. Ensembling methods include different random initialisations for stability. These act as wrapper classes to the neural networks which can be further used in high-level data pipelines within sktime models.', 'NOTE: sktime-dl is currently not maintained and replies to issues and PRs may be slow. We’re looking for a new maintainer to help us maintain sktime-dl.', 'EndNotes']","'pip install sktime'
 '0.08661468139978168', '0.8679245283018868', ""['0' '1' '2'] [60 54 44]"", ""[Text(0.5, 1.0, 'Example time series'), Text(0.5, 0, 'Time')]"", '0.8113207547169812', '0.8867924528301887'"
77,series_analysis_and_prediction/,https://analyticsindiamag.com/pyflux-guide-python-library-for-time-series-analysis-and-prediction/,"['Time series data is a type of data that changes over a time period. The sales data of a company does not remain the same for every year, sometimes it’s higher than the previous year, and sometimes it’s lower. Similarly, we see that stock prices are always changing.', 'Although it is not easy to predict the time series data due to various factors on which it depends still Python has different machine learning models that can be used to analyze and predict the time-series data.\xa0', 'PyFlux is a library for time series analysis and prediction. We can choose from a flexible range of modeling and inference options, and use the output for forecasting. PyFlux has most of the time series prediction models such as ARIMA, Garch, etc. predefined we just need to call the model we need to analyze.', 'In this article, we will explore PyFlux and the features that are present in PyFlux for time series analysis and prediction.', 'Implementation', 'We will start by installing PyFlux by pip install PyFlux.', 'Foe exploring PyFlux we will be analyzing the stock data, we will download the stock data from ‘Yahoo’ using Pandas DataReader and the ticker for the respective stock. Let us import the libraries required.', 'from pandas_datareader.data import DataReader', 'import matplotlib.pyplot as plt', 'import PyFlux as pf', 'We will be using the Microsoft stock data for this article, we can download it using Pandas DataReader and Yahoo. The stock symbol for Microsoft is MSFT.', ""msft\xa0 = DataReader('MSFT',\xa0 'yahoo', datetime(2000,6,1), datetime(2020,6,1))"", 'msft.head()', 'We need to calculate the return of the share and store it in a data frame named returns. This data frame will only contain the return and the date column.\xa0', '# Finding the returns', ""returns = pd.DataFrame(np.diff(np.log(msft['Adj Close'].values)))"", '#Setting date column as index', 'returns.index = msft.index.values[1:msft.index.values.shape[0]]', 'returns.columns = [""Returns""]', 'returns.head()', 'We will analyze the returns using the Matplotlib visualization library.', 'plt.figure(figsize=(15, 5))', 'plt.ylabel(""Returns"")', 'plt.plot(returns)', 'plt.show()', 'Similarly, we will use PyFlux for visualizing the ACR(Auto Correlation) Plot.', 'pf.acf_plot(returns.values.T[0])', 'Now we will create and analyze different models and predict returns accordingly. PyFlux supports different models but we will mainly focus on GARCH and ARIMA.', 'Generalized Autoregressive Conditional Heteroskedasticity (GARCH) is a model that is used to analyze different types of financial data. It is used to estimate the volatility of the stock returns etc.\xa0', 'We will start by creating a GARCH model which is predefined in PyFlux.', 'gar_model = pf.GARCH(p=1, q=1, data=returns)', 'The above statement defines our model with ‘p’ = the number of autoregressive lags and ‘q’ =\xa0 the number of ARCH terms.', 'Now we will fit the model and use the summary function to display the summary of the Model.', 'gar = gar_model.fit()', 'gar.summary()', 'The next step is visualizing the fit over a\xa0 chart.', 'gar_model.plot_fit(figsize=(15,5))', 'Here we can clearly visualize the estimated data with the actual data and how it picks up the volatility in the data.', 'The next step is visualizing the Prediction by the model. Here we will pass the ‘h’ parameter which defines the number of steps to be forecasted ahead.', 'gar_model.plot_predict(h=20, figsize=(15,5))', 'Here we can analyze the prediction which is given by the GARCH Model.', 'ARIMA stands for AutoRegressive Integrated Moving Average. It is a class of model that works on predicting the time series data based on the previous data given. It is pre-defined in PyFlux we just need to call it.', 'Let us create the ARIMA model by defining the Autoregressive lags and Moving Average lags. The family is the distribution of the time series which we will be using as pf.normal. We are trying to predict ‘Returns’ so our target value is ‘Returns’.', ""arm_model = pf.ARIMA(data=returns, ar=4, ma=4, target='Returns',\xa0                                                                      family = pf.Normal())"", 'Similar to the GARCH model we will fit this model with our data and analyze the summary using summary function. The latent_variable attribute we will use here can be ‘M-H’ or ‘MLE’, we will be using’ MLE’', 'arm = model.fit(""MLE"")', 'arm.summary()', 'Now similar to the steps followed for the GARCH model we will visualize the fit plot and the plot of the predicted values.', 'arm_model.plot_fit(figsize=(15,8))', 'We will predict the future values with ‘past values = 200’ and 20 steps ahead for forecasting i.e value of h=20.', 'arm_model.plot_predict(h=20,past_values=50,figsize=(15,5))', 'Here we can clearly analyze the forecasting of the returns on the Microsoft Stock using the ARIMA Model defined under PyFlux.']","'from pandas_datareader.data import DataReader'
 'import matplotlib.pyplot as plt', 'import PyFlux as pf', ""msft\xa0 = DataReader('MSFT',\xa0 'yahoo', datetime(2000,6,1), datetime(2020,6,1))"", 'msft.head()', '# Finding the returns', ""returns = pd.DataFrame(np.diff(np.log(msft['Adj Close'].values)))"", '#Setting date column as index', 'returns.index = msft.index.values[1:msft.index.values.shape[0]]', 'returns.columns = [""Returns""]', 'returns.head()', 'plt.figure(figsize=(15, 5))', 'plt.ylabel(""Returns"")', 'plt.plot(returns)', 'plt.show()', 'pf.acf_plot(returns.values.T[0])', 'gar_model = pf.GARCH(p=1, q=1, data=returns)', 'gar = gar_model.fit()', 'gar.summary()', 'gar_model.plot_fit(figsize=(15,5))', 'gar_model.plot_predict(h=20, figsize=(15,5))', ""arm_model = pf.ARIMA(data=returns, ar=4, ma=4, target='Returns',\xa0                                                                      family = pf.Normal())"", 'arm = model.fit(""MLE"")', 'arm.summary()', 'arm_model.plot_fit(figsize=(15,8))', 'arm_model.plot_predict(h=20,past_values=50,figsize=(15,5))'"
78,for_bayesian_forecasting_inference/,https://analyticsindiamag.com/hands-on-guide-to-orbit-ubers-python-framework-for-bayesian-forecasting-inference/,"['Although several machine learning and deep learning models have been adopted for time series forecasting tasks, parametric statistical approaches like ARIMA still reign supreme while dealing with low granularity data. Orbit is a Python framework created by Uber for Bayesian time series forecasting and inference; it is built upon probabilistic programming packages like PyStan and Uber’s own Pyro. Orbit currently supports the implementations of the following forecasting models:', 'It also supports the following sampling methods for model estimation:', 'Orbit refined two of the models, namely DLT and LGT. These tweaked models were compared with popular time series models such as SARIMA and Facebook Prophet. Symmetric mean absolute percentage error (SMAPE) was used as the forecast metric for comparing these models.\xa0', 'Here Xt is the value measured at time t, and h is the forecast horizon.', 'The benchmark study was conducted on five datasets:', 'Orbit’s refined models consistently deliver better accuracy than the other time series models in terms of SMAPE.', '!pip install orbit-ml', 'For other installation, methods see this.', 'We are going to use the iclaims_example dataset provided by Orbit. It contains the weekly initial claims for US unemployment benefits against a few related google trend queries (unemploy, filling, and job)from Jan 2010 – June 2018.', 'We will try out the three different wrappers for the Damped Local Trend model provided in Orbit- \xa0DLTMAP, DLTAggregated, and\xa0 DLTFull.', 'In full prediction, the prediction occurs as a function of each parameter posterior sample, and the prediction results are aggregated after prediction.', ""In aggregated prediction, the parameter posterior samples are reduced using aggregate_method ({ 'mean', 'median' }) before performing a single prediction."", 'More information about the different models is available here.', 'Code for the above implementation has been taken from the official example notebook available here.', 'This article introduced Orbit, Uber’s open-source Python library for time series forecasting. With the help of the underlying probabilistic programming packages, Orbit introduces multiple model refinements like additional global trends, transformation for multiplicative form, noise distribution, and priors’ choice. These refined models outperform other common forecasting models like SARIMA and Facebook Prophet. As per the paper, the developers are adding new models and features like dual seasonality support, full Pyro integration, etc.']","'!pip install orbit-ml'
 'iclaims_example', 'DLTMAP', 'DLTAggregated', 'DLTFull', ""aggregate_method ({ 'mean', 'median' })"""
79,hydrogeological_time_series_analysis/,https://analyticsindiamag.com/guide-to-pastas-a-python-framework-for-hydrogeological-time-series-analysis/,"['Pastas is an open-source Python framework designed for processing, simulation and analysis of hydrogeological time series models. It has built-in tools for statistically analyzing, visualizing and optimizing such models. It was introduced by Raoul A. Collenteur, Mark Bakker, Ruben Calje, Stijn A. Klop and Frans Schaars in an article published by the National Groundwater Association (NGWA) in July 2019.', 'Before going into Pastas’ details, let us first understand the core term ‘hydrogeology’ – an area in which the Pastas library marks its significant contribution.', 'The word ‘hydrogeology’ is a combination of two terms: ‘hydro’ meaning water and ‘geology’ which means studying the Earth. Hydrogeology is thus a branch of geology dealing with movement and distribution of groundwater in soil and rocks of Earth’s crust (also known as ‘aquifers’). You can read in-depth about this domain here.\xa0', 'The two major objectives of this state-of-the-art library are:', 'Pastas employ Transfer Function Noise (TFN) time series models using predefined response functions that define how groundwater level responds to evaporation, rainfall etc..', 'Pastas can be installed using pip or conda command (using Anaconda Distribution of Python is recommended for the installation step)', 'pip install pastas \xa0 -or-\xa0 conda install pastas', 'If the library has already been installed, it can be updated to use its latest version as:', 'pip install pastas --upgrade', 'Prerequisite: Python 3.6 or higher version', 'Dependencies:', 'These dependencies get automatically installed when you instal pastas using pip install manager.', 'Here, we have shown two use cases of Pastas library. The demonstrations have been coded in Google colab with Python version 3.7.10.', 'Step-wise explanation of both the examples is as follows:', '!pip install pastas', 'ps.show_versions()', 'Sample output:', 'The term ‘head’ here refers to the hydraulic head (also known as ‘static level’) of groundwater flow i.e. the elevation to which water will naturally rise. The file containing the head observations can be downloaded from here.', '‘parse_dates’ enables parsing of the ‘date’ column into datetime. ‘index_col’ sets the ‘date’ column as index and ‘squeeze’ parameter squeezes the head observations into a pandas Series object.', '\xa05. Check the initial records and datatype of head_data.', 'print(‘Data type of head_data:’,type(head_data))', 'head(head_data)', '6. Plot the head observations', '7. Load the stresses', 'The causes of head variations are believed to be rain and evaporation of water. We load the .csv files having data of measured rainfall and measured potential evaporation.', '8. Plot the rain and evaporation time series', '9. Calculate and plot recharge', 'The term ‘groundwater recharge’ (also known as ‘deep percolation’ or ‘deep drainage’) refers to the hydraulic process in which water moves downwards from surface water to groundwater.', 'We approximate recharge as the difference between rain and evaporation level.', '10. Time series model creation', 'Initiate a pastas time series model using Model class', 'model = ps.Model(head_data)', 'Create a stress model which translates input time series (here #‘recharge’) into a contribution explaining part of the output time series', 'For each stress, a StressModel object is created. The first parameter is the name of the time series of the stress. The second argument is the response function used, ‘name’ specifies just a name given to the series and ‘prec’(stands for ‘precipitation’) is the kind of series.', 'Add the stress model to the time series model', 'model.add_stressmodel(stressModel)', 'Solve model for specified time period (tmin to tmax).Different available solvers can be found here.', ""model.solve(tmin='1990', tmax='2015')"", 'Sample output:', '11. Plot the model', 'Here, a TFN model is created first with the net groundwater recharge as the single stress explaining the observed heads.Then, the model is extended to include the effect of adding a pumping well on the heads by adding another stress model. A comparison is made between the simulated heads in presence and absence of the pumping well.The addition of the pumping well is clearly found to have improved the simulation of the heads.', '(1)-(3) Perform steps (1) to (3) as explained in the above time series analysis demo first. Then, proceed as follows:', '4. Load the datasets containing information about head observations, rainfall, evaporation and \xa0 pumping extraction rate available here.', 'model = ps.Model(head, name=""groundwater"")', 'Add the stress model for the net groundwater recharge simulated using RechargeModel class', '‘rfunc’ is the response function used. ps.rch.Linear() is the linear model used for computing precipitation excess.', ' Solve the model\xa0', 'model.solve()', 'Plot the decomposition of the time series in various stresses', 'Compute residuals time series using residuals() method', 'model.residuals().plot(figsize=(20, 8))', 'Plot the residuals series']","'pip install pastas'
 'conda install pastas', 'pip install pastas --upgrade', '!pip install pastas', 'ps.show_versions()', 'print(‘Data type of head_data:’,type(head_data))', ""Data type of head_data: <class 'pandas.core.series.Series'>"", 'head(head_data)', 'model = ps.Model(head_data)', 'model.add_stressmodel(stressModel)', ""model.solve(tmin='1990', tmax='2015')"", 'model = ps.Model(head, name=""groundwater"")', 'model.solve()', 'model.residuals().plot(figsize=(20, 8))'"
80,for_time_series_forecasting/,https://analyticsindiamag.com/hands-on-guide-to-darts-a-python-tool-for-time-series-forecasting/,"['Data collected over a certain period of time is called Time-series data. These data points are usually collected at adjacent intervals and have some correlation with the target. There are certain datasets that contain columns with date, month or days that are important for making predictions like sales datasets, stock price prediction etc. But the problem here is how to use the time-series data and convert them into a format the machine can understand? Python made this process a lot simpler by introducing a package called Darts.\xa0', 'In this article, we will learn about Darts, implement this over a time-series dataset.', 'For a number of datasets, forecasting the time-series columns plays an important role in the decision making process for the model. Unit8.co developed a library to make the forecasting of time-series easy called darts. The idea behind this was to make darts as simple to use as sklearn for time-series. Darts attempts to smooth the overall process of using time series in machine learning.\xa0', 'The basic principles of darts are:', 'Regression models: these predict the output based on a set of input time-series.', 'Forecasting models: these predict a future output based on past values.', 'Darts is open-source and can be installed with the pip command. To install darts use:', 'pip install u8darts', 'Next, choose any time-series dataset of your choice. I have selected the monthly production of beer in Australia dataset. To download this click here. Let us now load the dataset and import the libraries needed.', 'from google.colab import drive', ""drive.mount('/content/gdrive/')"", 'import pandas as pd', 'from darts import TimeSeries', ""beer_data = pd.read_csv('/content/gdrive/My Drive/beer.csv')"", 'beer_data.head()', 'The dataset contains two columns- the month with the year and the beer production in that time period.\xa0', 'Let us now use the TimeSeries class and split the data into train and test. We will use a method called from_dataframe for doing this and pass column names in the method. Then, we will split the data based on the time period. The dataset has around 477 columns, so I chose the 275th time period to make the split (1978-10).', ""get_data = TimeSeries.from_dataframe(beer_data, 'Month', 'Monthly beer production')"", ""traindata, testdata = get_data.split_before(pd.Timestamp('1978-10'))"", 'Training of the model is very simple with darts. An exponential smoothing model is used here to fit the data. Similar to sklearn, fit() method is used to fit the dataset.\xa0', 'from darts.models import ExponentialSmoothing', 'beer_model = ExponentialSmoothing()', 'beer_model.fit(traindata)', 'This completes the training part. Let us now make predictions and plot the graph', 'prediction = beer_model.predict(len(test))', 'print(""predicted"" ,prediction[:5])', 'print(""actual"",test[:5])', 'import matplotlib.pyplot as plt', ""get_data.plot(label='actual')"", ""prediction.plot(label='predict', lw=3)"", 'plt.legend()', 'Here the monthly values after 1978 are forecasted due to the model exponential smoothing. It shows the time-series predictions with good accuracy.', 'Darts can also be used in neural networks, multivariate models and clustering models.\xa0']","'pip install u8darts'
 'from google.colab import drive', ""drive.mount('/content/gdrive/')"", 'import pandas as pd', 'from darts import TimeSeries', ""beer_data = pd.read_csv('/content/gdrive/My Drive/beer.csv')"", 'beer_data.head()', ""get_data = TimeSeries.from_dataframe(beer_data, 'Month', 'Monthly beer production')"", ""traindata, testdata = get_data.split_before(pd.Timestamp('1978-10'))"", 'from darts.models import ExponentialSmoothing', 'beer_model = ExponentialSmoothing()', 'beer_model.fit(traindata)', 'prediction = beer_model.predict(len(test))', 'print(""predicted"" ,prediction[:5])', 'print(""actual"",test[:5])', 'import matplotlib.pyplot as plt', ""get_data.plot(label='actual')"", ""prediction.plot(label='predict', lw=3)"", 'plt.legend()'"
81,forecasting_models_using_streamlit/,https://analyticsindiamag.com/how-to-deploy-time-series-forecasting-models-using-streamlit/,"['Streamlit.io is an open-source project which provides an interactive framework for Data Science experimentations. We have already covered real-time object detection and building a COVID-19 dashboard using the Streamlit API in our previous articles. In this article, we are going to use Streamlit for another use case called time series forecasting.', 'Before proceeding, refer to this post if you are unaware of the Streamlit framework.\xa0', 'Here, we have used the Anaconda prompt and Python version 3.8.5 for the implementation. Facebook’s Prophet open-source library has been used for forecasting (Python API for Prophet). The code allows the user to upload custom time-series data and visualise the Prophet’s forecast in Streamlit app on a web browser. Stepwise explanation of the code is as follows:', ""st.title('Time Series Forecasting Using Streamlit')"", 'streamlit.write() is used to write arguments to the app', 'The title and the text will appear in the UI as follows:', "" data = st.file_uploader('Upload here',type='csv')"", 'The file uploader widget will look something like this:', 'Some initial records of the uploaded file will appear below the widget as follows:', 'st.write(""SELECT FORECAST PERIOD"") #text displayed', 'Insert a numeric input widget using streamlit.number_input() so that the user can select the\xa0number of days for which he wants to forecast the future.', 'The section for horizon selection containing the numeric input widget will appear in UI as\xa0follows:', 'pip install fbprophet', 'IMP NOTE: If you get issues in installing fbprophet using pip command in Anaconda prompt, install it using conda as follows:', 'pip install --upgrade streamlit', 'Then, run the following command on the terminal, open the Streamlit app in your browser and run your application.', 'streamlit run FILEPATH', 'where FILEPATH is the path to the location where you have stored the my_app.py file.', 'When the app gets launched, the UI appears as follows:', 'On uploading the .csv file, suppose we select 100 as the number of days for which forecast should be made. Some of the forecasted records will appear on the UI as follows:', 'Plot showing actual values (black dots) and predicted values (blue):', 'Plots showing forecasted future trend:']","""st.title('Time Series Forecasting Using Streamlit')""
 ""data = st.file_uploader('Upload here',type='csv')"", 'st.write(""SELECT FORECAST PERIOD"") #text displayed', 'pip install fbprophet', 'pip install --upgrade streamlit', 'streamlit run FILEPATH'"
82,for_multiple_time_series/,https://analyticsindiamag.com/hands-on-guide-to-autots-effective-model-selection-for-multiple-time-series/,"['Time series data is a type of data that changes over a time period. The sales data of a company does not remain the same for every year, sometimes it’s higher than the previous year, and sometimes it’s lower. Similarly, we see that stock prices are always changing.', 'Time series prediction is predicting future data with respect to the historic data. Python has different models and libraries which can be used for prediction but we should know which library or model works best for which type of data. Because there are a large number of models but all work on different types of data.', 'AutoTS allows you to run your data through different models for time series prediction which are already present in it and give out the result for the best model that works for your data. It has around 20 built-in models which make it powerful enough to work on any type of time-series data.', 'Some of the features of AutoTS are:', 'In this article, we will explore how to use AutoTS to predict time-series data and forecast future data accordingly.', 'Like any other python library, we will install AutoTS using pip install autots.', 'We will use pandas for loading the dataset we will use and Autots for predicting the best model for forecasting of data.', 'import pandas as pd', 'from autots import AutoTS', 'I have downloaded the stock data of a stock name Biocon from Yahoo Finance in the csv format which I will use here. The dataset contains different attributes of stock data but we are mainly concerned with Date and Closing price.', ""df = pd.read_csv('biocon.csv')"", 'df', 'For calling the model with our dataset we first need to define the model and pass the parameters according to our requirements. While creating the model we will give below parameters:', '#Creating the Model', ""mod = AutoTS(forecast_length=3, frequency='infer', \xa0ensemble='simple', drop_data_older_than_periods=200\xa0)"", 'After creating our model the step is to fit the model according to our dataset. We will also print the name of the model which best works for our data. This step will take some time as it will run our data through different models and check the best model for our data. I won’t be able to show the output of the whole process so I will only print the name of the best Model.\xa0', ""mod = mod.fit(df, date_col='Date', value_col='Close', id_col=None)"", '# Name of the best model', 'print(mod)', 'Here we can see that the best model for our dataset is VECM(Vector Error Correction Model), now we use this model for prediction and forecasting.', 'We can use AutoTS forecast and predict function for this step. As we have selected forecast length as 3 so it will display the forecast of the next 3 days.\xa0', 'prediction = mod.predict()', 'forecast = prediction.forecast', 'model_results = model.results()', 'validation = model.results(""validation"")', 'Let’s print the forecast of the dataset for the future stock price. Also, we will see the validation of the model results.\xa0', '#Forecast', 'print(forecast)', '#Validation Results', 'print(validation)', 'The validation result also contains different attributes i.e errors, weighted errors, etc.\xa0 The score of the respective models is also present in the validation results which is used to select the best model.', 'This is how we can create a model using AutoTS for time series analysis, prediction and forecasting. We can select the model to use by passing a model list and naming the models we want in that list, which will run your data through those models only.']","'import pandas as pd'
 'from autots import AutoTS', ""df = pd.read_csv('biocon.csv')"", 'df', ""mod = AutoTS(forecast_length=3, frequency='infer', \xa0ensemble='simple', drop_data_older_than_periods=200\xa0)"", ""mod = mod.fit(df, date_col='Date', value_col='Close', id_col=None)"", '# Name of the best model', 'print(mod)', 'prediction = mod.predict()', 'forecast = prediction.forecast', 'model_results = model.results()', 'validation = model.results(""validation"")', '#Forecast', 'print(forecast)', '#Validation Results', 'print(validation)'"
83,to_build_recommendation_systems/,https://analyticsindiamag.com/pytorch-introduces-torchrec-an-open-source-library-to-build-recommendation-systems/,"['CEO Mark Zuckerberg has introduced TorchRec, an open source library for building state-of-the-art recommendation Systems under PyTorch, at Inside the Lab event. The new library provides common sparsity and parallelism primitives, enabling researchers to build state-of-the-art personalisation models and deploy them in production. It\xa0 includes a scalable low-level modelling foundation alongside rich batteries-included modules.', 'Meta initially targeted“two-tower” architectures with separate submodules to learn representations of candidate items and the query or context. Input signals can be a mix of floating point “dense” features or high-cardinality categorical “sparse” features that require large embedding tables to be trained. Efficient training of such architectures involves combining data parallelism that replicates the “dense” part of computation and model parallelism that partitions large embedding tables across many nodes.', 'The library includes optimised Recommendation Systems kernels that run on FBGEMM, a high-performance kernel library, modelling primitives such as jagged tensors and embedding bags, to create multinodal models using model parallelism. The PyTorch team has released TorchRec after close to two years in testing.', 'TorchRec was used to train a model with 1.25 million parameters that went into production in January and a 3 trillion parameter model that is expected to go into production.\xa0', 'Cautiously excited about @PyTorch launching TorchRec. This might be just what we need!']",
84,recommender_systems_using_lenskit/,https://analyticsindiamag.com/how-to-build-recommender-systems-using-lenskit/,"['Recommender systems are one of the major tools for attracting customers in different kinds of markets. A good recommendation increases the customer’s engagement and hence impacts the business positively. When it comes to the development of recommender systems, we find it very complex. LensKit is a library or toolkit which can facilitate us with building a good recommender system in a very easy way. In this article, we will discuss the LensKit toolkit for building recommender systems. The major points to be discussed in this article are listed below.', 'Table of contents\xa0', 'Let’s start with understanding what LensKit is.', 'What is LensKit?', 'LensKit is a library that includes a variety of tools for building and practising recommendation systems. It is the successor of the Java-based LensKit toolkit for python. Using this library of python, we can utilize to train, run, and evaluate the recommender algorithms. One of the most important things about building this library is to provide a flexible way for research in the field of recommendation systems.\xa0', 'LensKit has a variety of components and interfaces that can be utilized in designing and implementing a new algorithm. It has tools for scoring items that can be considered as a base tool for any recommendation system using which we can score the items or pick the top n recommender.', 'It also has facilities for predicting ratings. Predicting ratings can be considered as scores that depend on rating scales that we want to use. It is a representation of rating predictions to users. Using the Item Recommender interface of this tool we can provide our top recommendations. The below image can be considered as the workflow diagram of different components of this toolkit.', 'In the workflow diagram, we can see that the rating predictor and item recommender generate their respective result scores using the item scorer.', 'We can install this library in our environment using pip and the below lines of codes.', '%pip install LensKit', 'Or we can install it directly using the git command as,', 'pip install git+https://github.com/LensKit/lkpy', 'After installing it we are ready to use it. Let’s see how we can do this.', 'Building a recommender system', 'In this article, we are going to use the LensKit toolkit for nDCG evaluation. nDCG stands for normalized discounted cumulative gain that is a measure of ranking quality. Using this we can measure the effectiveness of the recommendation algorithm.\xa0 This toolkit is compatible with Pandas data frame and still provides some of the datasets for practising recommendation systems using some of its modules. One condition that we are required to follow is that we need data with expected variable names. For example, expected rating data can contain the following columns:', 'This data can also contain different columns.\xa0\xa0\xa0', 'In one of our articles, we saw the working of the surprise library. To check the compatibility of LensKit in this article, we will load the data using the surprise toolkit and other pieces of work will be performed using the LensKit toolkit.', 'Loading dataset', 'Let’s load a dataset', 'Here we can see the format of our data that is similar to the expected rating dataset format where we can see the user, item, rating, and timestamp columns. Let’s proceed to the further steps.', 'Importing the components\xa0', 'Instantiating algorithms', 'Functionalizing recommendations\xa0', 'After defining the algorithms we are ready to generate recommendations and measure them. Using this toolkit we can also evaluate the recommendation at the time of generation to save the memory. Here we will first generate the recommender and then evaluate it.', 'Using the below function we can generate recommendations in batch settings, which means this function will allow us to generate recommendations using one algorithm and some part of training and test data.\xa0\xa0\xa0', 'Fitting recommendation', 'After defining this function we can perform the generation of recommendations by looping the data and algorithm.', 'This output is similar to traditional processes of generating recommendation systems which have some warnings about runtime problems because of large matrices.', 'Evaluating recommendation\xa0', 'Now we are ready to see results. Before showing the results we can concatenate results into one data frame.\xa0\xa0', 'In the output, we can see the scores of our items with their ranks and the algorithm that is used to generate the result.\xa0', 'For better analysis, we can also concatenate all the test data into one data frame.\xa0', 'Now, this toolkit provides a module for analyzing the generated recommendations named as RecListAnalysis. Using this module we can line up our tests and recommendations properly. Let’s see how we can use it for evaluating the nDCG.\xa0', 'Here in the output, we can see that we have values for nDCG in data frame format and that can be evaluated using different methods. Let’s see which algorithm has the most nDCG values.', ""results.groupby('Algorithm').ndcg.mean()"", 'Let’s visualize our evaluation', ""results.groupby('Algorithm').ndcg.mean().plot.bar()"", 'Here we have our results. We can see that the alternative least square is having larger nDCG values.\xa0', 'Final words', 'In this article, we have discussed some of the important details about the LensKit toolkit that is designed to make and explore recommendation systems. Along with this, we have implemented one process where we used two algorithms to compare the nDCG values on the MovieLen rating dataset.\xa0']","'%pip install LensKit'
 'pip install git+https://github.com/LensKit/lkpy', ""import surprise\nimport pandas as pd\ndata = surprise.Dataset.load_builtin('ml-100k')\nddir = surprise.get_dataset_dir()\nr_cols = ['user', 'item', 'rating', 'timestamp']\nratings = pd.read_csv(f'{ddir}/ml-100k/ml-100k/u.data', sep='\\t', names=r_cols,\n                      encoding='latin-1')"", 'from LensKit import batch, topn, util, topn\nfrom LensKit import crossfold as xf\nfrom LensKit.algorithms import Recommender, als, item_knn as knn\n%matplotlib inline', 'algo_ii = knn.ItemItem(20)\nalgo_als = als.BiasedMF(50)', ""def eval(aname, algo, train, test):\n    fittable = util.clone(algo)\n    fittable = Recommender.adapt(fittable)\n    fittable.fit(train)\n    users = test.user.unique()\n    recs = batch.recommend(fittable, users, 100)\n    recs['Algorithm'] = aname\n    return recs"", ""all_recs = []\ntest_data = []\nfor train, test in xf.partition_users(ratings[['user', 'item', 'rating']], 5, xf.SampleFrac(0.2)):\n    test_data.append(test)\n    all_recs.append(eval('ItemItem', algo_ii, train, test))\n    all_recs.append(eval('ALS', algo_als, train, test))\n\n"", 'all_recs = pd.concat(all_recs, ignore_index=True)\nall_recs.head()\n', 'test_data = pd.concat(test_data, ignore_index=True)\ntest_data.head()\n', 'rla = topn.RecListAnalysis()\nrla.add_metric(topn.ndcg)\nresults = rla.compute(all_recs, test_data)\nresults.head()\n', ""results.groupby('Algorithm').ndcg.mean()"", ""results.groupby('Algorithm').ndcg.mean().plot.bar()"""
85,recommender_systems_with_rgrecsys/,https://analyticsindiamag.com/how-to-evaluate-recommender-systems-with-rgrecsys/,"['Traditionally, evaluations of recommender systems have focused on the performance of algorithms such as recommendation accuracy. However, today’s evaluation of such systems should not be limited to metrics such as accuracy but should also consider constraints such as user-fed incorrect data, evaluating sub-items or sub-products to be recommended, changes in data distribution, and so on. So, in this post, we will look at RGRecSys, a library that performs constraint evaluation of recommender systems. The major points to be covered in this article are listed below.', 'Let’s first quickly have a brief introduction to the recommender system.', 'A recommender system, sometimes known as a recommendation engine, is a type of information filtering system that attempts to forecast a user’s “rating” or “preference” for an item. Playlist generators for video and music services, product recommenders for online businesses, content recommenders for social media platforms, and open web content recommenders are all examples of recommender systems in use.\xa0', 'These systems can function with a single input, such as music, or with several inputs, such as news, books, and search queries, within and across platforms. There are other popular recommender systems for specific themes like restaurants and online dating. Recommender systems often employ collaborative filtering or content-based filtering (also known as the personality-based approach), in addition to other systems such as knowledge-based systems.\xa0', 'Collaborative filtering techniques build a model based on a user’s previous behaviour (products previously purchased or selected, as well as numerical ratings awarded to those items) and analogous decisions made by other users. This model is then used to estimate which things (or item ratings) the user would be interested in. By utilizing a collection of distinct, pre-tagged features of an item, content-based filtering approaches recommend additional things with similar attributes.', 'Traditionally, recommender systems have been created and analyzed under simple but unrealistic assumptions, such as i.i.d. assumptions (training and testing data being independent and identically distributed) and noiseless and abundant data.\xa0', 'Recent studies have relaxed these assumptions and are focusing on constructing models in more difficult, yet realistic settings in which data given into recommender systems is intentionally attacked, scarce, and biased. However, there are more aspects to consider while assessing robustness.', 'Some user and item features, for example, maybe corrupted (transformation), or the i.i.d. assumption of training and testing data may be violated (distributional shift). The performance of recommender systems that rely too heavily on unrealistic assumptions might suffer greatly. Hence in order to get rid of these all and such constraints, one should have properly evaluated the recommender system.\xa0', 'A comprehensive definition of robustness for recommendation systems that can include and formalize several perspectives on robustness, such as robustness with respect to subpopulation, transformations, distributional disparity, attack, and sparsity.', 'Robustness Gym for RecSys (RGRecSys) is a robustness evaluation toolkit for recommendation systems that allows us to quickly and uniformly conduct a comprehensive robustness evaluation for recommendation system models. RGRecSys assesses the robustness of recommender system models to data subpopulation, transformation, distributional shift, attack, and sparsity.', 'To demonstrate the utility of RGRecSys, this system makes use of RecBole built-in models. RecBole makes use of Pytorch throughout the library and proposes a unified framework that includes data, model, and evaluation modules. This library contains a diverse collection of models for general, sequential, context-aware, and knowledge-based recommendation.\xa0', 'Its general and extensible data structure makes it easy to add new models to the library and provides users with enough flexibility to set up experimental environments like hyper-parameters and splitting criteria. We can conduct unified and comprehensive robustness evaluations on recommender system models using RGRecSys’ robustness evaluation module.', 'The performance metrics averaged across all users and items are reported in most existing recommender system libraries. A single high-performance metric, on the other hand, does not guarantee that the model will perform well for a subset of users or items.\xa0', 'For example, a recommender system may perform well on average across all users but poorly on subgroups of users such as females or people of a specific race. With the increasing importance of fair recommender systems, reporting performance for a subset of users or items is critical.', 'Here the RGRecSys library allows us to evaluate model performance for any sub-group of interest, such as users of a specific feature, users’ activeness based on the number of interactions, and users’ critiques based on their rating score. That is, given a trained model, the library can slice test data to perform a fine-grained evaluation on models and assess their slicing robustness.', 'Many recommender system models are predicated on the assumption that training and testing data are distributed uniformly. However, in real-world scenarios, this i.i.d. assumption is frequently broken. RGRecSys can be used to validate recommender system models that are subject to distributional shifts.', 'To accomplish this, RGRecSys first provides users with the training data distribution based on user features and then allows them to manipulate the testing data distribution by sampling it to differ from the training data distribution. Our library users, for example, can select the female to male ratio in testing data that differs greatly from training data.', 'Because recommender systems have such a large economic impact, they are extremely vulnerable to attacks aimed at changing the rankings of specific items. When dealing with malicious attacks, it is critical to evaluate the performance of recommender system models.\xa0', 'RGRecSys allows us to test the models in the context of a Cross-Site Request Forgery (CSRF) attack, in which the attacker causes the victim user to unintentionally perform an action. For example, the user may change his ratings despite his will, resulting in corrupted interaction data in the training dataset. We can use RGRecSys to determine the severity of an attack by determining how much of the interaction will be corrupted.', 'Data fed into recommender systems usually include users’ explicit or implicit feedback, such as ratings or clicks. Typically, such data is sparse, and recommender systems have been known to perform poorly when fed sparse data. This library allows users to compare the robustness of different models under sparse data by randomly removing a fraction of user interaction data. We can select the level of sparsity as well as the users with whom you want to drop interactions based on their activity.', 'Most recommender system models require access to user and item features in order to provide users with a set of recommended items. Such information can be gathered by asking users and content providers to fill out a profile that includes some information about them or by the recommender system extracting it automatically, for example, from user reviews. However, misleading information, an error that occurs when recommender models attempt to extract it, or a malicious attack could taint this data.', 'As a result, it’s more realistic to assume that some user or item features are inaccurate. RGRecSys allows us to evaluate models under transformation on user or item features by allowing them to choose which features to transform and how severe the transformation should be. This transformation can be random, in which case the feature value can take any value, or structure, in which case the feature value is within a certain range of its true value.']",
86,tool_for_recommender_systems/,https://analyticsindiamag.com/a-guide-to-surprise-python-tool-for-recommender-systems/,"['Building a recommender system from scratch is a tedious task as it involves a lot of preprocessing steps and requires sophisticated coding skills. There are plenty of open-source toolkits available which give a state-of-the-art performance for a variety of recommendation systems. In contrast to the low code base, in this post, we will see how to build a state-of-the-art recommendation system using the Python Scikit package called Surprise. Before directly jumping to the implementation with surprise, first, we will see the context of the recommendation system and packages that can be used to build the system. The major points to be covered in this article are outlined below.', 'Recommender systems are computer programs that make recommendations to users based on a variety of parameters. These systems forecast the most likely product that users will buy and that they will be interested in. Netflix, Amazon, and other companies employ recommender systems to assist their users to find the right product or movie for them.', 'The recommender system filters a large volume of data by focusing on the most significant information based on the information provided by the user as well as other factors such as the user’s preferences and interests. In order to offer recommendations, it determines the compatibility of the user and the object, as well as the similarities between users and products.', 'Recommender systems in use include playlist generators for video and music services, product recommenders for online businesses, content recommenders for social media platforms, and open web content recommenders. Within and across platforms, these systems can work with a single input, such as music, or multiple inputs, such as news, books, and search queries.\xa0', 'It has the following benefits:', 'Collaborative filtering is a popular method for the creation of recommender systems. Collaborative filtering is predicated on the premise that people who agreed in the past will agree in the future, and that they will prefer comparable types of goods in the past. The technology creates suggestions based solely on rating profiles for various persons or things. They generate recommendations utilizing this neighbourhood by seeking peer users/items with rating histories similar to the current user or item.', 'Another method that is widely utilized when building recommender systems is content-based filtering. The description of an item and a profile of the user’s preferences are used in content-based filtering systems. When there is known data about an item (name, location, description, etc.) but not on the user, these strategies perform well. Content-based recommenders treat suggestions as a user-specific classification issue, developing a classifier for a user’s likes and dislikes based on an item’s properties.', 'The interactions of a user during a session are used to produce recommendations in these recommender systems. Youtube and Amazon both utilize session-based recommender systems. When a user’s history (such as previous clicks or transactions) is not available or relevant in the current user session, they are particularly valuable. Video, e-commerce, travel, music, and other domains are all examples of when session-based suggestions are useful. Most session-based recommender systems rely on the sequence of recent interactions inside a session without requiring any further information about the user (history, demographics).', 'Multi-criteria recommender systems (MCRS) are recommender systems that take into account many factors when making recommendations. Rather than developing recommendation techniques based on a single criterion value, such as user u’s overall preference for an item I, these systems attempt to predict a rating for unexplored items of u by leveraging preference information on multiple criteria that influence this overall preference value. Several researchers see MCRS as a multi-criteria decision-making (MCDM) problem and construct MCRS systems using MCDM approaches and techniques.', 'Let us have a look at the top python packages that are used for building a recommenders system by the community and researchers.\xa0\xa0', 'LensKit is a free and open-source framework for developing, investigating, and learning about recommender systems. It supports developing, running, and assessing recommender algorithms in a flexible manner appropriate for research and education. LensKit for Python (LKPY) is the Python-based successor of the Java-based LensKit toolkit and a component of the LensKit project. LKPY allows creating robust, adaptable, and reproducible experiments that leverage the broad and developing PyData and Scientific Python ecosystems, such as scikit-learn and TensorFlow.', 'Crab is a Python recommender engine that combines classic information filtering recommendation methods into a variety of scientific Python libraries, including Numpy, Scipy, and Matplotlib. It’s also known as Scikits recommender, and it seeks to give a comprehensive set of components from which one may build a personalised recommender system from a set of algorithms that can be utilized in a variety of situations. User-based filtering, item-based filtering, and other capabilities are available in Crab.', 'TensorRec is a Python recommendation system that lets you quickly create and customize recommendation systems using TensorFlow. User features, item features, and interactions are the three types of data that a TensorRec system consumes. It learns to produce and rank recommendations using this data. TensorRec learns by comparing the scores it generates to real-world interactions between users and items, such as likes and dislikes.', 'To have more details about similar packages you can follow this post.', 'Surprise is a Python module that allows you to create and test rate prediction systems. It was created to closely resemble the scikit-learn API, which users familiar with the Python machine learning ecosystem should be comfortable with. Surprise includes a set of estimators (or prediction algorithms) for evaluating predictions. Classic techniques, such as the main similarity-based algorithms, as well as matrix factorization algorithms like SVD and NMF, are implemented.', 'It also includes tools for model evaluation, such as cross-validation iterators and scikit-built-in learned metrics, as well as grid search and randomized search for model selection and automatic hyper-parameter search. Users can develop their own recommendation technique with fewer codes thanks to basic primitives and a light API.', 'Traditional datasets, such as the MovieLens datasets, are immediately available in the package, but user-defined datasets can be loaded as CSV files or used with pandas data frames. Surprise is primarily written in Python, with Cython being used to optimize the computationally heavy bits. Internally, Surprise uses NumPy arrays and built-in Python data structures (mostly dictionaries).', 'The surprise was created to help researchers quickly test novel recommendation ideas by allowing them to create bespoke prediction algorithms, but it can also be used as a learning resource for students and less experienced users due to its extensive documentation.', 'Here we will look at a quick example of how to download a dataset, split it into four folds for cross-validation, and compute the SVD algorithm’s Mean Average Error (MAE) and Root Mean Squared Error (RMSE).', 'If the movielens-100k dataset has not already been downloaded, the load_builtin() method will offer to download it and save it in the .surprise data folder in your home directory (you can also choose to save it somewhere else).', 'We’re using the well-known SVD algorithm here, although there are plenty of alternative options. For further information, see Using prediction algorithms. The cross-validate() function computes several accuracy metrics and executes a cross-validation procedure according to the cv argument. We’re using a traditional 5-fold cross-validation method here, although other iterators can be utilized.']","""! pip install surprise\nfrom surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\n\n# load the data\ndata = Dataset.load_builtin('ml-100k')\n\n# load algorithm\nalgo =SVD()\n\n# train and validate\ncross_validate(algo
 data, measures=['RMSE','MAE'], cv=5, verbose=True)\n"""
87,recommenders_with_python_code/,https://analyticsindiamag.com/a-complete-guide-to-tensorflow-recommenders-with-python-code/,"['Developing comprehensive recommendation systems is a tedious and complicated effort for both novices and experts. It involves several steps starting with obtaining a dataset, embedding the vectors, and, most importantly, the complete coding technique To avoid the complexity in developing the recommender systems, TensorFlow has launched an open-source package called Tensorflow Recommenders. Here in this article, we will discuss the concept behind Tensorflow Recommenders and with its implementation, we will see how quickly we can set up a system. The major points to be covered further are listed below.', 'Let us begin with the discussions.', 'TensorFlow Recommenders (TFRS) is an open-source TensorFlow package that simplifies the building, evaluation, and deployment of advanced recommender models. TFRS, which is based on TensorFlow 2. x, allows us to create and assess flexible candidate nomination models, freely include item, user, and context information into recommendation models, etc. We can train multi-task models that optimize many recommendation goals at the same time. So ultimately by using TensorFlow serving, we can efficiently serve our obtained models.', 'Many recommender systems aim to extract a few good recommendations from a pool of millions of candidates. A recommender system’s retrieval stage tackles the “needle in a haystack” challenge of identifying a shortlist of promising candidates from a large candidate list. Thankfully TensorFlow Recommenders simplifies the process by constructing two-tower retrieval models. Such models retrieve data in two steps:', 'TensorFlow 2. x and Keras are used to build TFRS, making it both familiar and user-friendly. It’s designed to be modular (so we can easily tweak specific layers and metrics), but it still works as a whole (so that the individual components work well together).', 'To give you an idea of how to utilize TensorFlow Recommenders, I’m demonstrating here a basic use case based on Tensorflow’s official implementation. We train a basic model for movie recommendations using the MovieLens dataset. This dataset contains information about which movies a user watched and what ratings they provided to those movies.', 'We’ll use this dataset to train a model that predicts which movies a user will watch and which they won’t. The so-called two-tower model is a frequent and effective design for this type of task which is a neural network with two sub-models that train representations for questions and candidates separately. A particular query-candidate pair’s score is simply the dot product of the outputs of these two towers (depicted in the below animation).', '', 'Two-Tower Analogy', 'On the query side, the inputs can be anything: user ids, search queries, or timestamps whereas on the candidate side, movie titles, descriptions, synopses, and lists of starring actors. We’ll keep things basic in this example by using user ids for the query tower and movie titles for the candidate tower.', 'Now let’s quickly set up our environment by installing and importing the dependencies.', 'Now we will prepare the dataset taken from the Tensorflow datasets. The MovieLens dataset has two files – one is a Rating file that holds attributes related to movies and users, and another file is Movies which holds information related to a movie. To see how quickly a recommendation can be built we will use only user_id and movie_titles for the system.\xa0', 'To implement the two-tower analogy we need to create a user tower that will map the user_ids into high dimensional vector space, similarly, we will create for movie_titles. These embeddings will later be used in the Keras embedding layer.\xa0', 'Below we are defining the class that holds the recommendation model where two methods are defined: __init__() and compute_loss(). Under the __init__() method we set up primary components of our model i,e., the user_ids, movie_titles representation, and the retrieval task. Comput_loss is defined for model training.', 'Now we will define the user model and movie model using Keras Sequential layer and the retrieval task using TFRS.\xa0', 'Now let us create, compile, and train a retrieval model.', 'To validate the model’s recommendations, the TFRS BruteForce layer is employed. The BruteForce layer is indexed with candidate representations that have already been computed, allowing us to find top movies in response to a query by computing the query-candidate score for all available candidates:']","'!pip install -q tensorflow-recommenders\n!pip install -q --upgrade tensorflow-datasets\n \nimport tensorflow_datasets as tfds\nimport tensorflow_recommenders as tfrs\n \nimport numpy as np\nimport tensorflow as tf\n \nfrom typing import Dict
 Text\nimport pprint', ""# ratings data\nrating = tfds.load('movielens/100k-ratings', split='train')\n# features of all the movies\nmovies = tfds.load('movielens/100k-movies', split='train')\n \n# limiting the features\nrating = rating.map(lambda x:{'movie_title':x['movie_title'],'user_id':x['user_id']})\nmovies = movies.map(lambda x: x['movie_title'])"", ""user_id_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(mask_token=None)\nuser_id_vocabulary.adapt(rating.map(lambda x: x['user_id']))\n \nmovies_title_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(mask_token=None)\nmovies_title_vocabulary.adapt(movies)\n"", 'class MovieLensModel(tfrs.Model):\n \n  def __init__(\n      self,\n      user_model: tf.keras.Model,\n      movie_model: tf.keras.Model,\n      task: tfrs.tasks.Retrieval):\n    super().__init__()\n \n    # Set up user and movie representations.\n    self.user_model = user_model\n    self.movie_model = movie_model\n \n    # Set up a retrieval task.\n    self.task = task\n \n  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n    # Define how the loss is computed.\n \n    user_embeddings = self.user_model(features[""user_id""])\n    movie_embeddings = self.movie_model(features[""movie_title""])\n \n    return self.task(user_embeddings, movie_embeddings)\n', 'users_model = tf.keras.Sequential([user_id_vocabulary,\n                                   tf.keras.layers.Embedding(user_id_vocabulary.vocab_size(),64)])\nmovie_model = tf.keras.Sequential([movies_title_vocabulary,                                   tf.keras.layers.Embedding(movies_title_vocabulary.vocab_size(),64)])\n \ntask = tfrs.tasks.Retrieval(metrics=tfrs.metrics.FactorizedTopK(\n    movies.batch(128).map(movie_model)))', 'model = MovieLensModel(users_model,movie_model,task)\nmodel.compile(optimizer=tf.keras.optimizers.Adagrad(0.5))\nmodel.fit(rating.batch(4096), epochs=3)', ""recommends = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\nrecommends.index_from_dataset(movies.batch(100).map(lambda title: (title, model.movie_model(title))))\n \nid_ = input('Enter the user_id: ')\n_, titles = recommends(np.array([str(id_)]))\nprint('Top recommendation for user',id_,titles[0, :3])"""
88,anomaly_detection_using_pyfbad/,https://analyticsindiamag.com/a-guide-to-end-to-end-anomaly-detection-using-pyfbad/,"['Essentially, in anomaly detection, we are looking for observations that deviate from the norm, that either outperform or trail what we’ve discovered or defined as normal. Anomaly detection thus provides benefits from both a business and a technical standpoint. To perform the anomaly, one must rely on tools such as SciKit Learn. However, when it comes to performing end-to-end tasks, there are only a few options, such as PyFBAD, a Python-based package. Starting from the beginning, we can load data from various distributed servers to run SOTA algorithms for anomaly detection. We will talk about these tools in this article, but first, we will go over some of the important points listed below.', 'Let’s start the discussion by understanding anomaly detection.', 'Anomalies are data points in a dataset that stand out from the rest and contradict the data’s expected behaviour. These data points or observations differ from the dataset’s typical patterns of behaviour. Anomaly detection is a technique for detecting anomalies in a dataset that is based on unsupervised data processing. Anomalies can be classified into several categories, including outliers, outliers, outliers, outliers, outliers, outliers, and outlier Anomaly patterns that appear in data collection in an ad hoc or non-systematic manner. Drifts, Long-term data change that is slow and asymmetric.', 'Anomaly detection is useful for detecting fraudulent transactions, detecting diseases, and handling case studies with a high-class imbalance. Data science models with more robust anomaly detection techniques can be built.\xa0', 'Outlier analysis (also known as anomaly detection) is a data mining step that detects data points, events, and/or observations that depart from a dataset’s regular behaviour. An unusual amount of data can disclose essential events, such as a technical glitch, or prospective possibilities, such as a change in consumer behaviour. Anomalies are increasingly being detected using machine learning.', 'Unsupervised, semi-supervised, and supervised anomaly detection techniques are the three types. The best anomaly detection method is essentially determined by the labels in the dataset. Supervised anomaly detection techniques require a data set with a complete set of “normal” and “abnormal” labels in order for a classification algorithm to work. The classifier must also be trained as part of this method.\xa0', 'Outlier detection is similar to traditional pattern recognition, with the exception that outlier detection creates a natural strong imbalance between the classes. Because anomaly detection is inherently unbalanced, it is not well-suited to all statistical classification algorithms.', 'Semi-supervised anomaly detection techniques build a model representing normal behaviour using a normal, labelled training data set. They then use that model to spot anomalies by determining how likely it is for the model to generate any given instance.', 'Unsupervised anomaly detection methods detect anomalies in an unlabeled test set of data solely based on the data’s intrinsic properties. The working assumption is that the vast majority of the instances in the data set will be normal, as in most cases. The anomaly detection algorithm will then look for instances that don’t seem to fit in with the rest of the data set.', 'The Isolation Forest algorithm detects anomalies using a tree-based approach. It is based on modelling normal data in order to isolate anomalies that are both few in number and distinct in the feature space. The algorithm essentially accomplishes this as, It generates a Random Forest in which Decision Trees are grown at random: at each node, features are chosen at random, and a random threshold value is chosen to divide the dataset in half.\xa0\xa0', 'It keeps chopping away at the dataset until all instances are isolated from one another. Because an anomaly is usually far away from other instances, it becomes isolated in fewer steps than normal instances on average (across all Decision Trees).', 'Common density-based techniques include K-Nearest Neighbor (KNN), Local Outlier Factor (LOF), and others. Regression and classification systems can both benefit from these techniques.', 'Following the line of highest data point density, each of these algorithms generates expected behaviour. Any points that fall outside of these dense zones by a statistically significant amount are flagged as anomalies. Because most of these techniques rely on the distance between points, it’s critical to scale the dataset and normalize the units to ensure accurate results.', 'A supervised learning model that yields a robust prediction model is the support vector machine (one-class SVM) technique. It is primarily used for classification. The technique employs a series of training examples, each of which is labelled as belonging to one of two groups.\xa0', 'The system then produces criteria for categorizing additional cases. To maximize the difference between the two categories, the algorithm translates examples to points in space.', 'The system identifies a value as an outlier if it is too far outside of either category’s range. If you don’t have labelled data, you can use an unsupervised learning strategy to establish categories by looking for grouping among cases.', 'The PyFBAD library is an unsupervised anomaly detection package that works from start to finish. All ml-flow phases have source codes in this package. With the numerous PyFBAD packages, data can be read from a file such as CSV, databases such as MongoDB, or MySQL. Preprocessing procedures can be used to prepare the read data for the model.\xa0', 'Different machine learning models, such as Prophet or Isolation Forest, can be used to train the model. Results of anomaly detection can be sent by email or slack. In other words, the entire project cycle can be completed utilizing only the source codes provided by PyFBAD and no other libraries.', 'Let’s start with this package first we’ll install the package with pip and import all the dependencies, also in this implementation Plotly dash is used for interactive plotting.', 'As we mention this tool as an end-to-end platform, we can leverage our data from advanced databases; this can be done by the database object. Here we are loading a standard CSV file that holds the stock information for Microsoft and it can be loaded as,\xa0', 'For time-series anomaly forecasts we need to create a feature set that contains a date_time and the data on which we want to detect an anomaly. Here in our case, it is the volume of shares.', 'Now next by using this features set generated above, PyFBAD provides a model object by which we can detect anomalies in it. By this time it has Prophet and Isolation Forest as algorithms to work on.\xa0', 'Now we have detected a set of anomalies in our dataset let’s visualize them using Plotly dash as below, the below first graph shows the main series followed by one showing the anomaly point that model has detected.']","'import plotly.express as px\nimport plotly.graph_objects as go\nfrom pyfbad.data import database as db\nfrom pyfbad.models import models as md\nfrom pyfbad.features import create_feature as cf\n'
 ""# initialize the connection\nconnection = db.File()\ndata = connection.read_from_csv('/content/Microsoft_Stock.csv')\ndata.head()\n"", ""features = cf.Features()\nfeatures_set = features.get_model_data(df=data, time_column_name = 'Date', value_column_name = 'Volume')\nfeatures_set\n"", '# initialize the algorithm\nmodels = md.Model_Prophet()\n# train algorithm on the features\ntrained_features = models.train_model(features_set)\n# get the anomalies\nforecast_anomaly = models.train_forecast(trained_features)\n'"
89,using_pycaret_for_optimization/,https://analyticsindiamag.com/how-to-visualize-different-ml-models-using-pycaret-for-optimization/,"['In machine learning, optimization of the results produced by models plays an important role in obtaining better results. We normally get these results in tabular form and optimizing models using such tabular results makes the procedure complex and time-consuming. Visualizing results in a good manner is very helpful in model optimization. Using the PyCaret library, we can visualize the results and optimize them. In this article, we are going to discuss how we can visualize results from different modelling procedures. The major points to be discussed in the article are listed below.', 'Table of contents', 'What is PyCaret?', 'In one of our articles, we have discussed that PyCaret is an open-source library that helps us in performing a variety of machine learning modelling automatically. This library aims to make the process of machine learning modelling simple using a low code interface. Along with such a low code interface, this library provides modules that are very efficient and low time-consuming.\xa0\xa0', 'To improve the explainability and interpretability of the process this library provides various visualizations implemented using some of the modules. This library combines visualization from various famous packages like yellowbricks, autovig, plotly, etc. in this article using visualization modules from the PyCaret we are going to plot results from the different models.\xa0', 'Let’s start with a classification model.', 'Visualizing a Classification model', 'Let’s start the procedure by importing data. Since we have multiple datasets in the PyCaret library, we can utilize them for practice purposes. In this article, we are going to use a heart disease dataset that has multiple values related to medical conditions that classify whether a person can have heart disease or not. Let’s import it directly from the library.\xa0\xa0', 'Here in the above output, we can see some of the values from the dataset. Now let’s set up a model.', 'In the setup of the model, we have provided the name of the dataset and the name of the target variable. After optimizing the dataset, the setup module has provided information about the variables that the dataset consists of.', 'Let’s fit the model.', 'Here we can see the information about the accuracy and AUC scores of the model. Hereafter fitting the model, the main purpose of the article comes into the picture.', 'Visualizing results\xa0', 'One more thing that is great about the PyCaret library is that we can utilize its features for visualizing the results from the model that we have in tabular form.', 'Plotting the AUC scores:', ""plot_model(random_forest, plot = 'auc')"", 'Here in the plot, we can see that we have a detailed visualization of AUC values from the model. There are various libraries like yellowbricks and autovig that help PyCaret to make such visualizations.', 'Let’s see the confusion matrix in a visualized form.', ""plot_model(random_forest , plot = 'confusion_matrix')"", 'Here we can see the confusion matrix. We can also convert values under the confusion matrix in the percentage.', ""plot_model(random_forest , plot = 'confusion_matrix', plot_kwargs = {'percent' : True})"", 'We can also plot results based on the training data by just passing use_train_data as True. Let’s plot the decision boundary of the model.', ""plot_model(random_forest, plot = 'boundary', use_train_data = True)"", 'Feature engineering plays a crucial role in data modelling. We can check the feature importance using the following lines of codes.', ""plot_model(random_forest, plot = 'feature_all', use_train_data = True)"", 'Here we have discussed the visualization of results by a classification model. We can also plot various results from a regression model.', 'Visualizing a Regression model', 'Let’s fit a regression model using the automobile dataset.', 'Here we can see the results from a random forest regression in a tabular form. Let’s plot these results.', 'Plotting learning curve\xa0\xa0\xa0', ""plot_model(random_forest_r, plot = 'learning', use_train_data = True)"", 'Let’s plot the errors and residual values from the model.', 'Error plot', ""plot_model(random_forest_r, plot = 'error', use_train_data = True)"", 'Residual plot', ""plot_model(random_forest_r, plot = 'residuals', use_train_data = True)"", 'Here we have seen some of the visualizations of results of a regression model. We can also plot results from the cluster modelling.', 'Visualizing a clustering model', 'For clustering, we are using the mice data and we will be fitting the k means model on the data.', 'Now we are ready to plot the results from cluster modelling.', 'Plotting clusters\xa0', ""plot_model(kmeans, plot = 'cluster')"", 'For this visualization, PyCaret uses the plotly package. This package provides interactive plots that are not provided here, we can find these plots here.', 'Let’s plot the clusters in 3D.', 'We can also plot the results like a silhouette and distance. Let’s plot the distance of the clusters.', ""plot_model(kmeans, plot = 'distance')"", 'Here we have seen some of the results visualized from the cluster analysis. We can also visualize the results from the anomaly detection process.', 'Visualizing anomaly detection', 'Let’s fit the model on the similar data that we used for clustering.', 'Let’s plot the results of anomaly detection.', 'plot_model(iforest)', 'We can also plot a\xa0 umap dimensionality plot.', 'plot_model(iforest, plot = ‘umap’)', 'These plots are also generated using the plotly library. We can also see in the plot what are the anomalies in the mice dataset.', 'Final words', 'In the article, we have discussed how we can visualize the results from the model using modules of the PyCaret library. Since every plot was informative enough, we can utilize them for optimizing the models and modelling procedures so that we can improve results.\xa0']","""from pycaret.datasets import get_data\ndata = get_data('heart_disease')""
 ""from pycaret.classification import *\nmodel1 = setup(data = data, target = 'Disease')"", ""random_forest = create_model('rf')"", ""plot_model(random_forest, plot = 'auc')"", ""plot_model(random_forest , plot = 'confusion_matrix')"", ""plot_model(random_forest , plot = 'confusion_matrix', plot_kwargs = {'percent' : True})"", ""plot_model(random_forest, plot = 'boundary', use_train_data = True)"", ""plot_model(random_forest, plot = 'feature_all', use_train_data = True)"", ""data = get_data('automobile')\nfrom pycaret.regression import *\nmodel2 = setup(data = data, target = 'price')\nrandom_forest_r = create_model('rf')"", ""plot_model(random_forest_r, plot = 'learning', use_train_data = True)"", ""plot_model(random_forest_r, plot = 'error', use_train_data = True)"", ""plot_model(random_forest_r, plot = 'residuals', use_train_data = True)"", ""data = get_data('mice')\nfrom pycaret.clustering import *\n \nmodel3 = setup(data, normalize = True, \n                   ignore_features = ['MouseID'],\n                   session_id = 123)\nkmeans = create_model('kmeans')"", ""plot_model(kmeans, plot = 'cluster')"", ""plot_model(kmeans, plot = 'distance')"", ""from pycaret.anomaly import *\n \nmodel4 = setup(data, normalize = True, \n                   ignore_features = ['MouseID'],\n                   session_id = 123)\niforest = create_model('iforest')"", 'plot_model(iforest)'"
90,classification_model_using_pycaret/,https://analyticsindiamag.com/building-an-ml-classification-model-using-pycaret/,"['Decision making is an important aspect of our day to day lives. We, humans, make many decisions every day. From what is to be done in the day to what to wear for the day, whatever we choose to do makes a significant impact on the future. With every decision made comes the learning of the rights and wrongs of it. The same idea can be applied to our systems, and we can create our own decision making and classification models, making use of machine learning algorithms and model creation. A general task for most machine learning algorithms is to recognize objects and entities and separate them into categories. This is done through a process called classification. Classification can help us segregate or differentiate within the vast quantities of data into discrete values such as 0 or 1, True or False, or a pre-defined output label class. Classification and Regression tasks both belong to Supervised Learning, a type of machine learning algorithm where the model learns by example. Along with the input variable, we also provide our model with the corresponding correct labels. So, While training, the model looks at which label corresponds to our data and can find patterns between our data and those corresponding labels.', 'Humans predict how a thing can be referred to and differentiated to a particular class every day. We make use of classification, which helps us make decisions when picking vegetables, for example in a supermarket, whether they are “green”, “perfect”, “rotten”. Speaking In terms of machine learning, we assign a label of one of the classes to every vegetable we hold in our hands. The efficiency of one’s Vegetable Picking, or as some would call it, a classification model, depends on how accurate the decision results were. The more often one goes to the supermarket by himself, the better he will pick out fresh vegetables. Machine learning algorithms and models created work the same way. Classification can also be defined and created as a form of “pattern recognizer”. The classification algorithms applied to the training data might help find the same patterns such as a similar number sequence, words or sentiments within the data sets.\xa0', 'To evaluate the accuracy of our classification model, we always require some accuracy measures. Methods such as Bias and Variance or Precision recall can be used to estimate how accurate our classifiers’ predictions are and how much they can be. Bias tells us the difference between our actual and predicted values, while variance tells us about the model’s sensitivity to fluctuations in the data. Precision can be used to calculate a model’s ability to classify values correctly; conversely, Recall is used to calculate the model’s ability to predict positive values.\xa0', 'PyCaret is an open-source machine learning library available in Python language that uses a lower number of codes and aims to reduce the number of hypotheses to insights within a cycle of time in a Machine Learning experiment created. Thus, it enables data scientists to perform end-to-end experiments more quickly and efficiently than other open-source machine learning libraries. With only a few lines of code, PyCaret enables us to perform complex machine learning tasks. A very simple and easy to use interface where all the operations performed are automatically stored in a custom PyCaret Pipeline that is fully orchestrated for and towards the development of models. PyCaret comes wrapped around with several other frameworks as well, such as scikit-learn, XGBoost, Microsoft LightGBM, spaCy, to name a few.', '\xa0PyCaret enables one to perform both simple and moderately sophisticated analytical tasks that would have required more expertise to understand and perform. PyCaret allows you to go from data preparation to model deployment in just a few seconds in the notebook environment you choose to run it in. Whether adding missing values or transforming categorical data, engineering the present features or optimizing hyperparameters in the present data, PyCaret can help automate it all. Its Machine Learning capabilities can also be seamlessly integrated with other environments supporting Python such as Microsoft Power BI, Tableau, Alteryx and KNIME. This gives immense power and flexibility to the users of these Business Intelligence platforms, who can now integrate PyCaret into the existing workflows and add a layer of Machine Learning without putting in much effort.', 'PyCaret can be used for the following data processing and pre-processing use cases\xa0 :\xa0', 'In this article, we will create a machine learning model, where we will be installing the Pycaret library and load up some custom dataset, more specifically a heart disease dataset where we are solving a binary classification problem by predicting whether or not a person does or does not have heart disease. Then using the PyCaret classification classes, build an automated machine learning classification model. The following execution is partially inspired by a PyCaret video tutorial whose link can be accessed from here.\xa0', 'Our first step will be to Install the PyCaret library. To do so, you can use the following code.', '!pip install pycaret pandas shap', '\xa0We have also used pandas for data analysis & shap, which will help us with our machine learning model’s interpretability of results.', '\xa0Going further, we will now import and call all our dependencies required to create this automated classification model using two lines of the following code.\xa0', 'We will now be importing the dataset. Here we have used a heart disease dataset for the classification model. The heart disease dataset used here is a modified version of the UCI ML repository dataset. Several categorical and numerical features are available and a target column present called “target”. We will try to predict a binary outcome as 1 or 0, where 1 means having been identified as having heart disease and 0 refers to the person not having a heart disease. The dataset can be downloaded from the link here.', 'Reading the dataset using pandas,\xa0', 'Viewing the columns and first five heads from our loaded dataset,\xa0', 'Viewing the data types present in the dataset,', 'We will get the following results,', 'PyCaret works on the concept of experiments, so the machine learning run and model being created here will be known as an experiment. Before we set up the experiment, we will define a list of categorical features for the model to understand.\xa0', 'Passing the categorical features into our experiment model to treat them in a better way. The setup() function will initiate the machine learning experiment and set up training pipelines. There are numerous other parameters as well that can be set for the experiment created within the function. The setup() function must also be called before executing any other function, its two mandatory parameters being “data” and “target”, which will be our main column for operation.', 'Using PyCaret, we can train our model on not one but a heap of different machine learning algorithms all at once to be able to predict the target model. It will show the best algorithm suited for the dataset ranking from the top.\xa0', 'In our case we can see that the Ridge Classifier Model performs better than the others for our dataset processed.\xa0', 'We will be using the best model received from PyCaret. It will automatically partition the data into train and test.', 'We can now see a new column called “Label” now, which shows our results predicted by the model.\xa0', 'We can again load the model and make predictions again,\xa0', 'Transformation Pipeline and Model Successfully Loaded', 'Verifying our Predictions again,\xa0', 'So, as we can see, the prediction provides us again with the same array of predicted results as before!', 'You can also create visualizations from any trained models such as ROC curves, Feature graphs, confusion matrixes and much more.\xa0', 'This article has talked about the PyCaret library and how fewer lines of code can enable us to create machine learning models at ease. We also got a hands-on view of what it takes to create a classification model using PyCaret, which can be saved for future use.\xa0 One can try to create and try different operations on more complex datasets to understand the power of PyCaret. The colab notebook to the above implementation can be found here.', 'Happy Learning!']","' import pandas as pd\n from pycaret.classification import * '
 "" df = pd.read_csv('/content/heart.csv') "", ' df.head() ', ' df.dtypes ', ' age \xa0 \xa0 \xa0 \xa0 \xa0 int64\n sex \xa0 \xa0 \xa0 \xa0 \xa0 int64\n cp\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 int64\n trestbps\xa0 \xa0 \xa0 int64\n chol\xa0 \xa0 \xa0 \xa0 \xa0 int64\n fbs \xa0 \xa0 \xa0 \xa0 \xa0 int64\n restecg \xa0 \xa0 \xa0 int64\n thalach \xa0 \xa0 \xa0 int64\n exang \xa0 \xa0 \xa0 \xa0 int64\n oldpeak \xa0 \xa0 float64\n slope \xa0 \xa0 \xa0 \xa0 int64\n ca\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 int64\n thal\xa0 \xa0 \xa0 \xa0 \xa0 int64\n target\xa0 \xa0 \xa0 \xa0 int64\n dtype: object ', "" cat_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'thal'] "", "" #setting the experiment\n experiment = setup(df, target='target', categorical_features=cat_features)\xa0 "", ' #show the best model and their statistics\n best_model = compare_models() ', ' #viewing results from the bottom\n predict_model(best_model, df.tail()) ', "" Saving the created model as a pickle file,\xa0\n save_model(best_model, model_name='ridge-model') "", "" Transformation Pipeline and Model Successfully Saved\n (Pipeline(memory=None,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0steps=[('dtypes',\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0DataTypes_Auto_infer(categorical_features=['sex', 'cp', 'fbs',\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'restecg', 'exang',\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'thal'],\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0display_types=True, features_todrop=[],\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0id_columns=[],\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0ml_usecase='classification',\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0numerical_features=[], target='target',\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0time_features=[])),\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0('imputer',\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0Simple_Imputer(categorical_strategy='not_available',\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0fill_value_categorical=Non...\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0('fix_perfect', Remove_100(target='target')),\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0('clean_names', Clean_Colum_Names()),\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0('feature_select', 'passthrough'), ('fix_multi', 'passthrough'),\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0('dfs', 'passthrough'), ('pca', 'passthrough'),\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0['trained_model',\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0fit_intercept=True, max_iter=None,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0normalize=False, random_state=899,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0solver='auto', tol=0.001)]],\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0verbose=False), 'ridge-model.pkl') "", "" model = load_model('ridge-model') "", ' model.predict(df.tail()) ', ' array([0, 1, 0, 0, 1]) ', "" lr = create_model('lr')\n # plotting a model\n plot_model(lr) "", "" plot_model(knn, plot = 'confusion_matrix') """
91,rule_mining_using_pycaret/,https://analyticsindiamag.com/a-guide-to-interpretable-association-rule-mining-using-pycaret/,"['Association rule mining is one of the major concepts in the field of data science that helps mainly in making marketing-related decisions and requires transactional data. Making this procedure interpretable and explainable plays an important role in decision making. In this article, we will discuss association rule mining and we will do a hands-on implementation of this technique using the PyCaret library. Using PyCaret for this task makes it more interpretable and explaining. The major points to be discussed in the article are listed below.', 'PyCaret is one of the open-source libraries that provide machine learning solutions with the aim of low coding in modelling and hypothesis testing. This library can be utilized in a variety of end-to-end machine learning experiments. Its low coding feature makes the modelling procedure very efficient and low time-consuming. Also, one thing that is noticeable about the library is that the module designed under the library is faster than the manual models.\xa0', 'With these all features, this library also provides several interactive visualizations of models and data that can also be used to make the machine learning procedure highly interpretable and explainable. In this article, we will discuss how we can perform association rule mining using the PyCaret library. We can install this library in the Google Colab environment using the following lines of code:', '!pip install pycaret', 'Association rule mining is a rule-generating machine learning method where rules tell us about the strength of the relationship between variables in a large dataset. We mainly find usage of association rules in market basket analysis where a strong positive relation between two products makes the seller sell them together and earn more profit.\xa0 Even the name of this machine learning method explains what we are trying to do. We are finding association rules between variables from a large dataset.\xa0', 'This method mostly intended to find strong rules from a large dataset or database by defining and using some measure of interestingness. For example, if {corn, cheese} → {pizza base} found in the rules that we are mining, will indicate that customers buying cheese and corn together are more likely to also buy pizza. Association rules mining helps in making decisions about marketing activities such as pricing or product placement.\xa0', 'In this article, we are going to use the PyCaret library for association rule mining that has a special module for the procedure. Let’s take a look at the module.', 'Pycaret has a pycaret.arules module for association rule mining that uses a supervised method of machine learning. This module can be utilized for finding relationship measures between the variables of the dataset. One of the interesting things about the module is that it automatically converts datasets with transaction values into the shape that is required for market basket analysis. Since PyCaret is specially designed for low code machine learning this algorithm also requires low code to design a better model.', 'We mostly found the usage of association rule mining in market basket analysis. So in this article also we will use samples from the Online Retail Dataset. This dataset contains details of transactions that occurred between 01/12/2010 and 09/12/2011 in an online retail store. This dataset contains the following variables:', 'We can find the original dataset here. We will be using the dataset that PyCaret provides for practice, we can import the dataset using the following lines of codes.', 'In this implementation, we are using the dataset of France only. In the output, we can see some of the values from the dataset. Now we are ready to implement our association rule mining project.', 'After calling the data we are required to import our association rule module and convert the data from transactional data to market basket data shape. We can do this using the following lines of codes.', 'Here in the output, we can see the unique number of transactions in our dataset that is the unique count of the invoice number and the unique number of items that we get using the Description column. Since we haven’t ignored any of the items we get no values.', 'Modelling association rules', 'We can simply instantiate a model using the following lines of codes.', 'model1 = create_model()', 'When we talk about the parameters of our choice we can define the following parameters in the model:', 'Let’s print the shape of the created rules and head.', 'Here in the output we can see the antecedents and consequents with their support, confidence, lift, leverage, and conviction values.', 'In the above step, we simply created a model. While converting the dataset we have seen an option to ignore items in the output, in the setup module we can define the ignore_item parameter to ignore any item from the list. This we can perform using the following lines of codes.', 'Here we can see that we have ignored the item POSTAGE. Let’s model this converted data to find the association rules. Let’s create and print details of our model while ignoring an item.', 'Here we can see the difference between this output and the above output.', 'Visualizing association rule mining', 'The PyCaret library is famous because of one more thing that is interpretability and explainability of models. That means we can visualize our models and their results and understand them better. Let’s visualize our model. Before visualizing models in Google Colab we are required to enable Colab for Pycaret. This can be done using the following lines of codes.', 'Let’s plot the model.', 'plot_model(model2)', 'We can see that the visualization that we get is on plotly which means they are interactive. We are not able to post interactive visualizations here. In practice, you can interact with them.', 'We can also plot this visualization in three dimensions.', ""plot_model(model2, plot = '3d')"", 'Here, the above output is also interactive and three-dimensional. You can find these visualizations in this notebook.', 'Final words', 'In this article, we have gone through the process that can be followed for implementing solutions based on association rule mining using the PyCaret library.\xa0 We found that using this python library we can perform this major and difficult task very efficiently and easily.\xa0']","'!pip install pycaret'
 ""from pycaret.datasets import get_data\ndata = get_data('france')\n"", ""from pycaret.arules import *\nexp_arul101 = setup(data = data, \n                    transaction_id = 'InvoiceNo',\n                    item_id = 'Description') \n"", 'model1 = create_model()', ""exp_arul101 = setup(data = data, \n                    transaction_id = 'InvoiceNo',\n                    item_id = 'Description',\n                    ignore_items = ['POSTAGE']) \n\n"", 'model2 = create_model()\nprint(model2.shape) \nmodel2.head()', 'from pycaret.utils import enable_colab\nenable_colab()', 'plot_model(model2)', ""plot_model(model2, plot = '3d')"""
92,an_explainable_ai_toolkit/,https://analyticsindiamag.com/a-guide-to-quantus-an-explainable-ai-toolkit/,"['Explainable AI refers to strategies and procedures used in the application of artificial intelligence (AI) that allow human specialists to understand the solution’s findings. To ensure that explanation methods are correct, they must be systematically reviewed and compared. In contrast to achieving quantitative explanation, in this article, we will discuss Quantus, a Python library that evaluates a convolutional neural network’s working, predictions and explanation of parameters. Below is the list of major points that will be discussed in this article.', 'Let’s first discuss Explainable AI.', 'Explainable artificial intelligence (XAI) refers to a set of processes and strategies that enable humans to comprehend and trust the results of machine learning algorithms. “Explainable AI” refers to the ability to define an AI model, its predicted impact, and potential biases.\xa0', 'It contributes to the definition of model correctness, fairness, transparency, and outcomes in AI-powered decision-making. The ability of an organization to generate trust and confidence is critical when deploying AI models. AI explainability also helps to implement a responsible AI development strategy.', 'Explainable AI is analogous to “showing your work” in a math problem. All AI decision-making and machine learning processes do not take place in a black box – it is a transparent service designed to be dissected and understood by human practitioners. In order to add ‘explanation’ to the output, input/output mapping is essential.', 'Understanding how an AI-enabled system has resulted in a specific output has numerous advantages. Explainability can assist developers in ensuring that the system is performing as expected, it may be required to meet regulatory standards, or it may be critical in allowing those affected by a decision to challenge or change the outcome.', 'Despite a lot of interest and action in the subject of XAI, evaluating explainable approaches is still a topic that hasn’t been solved. Because explaining, unlike traditional Machine Learning (ML), requires no ground-truth data, there is no commonly agreed definition of what constitutes a right explanation, and even less so, which properties an explanation should satisfy.\xa0', 'Because of the lack of standardized assessment techniques in XAI, diverse parameterizations, preprocessing, and normalizations are frequently used, each producing different or even contradictory results, making evaluation outcomes difficult to comprehend and compare.', 'For these reasons, we frequently rely on a qualitative assessment of explanation methods, such as believing that humans know what an accurate explanation would (or should look like, often ignoring the function of the explained model in the explanation process).\xa0 The notion that people can recognize a correct explanation, on the other hand, is rarely valid.\xa0', 'In order to address this problem, Anna Hedström et al. created Quantus, a versatile and comprehensive toolbox that collects, organizes, and explains a wide range of assessment metrics recommended for explanation techniques.', 'Quantus delivers a set of more than 25 reference metrics for evaluating explanations of ML forecasts to its intended users ML and XAI practitioners. Furthermore, it provides detailed instructions on how to apply these measurements, as well as advice on potential dangers. We can see some examples of analysis that can be done with Quantus in the diagram below.', 'For any type of CNN problem, as demonstrated in the above picture, section-a where a simple qualitative comparison of XAI approaches is frequently insufficient to determine whether the gradient-based method is favoured, such as Saliency, Integrated Gradients, Gradient Shape, or FusionGrad. We can get more information about how the methods compare with Quantus by doing a holistic quantification on several evaluation criteria (shown in section b) and running a sensitivity analysis on how a single parameter, such as the pixel replacement strategy of a faithfulness test, affects the ranking of explanation methods (shown in section c).', 'Here in this section, we’ll utilize Quantus on MNIST handwritten digits dataset to explain how a standard CNN works. Here we generate explanations for saliency and integrated gradients map qualitatively (in terms of visualizations) and quantitatively (in terms of scores) for a set of the data.', 'Here I’m replicating the official starter guide and will explain in the best possible manner. The CNN model used here is called LeNet5 which is a basic CNN model with all essential layers. It was one of the first convolutional neural networks to be developed and it aided in the advancement of deep learning. Now to move further we need to install and import all the dependencies as below.', 'Now, let’s load the model and dataset with a data loader.', 'Then, using the Quantus library, we generate qualitative explanations for some test set samples that we want to evaluate.', 'The following is a visualization of attributions for a given model and input-output pairs. Which is nothing but the saliency map of the digits and gradient map (feature importance map) given by the model.', 'Because we lack ground truth of what the explanations should look like, it is difficult to make inferences about the explainable evidence that we encounter, the qualitative features of the Saliency and Integrated Gradients explanations may appear to be quite uninterpretable.', 'We can utilize Quantus to quantitatively analyze the explanation. As a first step, we might want to see how sensitive the explanations are to relatively little changes. We can test the Max-Sensitivity of our explanations by evaluating them against a range of quantitative criteria. This metric examines how the explanations change the most when subjected to minor disturbances.\xa0', 'Get the scores for saliency and integrated gradients map as follows.', 'Here are the scores,\xa0', 'Now when the max-Sensitivity scores for the Saliency and Integrated Gradients explanations are compared, we may infer that Saliency is less resilient (scores 0.74 +-0.21std) in this experimental scenario than Integrated Gradients (scores 0.28 +-0.07std).', 'For details on code and implementations, please refer to this notebook.']","'# installing dependencies\n! pip install quantus\n! pip install captum\n! git clone https://github.com/understandable-machine-intelligence-lab/Quantus.git\n \nimport quantus\nimport torch\nimport torchvision\nfrom torchvision import transforms\n# load the model from repo\nfrom Quantus.quantus.helpers.models import LeNet\nimport captum\nfrom captum.attr import *\nimport pandas as pd\nimport numpy as np\n \n# Enable GPU.\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")'
 '# load the model\nmodel = LeNet()\nmodel.load_state_dict(torch.load(""Quantus/tutorials/assets/mnist""))\n\n# load the data\ntransformer = transforms.Compose([transforms.ToTensor()])\ntrain_set = torchvision.datasets.MNIST(root=\'./sample_data\', train=True, transform=transformer, download=True)\ntest_set = torchvision.datasets.MNIST(root=\'./sample_data\', train=False, transform=transformer, download=True)\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True, pin_memory=True)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=200, pin_memory=True)\n \n# Load a batch of inputs and outputs to use for evaluation.\nx_batch, y_batch = iter(test_loader).next()\nx_batch, y_batch = x_batch.to(device), y_batch.to(device)\n', '# Generate normalized Saliency and Integrated Gradients attributions of the first batch of the test set.\na_batch_saliency = quantus.normalise_by_negative(Saliency(model).attribute(inputs=x_batch, target=y_batch, abs=True).sum(axis=1).cpu().numpy())\na_batch_intgrad = quantus.normalise_by_negative(IntegratedGradients(model).attribute(inputs=x_batch, target=y_batch, baselines=torch.zeros_like(x_batch)).sum(axis=1).cpu().numpy())\n \n# Save x_batch and y_batch as NumPy arrays that will be used to call metric instances.\nx_batch, y_batch = x_batch.cpu().numpy(), y_batch.cpu().numpy()\n', '# Define params for evaluation.\nparams_eval = {\n    ""nr_samples"": 10,\n    ""perturb_radius"": 0.1,\n    ""norm_numerator"": quantus.fro_norm,\n    ""norm_denominator"": quantus.fro_norm,\n    ""perturb_func"": quantus.uniform_sampling,\n    ""similarity_func"": quantus.difference,\n    ""disable_warnings"": True}\n', 'scores_saliency = quantus.MaxSensitivity(**params_eval)(model=model, \n   x_batch=x_batch,\n   y_batch=y_batch,\n   a_batch=a_batch_saliency,\n   **{""explain_func"": quantus.explain, ""method"": ""Saliency"", ""device"": device, ""img_size"": 28, ""normalise"": False, ""abs"": False})\n\nscores_intgrad = quantus.MaxSensitivity(**params_eval)(model=model, \n   x_batch=x_batch,\n   y_batch=y_batch,\n   a_batch=a_batch_intgrad,\n   **{""explain_func"": quantus.explain, ""method"": ""IntegratedGradients"", ""device"": device, ""img_size"": 28, ""normalise"": False, ""abs"": False})\n'"
93,and_interpretable_machine_learning/,https://analyticsindiamag.com/a-guide-to-auto-viml-for-explainable-and-interpretable-machine-learning/,"['Explainability and interpretability in machine learning are important tasks that help non-tech users to understand the modelling procedure and results. It also helps the developers to figure out the working procedure of the black-box algorithms. There will always be a need for explainability and interpretability in modelling. Auto-ViML is a library that can be used for effective explainability and interpretability in machine learning model building. In this article, we will have an introduction to the Auto-ViML through its practical applications. The major points to be discussed in this article are listed below.', 'Let’s start with understanding the Auto-ViML.', 'Auto-ViML is a library for building high-performance interpretable machine learning models that are built using the python language. The name Auto-ViML can be separated into automatic variable interpretable machine learning. The word variable is in the name because using this library we can use multiple variables of the dataset with multiple models in an automatic way. The interpretable stands for interpreting the modelling procedure.', 'When summing up all the main features of the library we can say that using this library we can build interpretable models using the least number of features that are most necessary for the procedure. Based on many trials we can say that Auto-ViML can help in building models with 20 to 99% fewer features than a traditional model using all features with the same performance.\xa0', 'Using this library we can utilize the following things of modelling procedures:', 'By looking at the above points we can say that this library can be very helpful in a variety of modelling procedures. This library is built using scikit-learn, NumPy, pandas, and Matplotlib. While talking about the interpretability generation of the model this library uses the SHAP library.\xa0 We can install this library in our environment using the following line of codes.', 'pip install autoviml', 'or', 'pip install git+https://github.com/AutoViML/Auto_ViML.git', 'After installation, we are ready to use this library for implementation.', 'This implementation is an example that presents the working nature of Auto-ViML. For this, we will be performing data analysis and modelling using the titanic data. This dataset is one of the most famous datasets in the field of data science. We can download the dataset from here.\xa0', 'Before going into the implementation we are required to check the version of our library. The version check can be done using the following lines of code.', 'from autoviml.Auto_ViML import Auto_ViML', 'Now let’s call the dataset.', 'Most of the time using the data set we perform the survival analysis. In this article, we will do the same but one different thing is that we are using the Auto-ViML library for this.\xa0', 'Let’s split the dataset into training and test data.', 'Modelling\xa0', 'In the above, we are just required to import the Pandas library to import the dataset and only one module from the Auto-ViML library. Using only this module we can perform various steps of modelling. Let’s start the procedure.', 'Defining some parameter variables', 'Modelling using XGBoost\xa0\xa0', 'The above lines of code will give us various analyses of data and results. Let’s see them one by one.', 'The above output is our first output which tells us about the shape of training and test set, after that we can see that we have results of binary classification which tells us about the status of hyperparameter tuning and shuffling of the data. This output tells us about the history of data and changes that we need to perform on data. Let’s move toward the second output.', 'The above output is about the variable classification, this can tell us the type of features in the dataset and in what number they are available in the dataset. Also, a basic feature selection removes variables or features that have low relevancy or low information level with the availability of working devices like CPU and GPU. Let’s proceed to the next output.', 'Here in the above output, we can see details about data preparation where this library is checking for missing values in test and train datasets and the process is removing features that are highly correlated and separating float and categorical variables. Let’s move towards our next output.', 'Here in the above output, we can see that we have details of the feature selection process where we used XGBoost and 6 features. Let’s move toward the next output.\xa0', 'Here in the above output, we can see the model-building procedure where we have used the XGBoost model. Let’s move toward the visualization of the results of modelling.', 'In the above, we can see the predictive probability of the model and we can say that we have got a threshold for F-1 score at around 0.26 to 0.28. For cross-checking, the above output library checks accuracy scores that are given in the below output.', 'Now, this library also runs ensembling model techniques to improve the results. Let’s look at the next output.', 'This output tells us about the best model that the library observed with the scores that can help in optimizing the model.\xa0\xa0\xa0', 'Here we can see the final optimal result from the library. We also get the visualization of results. Let’s look at the next output:', 'Here in the above output, we can see the models that are being applied to the data and their scores. We can also see that the highest accuracy model is in the light colour. With these visualizations, we also get visualizations of the confusion matrix, ROC AUC curve, PR AUC Curve, and classification report. Let’s move on to the log loss graph across epochs.', 'Here we can see that with different epochs how loss has been covered.\xa0', 'In the above output, we can see how much time it took to complete the procedure with the directory where the results of the procedure are saved with the graph of feature importance.\xa0']","'pip install autoviml'
 'pip install git+https://github.com/AutoViML/Auto_ViML.git', 'from autoviml.Auto_ViML import Auto_ViML', ""import pandas as pd\ndf = pd.read_csv('https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/'+'titanic.csv')\ndf\n"", 'num = int(0.9*df.shape[0])\ntrain = df[:num]\nprint(train.shape)\ntest = df[num:]\nprint(test.shape)', ""target = 'Survived'\nsample_submission=''\nscoring_parameter = 'balanced-accuracy'\n"", ""m, feats, trainm, testm = Auto_ViML(train, target, test, sample_submission,\n                                    scoring_parameter=scoring_parameter,\n                                    hyper_param='GS',feature_reduction=True,\n                                     Boosting_Flag=True,Binning_Flag=False,\n                                    Add_Poly=0, Stacking_Flag=False,                                    \n                                    Imbalanced_Flag=False, \n                                    verbose=1)  \n"""
94,toolbox_for_adversarial_robustness/,https://analyticsindiamag.com/a-guide-to-advertorch-python-toolbox-for-adversarial-robustness/,"['AdverTorch is a set of tools for studying adversarial robustness. It includes a variety of assault and defence implementations, as well as robust training mechanisms. AdverTorch is based on PyTorch and takes advantage of the benefits of the dynamic computational graph to create succinct and efficient reference implementations. So we will walk through this toolset, its design principles, and how it works in this article. The important points that will be explored in this article are mentioned below.', 'Let’s first discuss an adversarial attack.', 'Adversarial attacks are deceptive acts aimed at undermining machine learning performance, causing model deviance, or gaining access to sensitive data. However, in recent years, the rise of deep learning and its incorporation into a wide range of applications has reawakened interest in adversarial machine learning.', 'In the security industry, there’s a growing fear that adversarial flaws might be used to attack AI-powered systems. Unlike traditional software, which requires engineers to manually set instructions and rules, machine learning algorithms learn via experience.', 'To design a lane-detection system, for example, the developer creates a machine learning algorithm and trains it by feeding it a large number of tagged photos of street lanes from various angles and lighting conditions. The machine learning algorithm is then fine-tuned to detect recurrent patterns in pictures of street lanes. If the algorithm structure is proper and there is enough training data, the model will be able to detect lanes in new images and videos with high accuracy.', 'Machine learning algorithms, despite their effectiveness in tough areas such as computer vision and speech recognition, are statistical inference engines: complicated mathematical functions that convert inputs to outputs.', 'When a machine learning system identifies an image as containing a specific object, it has discovered that the pixel values in that image are statistically similar to other photographs of the object examined during training.', 'Adversarial attacks take advantage of this feature to confuse machine learning systems by tampering with their input data. A malicious actor can, for example, influence a machine-learning algorithm to identify an image as something it is not by adding tiny and inconspicuous patches of pixels to it.', 'In adversarial attacks, the sorts of perturbations used are determined by the target data type and desired effect. To be reasonably adversarial, the threat model must be tailored for different data modalities. Small data disturbance, for example, makes sense to consider as a threat model for images and audios because it will not be clearly detected by a human but may cause the target model to misbehave, resulting in a discrepancy between human and machine.', 'However, for some data kinds, like text, the perturbation may disturb the semantics and be easily identified by humans by just changing a word or a letter. As a result, the threat model for text should be distinct from that for image or voice.', 'AdverTorch is a tool developed by the Borealis AI research lab that employs a number of attack-and-defence tactics. The goal of AdverTorch is to give academics the tools they need to do research on all areas of adversarial attacks and defence.', '\xa0Attacks, defences, and thorough training are all included in the current version of AdverTorch. Advorch strives for clear and consistent APIs for attacks and defences, concise reference implementations using PyTorch’s dynamic computational networks, and quick executions with GPU-powered PyTorch implementations, which are vital for attacking-the-loop techniques like adversarial training.', 'The various attack and defence strategies that come with areas as given below.', 'Fast gradient (sign) methods, projected gradient descent methods, Carlini-Wagner Assault, spatial transformation attacks, and other gradient-based attacks are available. Single-pixel assault, local search attack, and Jacobian saliency map attack are some of the gradient-free approaches.', 'Apart from specific assaults, it also includes a wrapper for the Backward Pass Differentiable Approximation (BPDA), an attack approach that improves gradient-based attacks against defended models with non-differentiable or gradient-obfuscating components.', 'The library includes two defence strategies:\xa0 preprocessing-based defences and ii) robust training. It implements the JPEG filter, bit squeezing, and several types of spatial smoothing filters for preprocessing-based protection. There is an example of a robust training approach implemented in their official repository. There is a script for adversarial training on the MNIST dataset there you can check it out here.', 'Here in this section, we’ll implement an L2PDGAttack which is a gradient-based attack. To implement this attack we need to install and import respective dependencies. The library is not optimized for the latest version of Pytorch so needs to install the respective version in order to carry out this attack.\xa0\xa0\xa0', 'Here we are going to perform an attack on a pre-trained transfer learning model called VGG16 with its pre-trained weights for the ImageNet competition.\xa0', 'Now we’ll load the image and associated label that later be used to check the severity of the attack.', 'Below is some utility user-defined function that is used to visualize the result and conversion procedure.', 'Now let’s perform the attack, The hyperparameters that we defined when using L2PGDAttack are as follows:', 'And here is the result,']","'# install torch\n! pip install torch==1.3.0 torchvision==0.4.1\n \n# install library\n! pip install advertorch\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.models import vgg16\n \n# preprocessing dependencies\nfrom advertorch.utils import predict_from_logits\nfrom advertorch_examples.utils import ImageNetClassNameLookup\nfrom advertorch_examples.utils import get_panda_image\nfrom advertorch_examples.utils import bhwc2bchw\nfrom advertorch_examples.utils import bchw2bhwc\n \n# load the attack class\nfrom advertorch.attacks import L2PGDAttack\n \ndevice = ""cuda"" if torch.cuda.is_available() else ""cpu""\n'
 '# load the model\nmodel = vgg16(pretrained=True)\nmodel.eval()\nmodel = nn.Sequential(model)\nmodel = model.to(device)\n', '# load the image and index for true label\nnp_img = get_panda_image()\nimg = torch.tensor(bhwc2bchw(np_img))[None, :, :, :].float().to(device)\nlabel = torch.tensor([388, ]).long().to(device) # true label\nimagenet_label2classname = ImageNetClassNameLookup()\n', 'def tensor2npimg(tensor):\n    return bchw2bhwc(tensor[0].cpu().numpy())\n \ndef _show_images(enhance=127):\n    np_advimg = tensor2npimg(advimg)\n    np_perturb = tensor2npimg(advimg - img)\n \n    pred = imagenet_label2classname(predict_from_logits(model(img)))\n    advpred = imagenet_label2classname(predict_from_logits(model(advimg)))\n \n    plt.figure(figsize=(10, 5))\n    plt.subplot(1, 3, 1)\n    plt.imshow(np_img)\n    \n    plt.axis(""off"")\n    plt.title(""original image\\n prediction: {}"".format(pred))\n    plt.subplot(1, 3, 2)\n    plt.imshow(np_perturb * enhance + 0.5)\n    \n    plt.axis(""off"")\n    plt.title(""The perturbation"")\n    plt.subplot(1, 3, 3)\n    plt.imshow(np_advimg)\n    plt.axis(""off"")\n    plt.title(""Perturbed image\\n prediction: {}"".format(advpred))\n    plt.show()\n', 'adversary = L2PGDAttack(\n    model, eps=1., eps_iter=1.*2/40, nb_iter=40,\n    rand_init=False, targeted=False)\nadvimg = adversary.perturb(img, label)\n_show_images()\n'"
95,optimization_machine_learning_toolkit/,https://analyticsindiamag.com/a-guide-to-omlt-an-optimization-machine-learning-toolkit/,"['The Optimization and Machine Learning Toolkit (OMLT) is an open-source software program that incorporates machine-learning-trained neural networks and gradient-boosted tree surrogate models into bigger optimization problems. We will talk about this library, its various functions, and its design in this article. Finally, we’ll see a practical demonstration of how to load our standard conventional Keras model into this framework, which will then be used for optimization. The major points to be discussed in this article are listed below.\xa0', 'Let’s first understand this toolkit.', 'The optimization and machine learning toolkit is an open-source software program for optimizing neural network (NN) and gradient-boosted tree high-level representations (GBTs). NNs or GBTs can be integrated into bigger decision-making issues by optimizing over-trained surrogate models.\xa0', 'Optimizing a neural acquisition function or verifying neural networks are two examples of computer science applications. Gray-box optimization in engineering combines mechanistic, model-based optimization with surrogate models learned from data.\xa0', 'GBTs are supported by an ONNX interface (ONNX is an intermediary machine learning framework used to convert between multiple machine learning frameworks.) and NNs are supported by both ONNX and Keras interfaces in OMLT.', 'To encode the optimization formulations, OMLT translates these pre-trained machine learning models into the algebraic modelling language Pyomo (Pyomo allows users to define optimization problems in Python in a style that is close to the notation frequently used in mathematical optimization.)', 'A formulation containing the decision variables, objectives, constraints, and any parameters is required as input to mathematical optimization solver software. OMLT streamlines the normally time-consuming and error-prone procedure of converting previously trained NN and GBT models into optimization formulas appropriate for solver software. OMLT uses ONNX as an input interface because the ONNX interoperability capabilities allow OMLT to support packages such as Keras, PyTorch, and TensorFlow.', 'OMLT is a much more generic tool that incorporates both NNs and GBTs, a large number of input models via ONNX interoperability, fully-dense and convolutional layers, many activation functions, and a variety of optimization formulations.', 'For optimization, OMLT uses Pyomo, a Python-based algebraic modelling language. Because most machine learning frameworks utilize Python as their primary interface, Python is a suitable starting point for OMLT. Pyomo provides a flexible modelling interface to Pyomo-enabled solvers: switching solvers enables OMLT users to choose the appropriate optimization solution for an application without directly dealing with each solver. Pyomo is frequently used by OMLT.\xa0', 'To begin, Pyomo’s efficient auto-differentiation of nonlinear functions utilizing the AMPL solver library allows for NN nonlinear activation functions. Second, Pyomo’s numerous expansions, such as decomposition for large-scale issues, enable OMLT to interact with cutting-edge optimization methodologies.', 'The most crucial aspect is that OMLT employs Pyomo blocks. Pyomo blocks in OMLT are used to encapsulate the GBT or NN components of a larger optimization formulation. When linking to the inputs and outputs of a Pyomo block, users just need to understand the input/output structure of the NN or GBT.', 'OmltBlock is a Pyomo block that delegates the generation of the surrogate model’s optimization formulation to the PyomoFormulation object. Users of OMLT build the input/output objects, such as constraints, that connect the surrogate model to the wider optimization problem and the user-defined variables.\xa0', 'The surrogate’s optimization formulation is derived automatically from its higher level (ONNX or Keras) representation. Users of OMLT can also define a scale object for the variables as well as a dictionary of variable boundaries. Some optimization formulations demand scaling and variable bound information, which may not be present in ONNX or Keras representations.', 'For GBTs, OMLT automatically creates the optimization formulation from the higher-level representation, such as ONNX. Because neural networks are substantially more difficult to optimize, OMLT instead generates an intermediate representation (NetworkDefinition) of the neural network that serves as a doorway to various other mathematical optimization formulations of neural networks.', 'We’ll go over how to import your neural networks into OMLT in this section. OMLT includes an importer for ONNX-saved neural networks. This notebook demonstrates how to import neural networks from a variety of popular machine learning frameworks into ONNX. Finally, we demonstrate how to open an ONNX file with OMLT.', 'We’ll use the Pima Indians Diabetes dataset to train a neural network that can predict whether a patient has diabetes based on a set of medical data throughout this notebook.', 'The below implementation is part of official implementations.\xa0', 'First, install the packages.', 'Let’s first load and prepare the data.\xa0', 'When developing optimization models, it’s crucial to establish variable boundaries. In order to obtain a tighter MIP formulation of the ReLU activations, OMLT demands that all input variables be limited if the neural network incorporates ReLU activation functions.', 'The OMLT program offers a function for writing the input boundaries in a format that can be read later by the ONNX model. We start by computing limits for the eight input variables in our rudimentary neural networks. We’ll use input bounds to preserve the ONNX model.', 'Now we’ll build and train a Keras based model that the latter will load to OMLT.', 'Now we’ll import the network to OMLT for that we import the function used to write the ONNX model together with its bounds. After this, we can simply now export the Keras model to ONNX.', 'That’s how we have imported our model into OMLT.']","'! pip install omlt\n! pip install onnx\n! pip install tf2onnx'
 ""import pandas as pd\nimport numpy as np\n\ndata = pd.read_csv('/content/diabetes.csv')\nX = data.drop(['Outcome'], axis=1).values\nY = data['Outcome'].value"", '# computing bounds\nlb = np.min(X, axis=0)\nub = np.max(X, axis=0)\ninput_bounds = [(l, u) for l, u in zip(lb, ub)]\ninput_bounds\n', ""import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n \n# model\nmodel = Sequential()\nmodel.add(Dense(15, input_dim=8, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='linear'))\n# compile\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n \n# train\nmodel.fit(X, Y, epochs=20, batch_size=10)"", 'import tempfile\nfrom omlt.io import write_onnx_model_with_bounds\nimport tf2onnx\n\nonnx_model, _ = tf2onnx.convert.from_keras(model)\n \nwith tempfile.NamedTemporaryFile(suffix=\'.onnx\', delete=False) as f:\n    write_onnx_model_with_bounds(f.name, onnx_model, input_bounds)\n    print(f""Wrote ONNX model with bounds at {f.name}"")\n'"
96,be_done_with_causalml/,https://analyticsindiamag.com/what-is-uplift-modelling-and-how-can-it-be-done-with-causalml/,"['Uplift modelling is a predictive modelling technique that uses machine learning models to estimate the treatment’s incremental effect at the user level. It’s frequently used for personalizing product offerings, as well as targeting promotions and advertisements. In the context of causal inference, in this article, we will discuss the uplift modelling, its types of modelling and lastly, we will see how a Python-based package called CausalML can be used to address the causal inference. Following are the major points to be discussed in this article.\xa0\xa0\xa0', 'Let’s start the discussion by understanding the uplift modelling.\xa0\xa0', 'Uplift modelling is a predictive modelling technique that predicts the incremental influence of a therapy (such as a direct marketing campaign) on a person’s behaviour. Uplift modelling uses a randomized scientific control to test the efficacy of an intervention as well as build a predictive model that projects the incremental response to the activity.\xa0', 'The response could be discrete (for example, a website visit) or continuous (for example, a phone call) (for example, customer revenue). Uplift modelling is a data mining approach that has mostly been utilized for up-sell, cross-sell, churn, and retention operations in the financial services, telecoms, and retail direct marketing industries.', 'Machine learning is used to answer the question, “How likely is the consumer to purchase in the future?” in the propensity to purchase model, which essentially explains a customer’s behaviour toward a given action. This is improved by uplift modelling, which addresses more urgent issues:', 'To put it another way, a traditional propensity model (as well as most machine learning algorithms) predicts the outcome (y) based on a set of variables (x). Given certain variables, uplift seeks to determine the influence of therapy (t) on the target (y).', 'The term “uplift” refers to the increased chance of the outcome with the treatment as compared to the outcome without the treatment. We cannot directly perceive this difference or causal effect but must deduce it through an experiment. As indicated in the picture below, it is highly beneficial to visualize a 2 x 2 matrix with four kinds of persons (say) to be classed as (a) Persuadable, (b) Sure Thing, (c) Do-Not-Disturb, and (d) Lost Cause.', 'We target the “a” demographic, those who are Persuadable, in order to encourage the desired reaction. The treatment is either ineffective or wasteful for everyone else, including the Do-Not-Disturbs. It’s risky to “wake a sleeping dog” by contacting the Do-Not-Disturbs.', 'Finding persuadable is the goal of uplift modelling. Of course, uplift modeling may be used to model any projected outcome, human or not, such as the influence of fertilizer on crop yields or the sending of political campaign emails.\xa0', 'Uplift modelling focuses on the effectiveness of the treatment, whereas typical predictive modelling focuses on the result. Then you may focus your efforts on the cases that are most likely to benefit you.', 'Direct modelling and indirect modelling are the two basic approaches here. The fundamental difference between the two approaches is how uplift models are measured and evaluated.', 'In direct modelling, we “directly” model the difference in probabilities between two distinct groups. There are numerous approaches to this, almost all of which rely on tree-based algorithms that have been slightly modified to accommodate uplift modelling.', 'Tree-based models are ideal because they naturally model at the group level by iteratively splitting a group into two groups with each splitting decision. Unlike traditional tree-based models, which are designed to divide data into smaller and smaller homogeneous groups, uplift models are designed to divide our customers into heterogeneous groups each time they split (by maximizing a measure of uplift).\xa0', 'They employ various splitting criteria, such as Kullback-Leibler divergence, Euclidean Distance, p-value, and Chi-squared Distance. Hundreds of trees would be fitted in an ensemble fashion, similar to traditional tree-based methods.', 'Regular response models are repurposed to infer uplift using indirect uplift modelling techniques (meta-learners), which can be based on any base algorithm. We are modelling the expected value of the response for different treatments, rather than attempting to optimize some measure of uplift directly.', 'For our Direct Mail campaign, we would calculate the probability that the customer will use their credit card if the DM is sent, and the probability that they will use their product if the DM is not sent. The estimated uplift is the difference between the two estimated probabilities. In practice, this can be a two-model approach (a separate model fitted to all control/treatment groups) or a unified model (a single model with the allocated treatment part of the feature space).', 'Below are some of the possible applications that are briefed about various industries.', 'CausalML is a Python module that provides a suite of uplift modelling and causal inference tools that are based on cutting-edge research and machine learning algorithms. Traditional causal analysis approaches, such as performing t-tests on randomized trials (A/B testing), can estimate the Average Treatment Effect (ATE) of a treatment or intervention.\xa0', 'However, in many applications, estimating these impacts at a finer scale is frequently desirable and useful. CausalML can be used by the end-user to estimate the Conditional Average Treatment Effect (CATE), which is the effect at the individual or segment level. Such estimates can enable a wide range of applications for personalization and optimization by applying different treatments to various users.', 'Uplift modelling is a crucial modeling approach made possible by CausalML. Uplift modeling is a causal learning approach for estimating an experiment’s individual treatment effect. Using experimental data, the end-user can calculate the incremental impact of a treatment (such as a direct marketing action) on an individual’s behaviour.', 'For example, if a corporation is deciding between many product lines to up-sell / cross-sell to its clients, CausalML can be used as a recommendation engine to identify products that yield the maximum expected lift for each given user.', 'CausalML provides a consistent API for running uplift algorithms, making it as simple as fitting a standard classification or regression model. The included metrics and visualization functions, such as uplift curves, can be used to assess model performance. The first version of CausalML includes eight state-of-the-art uplift modelling algorithms (shown in the below figure).', 'Targeting optimization, engagement personalization, and causal impact analysis are just a few of the use cases for CausalMLs.', 'We may use CausalML to target promotions to the people who will bring the most value to the company. For example, we can provide promotions to consumers who are more likely to utilize a new product as a result of exposure to the promotions in a cross-sell marketing campaign for existing customers.', 'We can also use CausalML to examine the causal impact of a specific event using experimental or observational data with rich attributes. For example, we can examine how a customer cross-sells event influences long-term platform expenditure.', 'To personalize engagement, CausalML can be employed. A corporation can communicate with its customers in a variety of ways, such as offering up-sell options or using message channels for interactions. CausalML can be used to assess the effect of each combination for each client and present customers with the most customized offers possible.']",
97,hyperparameter_optimization_using_hpbandster/,https://analyticsindiamag.com/a-guide-to-hyperparameter-optimization-using-hpbandster/,"['In the field of machine learning, we have witnessed successes in a wide range of application areas. One of the most important tasks on which many tasks are dependent is choosing the correct value of the hyperparameter. Finding the best combination of hyperparameters can be considered as hyperparameter optimization which is a crucial task in the machine learning procedure. In this article, we will discuss a hyperparameter optimization technique named Hyperband. We will also go through a python package for hyperparameter optimization named HpBandSter and we will use this package for implementing hyperparameter optimization. The major points to be discussed in this article are listed below.', 'Table of Contents', 'Let’s begin the discussion by understanding what hyperparameter optimization is.', 'Hyperparameter Optimization', 'In machine learning, we can see various models which use various hyperparameters that are distinct and complex. These hyperparameters are the reason for the generation of an enormous search space while modelling.\xa0 As we move towards deep learning models from traditional machine learning models, we find an increment in the size of search space. Tuning the hyperparameter in a massive search space is a challenging task to perform.', 'Hyperparameter optimization algorithms are those algorithms that help in tuning the hyperparameter in a massive search space. The below image is a representation of approaches for hyperparameter optimization.', 'In one of our previous articles, we have discussed Bayesian optimization and seen some of its advantages. Bayesian optimization can operate very well with the black box function and it is data-efficient and robust with noise. An implementation of Bayesian optimization using the HyperOpt package can be found here.\xa0 Also, we have seen that Bayesian optimization is a sequential process so using Bayesian optimization we can not work well in a parallel situation and the objective function in Bayesian optimization becomes very expensive when performing the estimation on it. So now we are required to find a way to estimate the objective function cheaper which can be done using multi-fidelity optimization. Now let’s understand this approach.', 'Multi-Fidelity Optimization', 'Multifidelity optimization can be considered as a way to increase the accuracy of the model estimation by minimizing the cost associated with the estimation of the objective function. This optimization method leverages both high and low fidelity data. Success halving and hyperband are two types of Multi fidelity optimization. Let’s see what is successive-halving.\xa0', 'Successive Halving', 'We can consider successive halving optimization as a technique for hyperparameter optimization where competition between candidate parameter combinations helps in hyperparameter optimization. It is also an iterative procedure where all the parameter combinations are used at the first iteration for evaluation and the number of resources should be below. Only a few combinations are evaluated in the second iteration with an increase in resources. The below image is a representation of successive halving.', 'Image source', 'In the above image, we can see that as the iterations are increasing the number of candidates is decreasing and the number of resources increases. The main motive behind the successive halving is to keep the best-fit halves and remove the halves that are not important. This is a good way for hyperparameter tuning but because of the trade-off between configuration and cuts it becomes a problem for this technique and Hyperband can be used to solve this problem. Let’s move toward the hyperband algorithm which is also a part of multi-fidelity optimization.', 'We can perform this using the scikit-learn provided module HalvingGridSearchCV and HalvingRandomSearchCV.\xa0', 'Hyperband', 'This method can be considered as an extension of the successive halving method; the motive behind the Hyperband is to perform successive halving frequently so that the trade-off between the number of configurations and allocation of resources can be solved. Also, using successive halving we can identify the best combination in lower time.', 'We can also say that using the Hyperband, we can perform an evaluation with many combinations on the smallest budget and very conservative runs on the full budget. In real life implementation performance of hyperband is well in every range of budget. The below image can be a representation of Hyperband advantages over random search.\xa0', 'Image source', 'In the next section of the article, we will introduce HpBandSter, A tool that can be used for implementing hyperband optimization.', 'What is HpBandSter?', 'HpBandSter is a python package framework that can be used for distributed hyperparameter optimization. In the above section, we have seen the importance of hyperparameter optimization and using this framework we can perform a variety of cutting-edge hyperparameter algorithms including the HyperBand. Using this tool we can initiate a random search to find out the best fit parameter combination in less time.\xa0', 'Using the below code we can install this framework.', 'We can also use the Sci-Kit Learn wrapper hpbandster-sklearn for HpBandSer. Using the below code we can install it.', '!pip install hpbandster-sklearn', 'Implementation of Hyperband', 'In this section, we will discuss how we can implement HyperBand using the scikit learn wrapper for HpBandStar. Let’s start by calling the libraries:', 'Making data and model instances:', 'Defining a search space:', 'search_space = {""max_depth"": [2, 3, 4], ""min_samples_split"": list(range(2, 12))}', 'Fitting the model, data, and search space in the optimization module:', 'Here we can see the results of hyperparameters optimization with the details. Lets check the instance which we created for optimization.', 'search', 'Lets check the best parameter combination from the search instance:', 'search.best_params_', 'We can also use the configuration space instead of a dictionary for defining the search space.', 'Fitting model data and search space in the module.', 'Since ultimately we are performing successive halving\xa0 we are required to perform early stopping so that we can configure the resources and budget. Configuration can be done in the following way.', 'Let’s check the final instance as,', 'search.get_params()', 'In the above output, we can see all the details of the module we are using for hyperparameter optimization.', 'Final Words', 'In the article, we have gone through the introduction of hyperparameter optimization, Bayesian optimization, successive halving, and hyperband optimization. Then we have discussed a framework HpBandSter for hyperparameter optimization which also includes hyperband optimization.']","'!pip install hpbandster'
 '!pip install hpbandster-sklearn', 'import numpy as np\nfrom sklearn.datasets import load_iris\nfrom sklearn import tree\nfrom sklearn.utils.validation import check_is_fitted\nfrom hpbandster_sklearn import HpBandSterSearchCV\n', 'X, y = load_iris(return_X_y=True)\nclf = tree.DecisionTreeClassifier(random_state=0)\nnp.random.seed(0)\n', 'search_space = {""max_depth"": [2, 3, 4], ""min_samples_split"": list(range(2, 12))}', ""search = HpBandSterSearchCV(clf, \n                            search_space,\n                            random_state=0, n_jobs=1, n_iter=10, verbose=1\n                            optimizer = 'hyperband').fit(X, y)\n"", 'search', 'search.best_params_', 'import ConfigSpace as CS\nimport ConfigSpace.hyperparameters as CSH\n\nsearch_space = CS.ConfigurationSpace(seed=42)\nsearch_space.add_hyperparameter(CSH.UniformIntegerHyperparameter(""min_samples_split"", 2, 11))\nsearch_space.add_hyperparameter(CSH.UniformIntegerHyperparameter(""max_depth"", 2, 4))\n', ""search = HpBandSterSearchCV(clf, \n                            search_space,\n                            random_state=0, n_jobs=1, n_iter=10, verbose=1,\n                            optimizer = 'hyperband').fit(X, y)\n"", ""search = HpBandSterSearchCV(\n    clf,\n    search_space,\n    resource_name='n_samples',\n    resource_type=float, \n    min_budget=0.2,\n    max_budget=1, optimizer = 'hyperband'\n)\n"", 'search.get_params()'"
98,neural_networks_using_foolbox/,https://analyticsindiamag.com/how-to-ensure-robustness-of-neural-networks-using-foolbox/,"['Most advanced machine learning models based on CNN can now be easily fooled by very small changes to the samples on which we are going to make a prediction, and the confidence in such a prediction is much higher than with normal samples. So, in order to improve our performance on such samples, we should test the model before mass production. In terms of the model’s robustness, in this article, we will talk about adversarial attacks and a Python-based toolbox that can help us test some predefined attacks using a unified and simple API. Following are the major points to be discussed in this article.', 'Let’s start the discussion by understanding the adversarial attacks.', 'Adversarial machine learning is a type of machine learning that tries to exploit models by creating hostile assaults based on publicly available model data. To make a machine learning model fail is the most prevalent explanation.', 'The great majority of machine learning algorithms were created to work on certain problem sets with data from the same statistical distribution for training and testing. Opponents may provide data that violates that statistical assumption when those models are applied to real-world data. This material could be arranged in such a way that it takes advantage of vulnerabilities in the system and taints the findings.', 'An adversarial attack is a method of causing the machine learning model to misclassify objects by making small changes to them. Such attacks are known to be vulnerable to neural networks (NN). Historically, research into adversarial methods began in the field of image recognition. It has been demonstrated that minor changes in images, such as the addition of insignificant noise, can cause significant changes in classifier predictions and even completely confuse ML models.', 'Consider the following demonstration of adversarial examples: starting with an image of a Stop Sign, the attacker now typically adds a small perturbation with the goal of forcing the model to make a false prediction as a Yield sign, which the model does as calculated. Both images appear identical to us, but because the model is based on numbers, adding such a perturbation has a significant impact on the pixel values, resulting in a false prediction.', '(Source)', 'Now we will discuss commonly known or identified Strategies of the attacks.', 'The most common type of attack is an evasion attack. Spammers and hackers, for example, frequently try to avoid detection by obscuring the content of spam emails and malware. Samples are tampered with in order to avoid detection and be classified as legitimate. This does not imply that you have any control over the training data. Image-based spam, in which the spam content is embedded within an attached image to avoid textual analysis by anti-spam filters, is a good example of evasion. Spoofing attacks against biometric verification systems are another example of evasion.', 'Poisoning is the process of contaminating training data in an adversarial way. Data collected during operations can be used to retrain machine learning systems. Intrusion detection systems (IDSs), for example, are frequently retrained using such data. An attacker could contaminate this data by injecting malicious samples into the system during operation, causing retraining to fail.', 'Model stealing (also known as model extraction) is the act of an adversary probing a black box machine learning system in order to reconstruct or extract the data used to train the model. This can be challenging if the training data or model itself is sensitive and proprietary. Model stealing, for example, might be used to extract a proprietary stock trading model, which the adversary could then employ for financial gain.', 'Inference attacks take advantage of overgeneralization on training data, which is a prevalent flaw in supervised machine learning models, to identify data utilized during model training. Assailants can do this even if they don’t know or have access to the parameters of a target model, presenting security risks for models trained on sensitive data.', 'Foolbox is a new Python module for creating adversarial perturbations as well as quantifying and comparing the robustness of machine learning models. Foolbox communicates with the most prominent deep learning frameworks, including PyTorch, Keras, TensorFlow, Theano, and MXNet, and supports a variety of adversarial criteria, including targeted misclassification and top-k misclassification, as well as various distance metrics. Let us now briefly look at FoolBox’s structure.', 'Five elements are required to create adversarial examples, and these elements result in the five pillars of FoolBox: first, a model that takes input such as an image and makes a prediction e.g. class probabilities. Second, a criterion for determining what constitutes an adversarial e.g. misclassification.', 'Finally, a distance measure is used to determine the size of a perturbation e.g. L1-norm. Finally, an attack algorithm generates an adversarial perturbation using input and its label, as well as the model, the adversarial criterion, and the distance measure.', 'In this section, we will look at some use cases of this toolbox. As we discussed earlier this framework supports mostly all widely used deep learning frameworks, to work with your desired framework make sure you have installed it, and then by simply using pip install the FoolBox.', 'In this example, we’ll use TensorFlow. First, we’ll need to create our transfer learning model (tf.keras.applications, in this case, I’m using the ResNet50), which we’ll then pass to Foolbox’s TensorFlowModel class. A similar class is available for other frameworks. We should also specify the preprocessing expected by the respective model, such as flipping an axis, converting from RGB to BGR, subtracting mean, and dividing by std, as well as the bounds of the input space, which should only contain values expected by the model.\xa0\xa0\xa0', 'Now we have initiated the ResNet and FoolBox model too, before proceeding to the attack formulation we need to have some sample of image data that can be obtained directly using Foolbox which comes with helper functions under the utils package that provides a small set of sample images from different computer vision datasets.\xa0', ""images, labels = fb.utils.samples(fmodel, dataset='imagenet', batchsize=16)"", 'To launch an attack, we must first create an instance of the corresponding class. Foolbox employs a wide range of adversarial attacks. Each attack begins with a model for locating adversaries and a criterion for defining what an adversary is. Misclassification is the default criterion.', 'It can then be applied to a reference input and the corresponding label to which the adversarial should be close. Internal hyperparameter tuning is used by attacks to find the least amount of perturbation.', 'For example, while implementing the famous fast gradient sign method (FGSM), it looks for the smallest step-size that converts the input to an adversarial. As a result, manual hyperparameters tuning for attacks like FGSM are no longer required.', 'As shown below we can choose the type of Attack and under which we can feed the Tensorflow model. Additionally, epsilons are defined which are nothing but the level of perturbation that we want to test.\xa0', 'Now we can simply check the robust accuracy by averaging out the is_adv and will also it w.r.to epsilons.\xa0', 'As we can see from the above plot when there is 0 perturbation the accuracy of the model is at its top as the toolbox starts testing epsilon values, and accuracy tends to decrease so fast even for a very small change in perturbation. From this, we can say that neural network-based models are highly prone to such attacks.\xa0', 'Through this article, we have discussed adversarial attacks in neural networks and possible strategies of attacks that may be considered. In contrast, to ensure the robustness of the model we discussed a framework called FoolBox which aims to test our model by some predefined attacks, and ultimately we can check how good our model is.\xa0\xa0\xa0\xa0']","'import foolbox as fb\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Tensorflow based\nmodel = tf.keras.applications.ResNet50V2(weights=""imagenet"")\npreprocessing = dict()\nbounds = (-1
 1)\nfmodel = fb.TensorFlowModel(model, bounds=bounds, preprocessing=preprocessing)\n', ""mages, labels = fb.utils.samples(fmodel, dataset='imagenet', batchsize=16)"", '# Intialize the attack class\nepsilons = np.linspace(0.0, 0.005, num=20)\nattack = fb.attacks.LinfDeepFoolAttack()\nraw, clipped, is_adv = attack(fmodel, images, labels, epsilons=epsilons)\n', ""# accuracy when model is attacked\nrobust_accuracy = 1 - np.float32(is_adv).mean(axis=-1)\n# visualizing the result\nplt.plot(epsilons, robust_accuracy)\nplt.title('Perturbation Vs Accuracy of the Model')"""
99,for_distributed_hyperparameter_optimisation/,https://analyticsindiamag.com/how-to-use-hyperopt-for-distributed-hyperparameter-optimisation/,"['In machine learning, finding the best-fit models and hyperparameters for the model to fit on data is a crucial task in the whole modelling procedure. Various hyperparameter optimizers are available for this task, like BayesianOptimisation, GPyOpt, Hyperopt, and many more. In this article, we are going to discuss the Hyperopt optimization package in Python. This package can be used for hyperparameter optimization using the Bayesian optimization technique. The major points that we will discuss here are listed below.', 'Table of Contents', 'Let’s start with understanding what hyperopt is.', 'What is Hyperopt?', 'Hyperopt is a tool for hyperparameter optimization. It helps in finding the best value over a set of possible arguments to a function that can be a scalar-valued stochastic function. One of the major differences between other optimizers and this tool is that other optimizers assume that input vectors are drawn from a space vector where using hyperopt we can describe our search space in a more explainable way. It helps us in describing more information about the space where the function is defined and the space where we think the best values are presented. We can search more efficiently by allowing algorithms in hyperopt.\xa0', 'We can use the various packages under the hyperopt library for different purposes. The list of the packages are as follows:', 'In this article, we will discuss how we can perform hyperparameter optimization using it. Let’s start by discussing different calling conventions that help in defining the communication between hyperopt, search space, and an objective function.\xa0', 'Using the following lines of codes, we can install the hyperopt.', '!pip install hyperopt', 'Since I am using Google Colab in this article, it already has the facility of hyperopt for hyperparameter optimization. Let’s start with a simple implementation of it.', 'Simple Implementation of Hyperopt', 'Using the following lines of codes, we can define a search space.', 'Using the above code snippet, we have defined a search space bounded between -10 to 10.', 'As we have seen above, we have defined a space where the it’s optimization algorithm can search for an optimal value so that any objective function can receive a valid point. Let’s see in the simplest way how we can perform it.', 'Here using the above codes, we can see how the codes are easy to write using the it where we just need to have a function and iteration value. In the output, we can see that it came with floating-point loss.\xa0', 'The above example is the simplest example of finding an optimal value for our objective function. We can use various trial objects provided by hyperopt to make the process more explainable. There is always a need to save more statistics and diagnostic information in a nested dictionary. We can pass some more keys with the fmin function. Two important key values are:', 'There are also many optional keys that can be used like:', 'How to use these key values and trial objects in our codes can be found here. Here we can find how we can save and represent information and diagnosis using the trial object shown. To make the size of the article compact we are not discussing this here.', 'Since our main motive here is to perform hyperparameter optimization using the this tool, in the next section we will see an approach to perform this. Before performing this, we are required to know about the parameter expressions for defining space which can be used with hyperopt optimization algorithms. Some of these expressions are listed below.', 'In the above list, we have seen important expressions for making a search space for our objective function. Now we can move towards an implementation of a simple modelling procedure where we will perform hyperparameter optimization using the hyperopt-sklearn.', 'Note: hyperopt-sklearn is a model based on the hyperopt tool where we can perform model selection using the machine learning algorithms of scikit-learn.', 'Model Selection using Hyperopt', 'In this article, we are using the hyperopt-sklearn for performing classification model selection on the iris dataset. This dataset can be found in the sklearn library so we will be importing it from there. Let’s start by installing and importing some necessary libraries. We only need to install hyperopt-sklearn, which can be done using the following codes.', 'pip install git+https://github.com/hyperopt/hyperopt-sklearn', 'Now we are ready to use the library. We also required sklearn, NumPy, and Pandas library for this implementation.', 'Importing libraries', 'Importing the data', 'Splitting the data', 'Defining estimator using the hyperopt estimator', 'Performing models selection using estimator on a subset of data where different models have been tried to perform on the data as,', 'training the best model on whole data\xa0', 'estimator.retrain_best_model_on_full_data(X_train, y_train)', 'Now we can see the results of the model selection process as,', 'Here in the output, we can see the results. We have a range of best features, the best model with parameters and accuracy of the model.\xa0', 'Final Words\xa0', 'Here in the article, we have introduced the hyperopt tool for hyperparameter optimization. Along with that, we discussed some of the features from this tool and we have successfully implemented an example for model selection using the hyperopt-sklearn tool that is provided by hyperopt for the models of the SK-Learn library.']","'!pip install hyperopt'
 ""from hyperopt import hp\nspace = hp.uniform('x', -10, 10)"", 'from hyperopt import fmin, tpe\nfn=lambda x: x ** 2\nalgo=tpe.suggest\nmax_evals=100\nbest = fmin(fn, space, algo, max_evals)\nprint(best)', 'pip install git+https://github.com/hyperopt/hyperopt-sklearn', 'import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\nimport hyperopt.tpe\nimport hpsklearn\nimport hpsklearn.demo_support', ""iris = load_iris()\ndf = pd.DataFrame(iris.data, columns=iris.feature_names)\ndf['species_name'] = pd.Categorical.from_codes(iris.target, iris.target_names)\ndf\n"", ""y = df['species_name']\nX = df.drop(['species_name'], axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"", ""estimator = hpsklearn.HyperoptEstimator(\n    preprocessing=hpsklearn.components.any_preprocessing('pp'),\n    classifier=hpsklearn.components.any_classifier('clf'),\n    algo=hyperopt.tpe.suggest,\n    trial_timeout=15.0, # seconds\n    max_evals=20,\n    )\n"", '# Demo version of estimator.fit()\nfit_iterator = estimator.fit_iter(X_train,y_train)\nfit_iterator.__next__()\nplot_helper = hpsklearn.demo_support.PlotHelper(estimator,\n                                                mintodate_ylim=(-.01, .10))\nwhile len(estimator.trials.trials) < estimator.max_evals:\n    fit_iterator.send(1) # -- try one more model\n    plot_helper.post_iter()\nplot_helper.post_loop()\n', 'estimator.retrain_best_model_on_full_data(X_train, y_train)', ""print('Best preprocessing pipeline:')\nfor pp in estimator._best_preprocs:\n    print(pp)\nprint('\\n')\nprint('Best classifier:\\n', estimator._best_learner)\ntest_predictions = estimator.predict(X_test)\nacc_in_percent = 100 * np.mean(test_predictions == y_test)\nprint('\\n')\nprint('Prediction accuracy in generalization is %.1f%%' % acc_in_percent)"""
100,additive_explanations_for_practitioners/,https://analyticsindiamag.com/a-complete-guide-to-shap-shapley-additive-explanations-for-practitioners/,"['There are many machine learning models which are very accurate and high performing while making predictions. One of the limitations with these models we always find is that we can not explain the quality of outcomes produced by them. There is always a need to make the outcomes from the model more explainable. In this article, we are going to introduce a tool “SHAP (SHAPley Additive exPlanations)” that can help us in making the outcomes of the machine learning models more explainable. The major points to be discussed in this article are listed below.', 'The most comprehensive Repository of Python Libraries for Data Science >>', 'Table of Contents', 'What is SHAP?', 'SHAP or SHAPley Additive exPlanations is a visualization tool that can be used for making a machine learning model more explainable by visualizing its output. It can be used for explaining the prediction of any model by computing the contribution of each feature to the prediction. It is a combination of various tools like lime, SHAPely sampling values, DeepLift, QII, and many more.\xa0', 'One of the main components of the SHAP tool is SHAPley values because using it, SHAP connects optimal credit allocation with local explanations. When we talk about the SHAPley values we can consider them as a method that can tell how to accurately distribute the contribution by the features, among the features.\xa0', 'One of the good things about the SHAP is, it supports modelling procedures followed by using libraries like SciKit-Learn, PySpark, TensorFlow, Keras, PyTorch, and many more. These are the widely used libraries for data modelling and the basic problem with these libraries is that model outcomes are not so explainable. Using SHAP, we can make outcomes more understandable for users who are not so knowledgeable about the outcomes of machine learning models. With this ability of SHAP, we can also use it for data visualization. Let’s start with the installation process of the SHAP tool in our environment.', 'Installing SHAP', 'We can install the SHAP tool by using the following pip command:', '!pip install SHAP', 'So as we have installed the SHAP tool, now we can start by making models with simple data.', 'Simple Implementation of SHAP', 'As we have discussed, that we can utilize the SHAP tool with many modelling libraries, in this section, we will look at how simply we can use this tool to make the outcomes from simple models more explainable.\xa0', 'Let’s start by loading data. With the SHAP tool installation, we also get some ready datasets with this package which we will use here. In this article, we are going to use the IRIS dataset for classification.', 'Loading the data', 'Splitting the data', 'Checking the data', 'For classification, we are using the SVM model from the SK-Learn library.', 'Importing and fitting model', 'Here we have got an accuracy of 100%. Now we can use the SHAP tool for explaining the prediction in the test set using visualization.', 'Explaining the prediction using an explainer', 'Plotting the prediction\xa0', 'We can move the cursor to see the values in the output. Here I am just posting the picture of the output. Here we have used the force plot to plot outcomes from the model. By visualizing the force plot we can understand the impact of every feature on the prediction by the model even for a specific instance of the data.\xa0', 'We can say that the force plot is an explanation of feature importance based on the game theory method, i.e., SHAPley values. The Force plot shows the influence of each feature on the current prediction. Values in the blue colour can be considered as the values that have a positive influence on the prediction whereas values in the red colour have a negative influence on the prediction.', 'Here in the above example, we have seen a general idea of applying the SHAP tool to the models. Let’s have a look at the explanation of the SHAPe values which we have created in the modelling in the last section.', 'Explaining Models With SHAPely Values', 'In this section of the article, we will see how we can make a machine learning model more explainable using the SHAPley values. For this purpose, we will use a simple linear regression model on the IRIS data set which we have already used in the last section of the article.', 'Let’s start with fighting the model on the previously loaded data.', 'Examining the Model Coefficients', 'One of the most common procedures of explaining linear model success is to find out the level of coefficient learned by the model for each feature. Since the SHAPley values, consider that every value is important from the data for the output. By examining the coefficient we can tell how much the output can change if we change the feature.', 'Here, what we have done is a traditional method to examine the model. We can say that the petal width feature from the dataset is the most influencing feature. Since we have the SHAP tool we can make a clearer picture using the partial dependence plot.', 'Partial Dependence Plots', 'The importance of the feature can be found by knowing the impact of the feature on the output or by knowing the distribution of the feature. So if we can plot the model and the distribution in a single plot, it would become more beneficial and informative for us. Let’s see how we can do this.', 'Here on the X-axis, we can see the histogram of the distribution of the data, and the blue line in the plot is the average value of the model output which passes through a centre point which is also the intersection point of the expected value lines.\xa0', 'Using this plot we can read the SHAP value which can be considered as the SHAPley values that are applied to any conditional expectation function of a model.', 'For example, we can extract a few values from the data and use them as a sample for background distribution. Let’s say we have extracted 50 instances. Using which we can make the SHAP values.', 'Computing\xa0 the SHAP values', 'Partial dependence plot', 'Here we can see that a close correspondence between the partial dependence plot and SHAP value. It means that we have plotted a mean-centred version of the partial dependence plot for that feature.', 'Let’s check the distribution of the SHAP value.', 'SHAP.plots.scatter(SHAP_values[:,""petal length (cm)""])', 'This is a clearer outcome where we can see that the SHAP values distribution is similar to the distribution of the portal length distribution.', 'Waterfall Plot', 'These SHAP values of all input features will always be summed up to the difference between the expected output from the model and that is how the output from the current model for the prediction becomes explained. We can see it through the waterfall plot.', 'SHAP.plots.waterfall(SHAP_values[sample_ind])', 'By seeing in the waterfall plot, we can imagine how we get the predicted values with SHAP.\xa0', 'Final Words', 'In this article, we have seen what is SHAP tool, and how we can simply apply this to our models to make the outcome from the model more explainable. Along with this, we have also seen how we can use the SHAP values to improve the explainability of any model.\xa0']","'!pip install SHAP'
 'import SHAP\nX,y = SHAP.datasets.iris(display=True)', 'from sklearn.model_selection import train_test_split\nX_train,X_test = train_test_split(X,test_size=0.2, random_state=0)\nY_train,Y_test = train_test_split(y, test_size=0.2, random_state=0)', 'from google.colab import data_table\ndata_table.enable_dataframe_formatter()\nX_train\n', ""from sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score\nsvc = SVC(kernel='linear', probability=True)\nsvc.fit(X_train, Y_train)\ny_pred = svc.predict(X_test)\naccuracy_score(Y_test, y_pred)"", 'explainer = SHAP.KernelExplainer(svc.predict_proba, X_train)\nSHAP_values = explainer.SHAP_values(X_test)\n', 'SHAP.initjs()\nSHAP.force_plot(explainer.expected_value[0], SHAP_values[0], X_test)', 'model = sklearn.linear_model.LinearRegression()\nmodel.fit(X, y)\n', 'SHAP.plots.partial_dependence(\n    ""petal length (cm)"", model.predict, X50, ice=False,\n    model_expected_value=True, feature_expected_value=True\n)\n', 'X50 = SHAP.utils.sample(X, 50)\nexplainer = SHAP.Explainer(model.predict, X50)\nSHAP_values = explainer(X)\n', 'sample_ind = 18\nSHAP.partial_dependence_plot(\n    ""petal length (cm)"", model.predict, X50, model_expected_value=True,\n    feature_expected_value=True, ice=False,\n    SHAP_values=SHAP_values[sample_ind:sample_ind+1,:]\n', 'SHAP.plots.scatter(SHAP_values[:,""petal length (cm)""])', 'SHAP.plots.waterfall(SHAP_values[sample_ind])'"
101,for_deep_learning_models/,https://analyticsindiamag.com/hands-on-guide-to-cockpit-a-debugging-tool-for-deep-learning-models/,"['It is important to understand the limitations that prevent machine learning adoption in many industries. Machine learning models are excellent at certain jobs, but they can also make a lot of mistakes. Understanding how your model can fail and preparing adequate answers ahead of time is the key to a successful project. There are numerous frameworks available for understanding model behaviour, specifically for deep learning, such as TensorBoard, Weights and Biases, and so on. In this article, we will have a look at Cockpit, a framework that provides us with practically all of the information we need about our model. We will go through the following table of contents to understand this debugging framework.\xa0', 'Table of Contents', 'Let’s start the discussion by understanding why debugging is so important when it comes to building a robust application.', 'Debuggers are essential in the development of traditional software. When things go wrong, they provide you access to the code’s inner workings, allowing you to see “inside the box.” This is far more efficient than rerunning the program with new inputs. Deep learning, on the other hand, is arguably closer to the latter.\xa0', 'If a deep net training attempt fails, we must decide whether to alter the training hyperparameters (how?), the optimizer (to which?), the model (how?), or just re-run with a different seed. Machine learning toolboxes don’t offer much in the way of guidance for these decisions.', 'Debugging Tools in Deep Learning', 'Deep learning can be debugged with standard debuggers. They’ll offer us access to each and every weight in a neural network, as well as the individual pixels in the training data. However, this rarely offers useful information for effective training. To extract useful data, you’ll need to use a statistical method and condense the confusing complexity into a simple summary.\xa0', 'TensorBoard and Weights & Biases were created in part to make this visualization easier. However, because they do not show the network’s internal state, the extensively observed quantities (primarily train/test loss and accuracy) provide only a rudimentary explanation for relative variances between several training cycles.', 'These debuggers, as you may have seen, provide learning curves that characterize the model’s current state – whether it is performing well or not – but no information regarding training state and dynamics. They tell the user if things are going well or not, but not why. It’s like piloting a plane with your eyes closed and no Cockpit feedback. As a result, it should come as no surprise that achieving cutting-edge deep learning performance necessitates specialist skill or plain trial and error.', 'Here comes Cockpit, which adds a visual and statistical debugging tool to the deep learning pipeline that employs both newly proposed and established observables. It uses and augments current modifications to automatic differentiation (i.e. BackPack for PyTorch) to effectively access second-order statistical (e.g. gradient variances) and geometric (e.g. Hessian) information, according to the official publication.', 'In their work, they explain how these numbers might help deep learning engineers with tasks like learning rate selection and finding frequent problems in data processing or model architectures. Practically, we’ll see all of these. It’s open-source, expandable code that seamlessly interfaces with current PyTorch training loops.', 'In this section, we will see practically how we can access the various internal parameters of a particular model and will discuss the meaning of each.\xa0', 'Below we are implementing the example taken from the official documentation of Cockpit. To continue with this example, you need a lactate _utils_exmple.py file from the repository to your working directory in order to do successful data imports. Before going further you need to install a Cockpit and can be done using a simple pip command as ! pip install Cockpit-for-pytorch', 'In addition to PyTorch, we import BackPack, which will be installed automatically when Cockpit is installed. We also include the Cockpit and CockpitPlotter classes, which will allow us to track and visualize useful data.', 'In the next lines of code, we import from a utils file that contains the Fashion-MNIST data.', 'Then, for our Fashion-MNIST data set, we create a basic classifier. The main difference from a standard training loop is that we must use BackPack to extend both the model and the loss function. It’s as simple as wrapping the standard model and loss function with BackPack extend() method. It informs BackPack that extra values (such as individual gradients) for these parameters should be computed.', 'We also need access to the individual loss values for the Alpha (Will discuss shortly) quantity, which can be computed inexpensively but isn’t generally part of a traditional training loop. By setting the reduction=None, we may generate this function in the same way as the standard loss function. There is no need to inform BackPack of its existence because the losses will be the same.', 'The Cockpit class is in charge of computing the quantities and storing the results. We must provide model parameters as well as a list of values indicating what should be tracked and when. Cockpit has three alternative computational complexity configurations: “economy,” “business,” and “full” (see also configuration()). To keep track of all possible quantities, we’ll use the utility function given.', 'Now, let’s move to the training loop. The training itself is simple. We draw a mini-batch at each iteration, compute the model predictions and losses, then conduct a backward pass and update the parameters. The primary difference between Cockpit and backward calls is that the backward call is wrapped by a Cockpit(…) context, which manages the extra computations during the backward pass. The info parameter is used to pass additional information required by specific amounts.', 'The computed metrics may be viewed at any point during the training, which we do in every iteration, by accessing the plotting capabilities of the CockpitPlotter via plot (). The whole Cockpit view is shown here after the final iteration.', 'Let us interpret each of these plots in detail.', 'It has built a noise-informed univariate quadratic approximation in the step direction (i.e. the loss as a function of the step size) and assessed to which point on this parabola our optimizer advances using individual loss and gradient observations at the start and conclusion of each iteration.\xa0', 'This value has been standardized by Inventor so that stepping to the valley floor is assigned a value of ???? = 0, the beginning point is allocated a value of ???? = 1, and updates to the point precisely opposite the starting point are assigned a value of ???? = 1.', 'The orange trajectory is stationary, as evidenced by the update size. But why is that? Slowing down can be caused by both a slow learning rate and loss of landscape plateaus. These two causes are distinguished by the gradient norm.', 'Individual gradients disperse around the mean using a standardized radius and two bandwidths (parallel and orthogonal to the gradient mean) in the norm, inner product, and orthogonality tests.\xa0', 'These settings are used in the original works to adjust batch sizes. Instead, Cockpit visualizes the standardized noise radius and bandwidths by combining all three tests into a single gauge (top centre plot).\xa0', 'These noise signals can be utilized to direct batch size adaptation both on and off the computer, as well as to investigate the impact of gradient alignment on training speed and generalization.', 'The largest Hessian eigenvalue defines the optimum step size in convex optimization. The cockpit takes use of this to calculate the Hessian’s biggest eigenvalue and trace (top and centre plots). The former resembles the sharpest valley on the loss surface and hence may indicate training instabilities. The graph depicts the concept of “average curvature.”', 'The Takeuchi Information Criterion (TIC) uses a ratio between Hessian and non-central second gradient moments to determine the generalization gap. It also gives insight into the changes in the goal function that gradient noise implies. Cockpit delivers TIC estimates in small batches.', 'The gradient elements are represented via a univariate histogram in Cockpit. A combined histogram of parameter-gradient pairs is also included. The mini-batch approach provides a two-dimensional glimpse into the network’s gradient and parameter values.']",'! pip install Cockpit-for-pytorch'
102,learning_models_using_visualkeras/,https://analyticsindiamag.com/how-to-visualize-deep-learning-models-using-visualkeras/,"['Building a machine learning or deep learning model for accurate predictions is important but adding interpretability along with these models can make the process more useful and interesting. There are a few methods to visualize the defined predictive models but visualizing a deep learning model with its complex structure is a challenge. In this article, we will learn to visualize the deep learning models in order to achieve interpretability. We will go through different steps to see how to customize these visualizations of the deep learning models to make them more understandable. The major points to be covered in this article are listed below.\xa0', 'Table of Contents\xa0', 'Need of Interpretability', 'Most of the machine learning models are considered black-box models, especially the neural networks. It is not easy to understand how a defined model is functioning with the data. The approach to making the models understandable and interpretable by everyone is very important. Considering this scope, interpretability or explainability is trending nowadays. In the context of deep learning, there are different approaches used for explainable deep learning. Here we are going to discuss how can we visualize a defined deep learning model so that any person can understand that a model which is giving so accurate results, how does it look.', 'About VisualKeras', 'For the visualization of deep learning models, we will use a python package named visualkeras. Let us understand about this package before going forward. Visualkeras is a python package with the facility of visualization of deep learning models architecture build using the Keras API also we can visualize the networks built using Keras included in TensorFlow. This library supports the layered and graph style architecture of neural networks.\xa0', 'Visualization of Deep Learning Models', 'In this section, we will see how we can define and visualize deep learning models using visualkeras. Let us go through the elbow steps.', '1. Installing Dependency', 'Let’s start with the installation of the library.', 'Using the following code we can install the visualkeras package.', '2. Defining the Model', 'As a next step, we are making a simple model for this we are required to import some libraries.', 'For a small example, we can make a sequential model with convolutional layer and pooling layers.', 'Let’s check for the summary of the defined model.', 'model.summary()', 'Here we can see that the model we have defined a model with 3 convolutional layers and 2 pooling layers in the network. Now the question which comes to mind is how we can visualize it?', '3. Visualizing the Model', 'Let’s import the visualkeras package', 'import visualkeras', 'Now we are ready to visualize the defined network. This can be done by simply using the following code.', 'visualkeras.layered_view(model)\xa0', 'Here we can see that convolutional layers are in yellow and pooling layers are in pink colour as we have seen in the summary there are three convolutional and 2 pooling layers.', '4. Adding Dense Layer', 'Let’s add a dense layer to the network.', ""model.add(layers.Dense(64, activation='relu'))"", 'Visualizing the model added with a dense layer\xa0', 'Here we can see that now we have different colours for dense layers.\xa0', '5. Labelling the Layers', 'Since we are talking about interpretability it will be more interpretable visualization if the name of the layers is assigned with layers themselves. Using this package we can also assign the name of the layer in the visualization.\xa0', 'For this, we need to import the image font for the PIL library.', 'Image font can be used in with the visualkeras', '6. Adding Flatten Layer', 'Adding a flatten layer in the model.', '7. Visualising in 2-D Space', 'We can also visualize the network in 2D space or we can say in flat style using the following codes.', '8. Customizing the Space between Layers', 'We can manage to space between the layers.', 'visualkeras.layered_view(model, legend=True, font=font, draw_volume=False,spacing=50)\xa0', '9. Customizing the Layers Colours', 'We can also customize the colours of the layers.', 'visualkeras.layered_view(model, legend=True, font=font,color_map=color_map)\xa0', 'Here we can see how we can visualize a deep learning model built using Keras. The layers I have used in the model are some of the most used layers in the field of modelling neural networks.', 'Final Words', 'In the article, we learnt how to visualize a deep learning model using a python package named visualkeras. We saw how to plot the models with so many customizations to make them understandable and interpretable. This visualization is not much difficult and can be done very quickly. Adding such visualizations in the deep learning-based reports can make the report more attractive and interpretable.\xa0']","'pip install visualkeras'
 'from tensorflow import keras\nfrom tensorflow.keras import layers, models', ""model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))"", 'model.summary()', 'import visualkeras', 'visualkeras.layered_view(model)\xa0', ""model.add(layers.Dense(64, activation='relu'))"", 'from PIL import ImageFont', 'from PIL import ImageFont\nvisualkeras.layered_view(model, legend=True, font=font)\xa0', 'model.add(layers.Flatten())\nvisualkeras.layered_view(model, legend=True, font=font)\xa0', '\n\nvisualkeras.layered_view(model, legend=True, font=font, draw_volume=False)\xa0\n\n', 'visualkeras.layered_view(model, legend=True, font=font, draw_volume=False)\xa0', 'visualkeras.layered_view(model, legend=True, font=font, draw_volume=False,spacing=50)\xa0', ""from collections import defaultdict\n\ncolor_map = defaultdict(dict)\ncolor_map[layers.Conv2D]['fill'] = 'orange'\ncolor_map[layers.MaxPooling2D]['fill'] = 'red'\ncolor_map[layers.Dense]['fill'] = 'black'\ncolor_map[layers.Flatten]['fill'] = 'teal'"""
103,learning_models_using_eli5/,https://analyticsindiamag.com/how-to-visualize-and-debug-machine-learning-models-using-eli5/,"['Machine learning models are generally considered black-box models in the entire community despite their massive implementation. It becomes very essential to Understand how particular predictions are being made and how models focus on various aspects of parameters it has learned. Models are usually assessed using certain evaluation matrices on a given test dataset. Real-world data, on the other hand, is frequently different, so the evaluation metric may not accurately reflect the product’s purpose.\xa0', 'In addition to such metrics, evaluating individual predictions and their justifications is a viable solution for optimizing performance. In this article, we will discuss debugging and visualizing machine learning algorithms using ELI5. ELI5 is a tool in Python that is used to visualize and debug various Machine Learning models using a unified API. The major points to be covered in this article are given below.', 'Now, let us start with understanding explainability and interpretability.\xa0', 'Explainability and interpretability are frequently used in machine learning and artificial intelligence. Even though they are extremely similar, it’s worth exploring the differences, if only to demonstrate how difficult things can get once you start looking into machine learning systems. The amount to which a cause and effect may be observed within a system is known as interpretability. To put it another way, it refers to your ability to forecast what will happen in response to a change in input or computational parameters. It’s the ability to look at an algorithm and pertaining, what’s going on there.', 'Meanwhile, explainability refers to how well the internal mechanics of a machine or deep learning system can be communicated in human terms. It’s easy to overlook the tiny distinction with interpretability, but think of it this way: interpretability is about being able to understand mechanics without necessarily knowing why. Explainability refers to the ability to explain what is happening in detail. Simple models (Like\xa0 Linear or Logistic regression) can be used to explain findings for a sample data set. Typically, these models are insufficient, and we must go to Deep Learning models, which deliver great performance but are a mystery to the majority of Data Science practitioners. Machine learning models are currently utilized to make a variety of essential judgments, including fraud detection, credit rating, self-driving, and patient examination.', 'It becomes very important to every practitioner that enhancing the interpretability and explainability of models is now crucial in most development and that can make us stand differently than others. We can address the issues and goals of the problem statement correctly by understanding how algorithms work.\xa0\xa0', 'ELI5 is a Python toolkit that uses a uniform API to visualize and debug diverse Machine Learning models. It supports all scikit-learn algorithms (including the fit() and predict() methods). It includes built-in support for numerous ML frameworks and allows you to explain white-box models (Linear Regression, Decision Trees) as well as black-box models (Keras, XGBoost, LightGBM). It is applicable to both regression and classification models.', 'Now we are going to see how ELI5 interprets and explains a model using its eli5.show_weights and eli5.show_prediction API. The practical demo is divided into two parts. First, we are going to interpret and explain XGBoost. Following it we will see the same for Keras application.', 'The data set here we are using is the sklearn built-in data set for breast cancer prediction while implementing, create a pandas data frame for breast cancer dataset with the proper header this is because when we execute eli5 it retrieves feature information from the model.\xa0\xa0\xa0', 'Build a classifier:', 'Now we need to use just two simple functional API’s of ELI5 as below.', 'The left side shows weights assigned for each feature and the right side shows the prediction for one instance', 'As you can see from the above two tables how XGBoost assigned weights for each feature based on training data and from the other table, for a particular instance, to reach a probability of 0.981 for class 1 how each feature has contributed.\xa0\xa0\xa0', 'Similarly, next, we are going to see the same interpretation for Keras’s application.', 'If we have a model that takes an image as input and returns class scores (probabilities that a specific object is present in the image), we can use ELI5 to see what was in the image that caused the model to predict a specific class score.', 'For the Keras demo, we are using a VGG16 pre-trained network and its interpretation for a random image.\xa0\xa0', 'from tensorflow.keras.applications import VGG16\xa0', 'As you can see ELI5 shows how the VGG16 looks for objects for which a given image is to be classified.\xa0\xa0', 'ELI5, can use an existing function and produce good results that are formatted as well, It also allows code to be reused across different machine learning frameworks, It can deal with a slew of minor inconsistencies.', 'ELI5 can be used to inspect basic model parameters and to figure out how the models perform on a global scale. ELI5 can be used to examine specific predictions provided by a single model, as well as the decisions made by the models.']","'! pip install eli5\nfrom xgboost import XGBClassifier\nimport eli5\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np'
 ""data = load_breast_cancer()\ndf = pd.DataFrame(data.data)\ndf.columns = data.feature_names\ndf['target'] = data.target"", 'model = XGBClassifier()\nmodel.fit(x_train,y_train)\n', 'eli5.show_weights(model, top=30)\neli5.explain_prediction_xgboost(model,x_test.iloc[0])', ""from tensorflow.keras.applications import VGG16 \nimport keras\n\nvgg16 = VGG16(include_top=True, weights='imagenet', classes=1000)\n \n# load image\nim = keras.preprocessing.image.load_img('/content/HAL-TEDBF-Fighter-Jet-With-Vikrant-Aircraft-Carrier-Art.jpg', target_size=(224, 224))\ndoc = keras.preprocessing.image.img_to_array(im)\ndoc = np.expand_dims(doc, axis=0)\ndoc = keras.applications.vgg16.preprocess_input(doc[0])\n# visualize the image\nkeras.preprocessing.image.array_to_img(doc[0])"", '# explain\neli5.show_prediction(vgg16, doc)\n'"
104,for_black_box_optimization/,https://analyticsindiamag.com/hands-on-guide-to-openbox-toolkit-for-black-box-optimization/,"['A black box is an electronic device placed in an aircraft to facilitate the investigation of aircraft accidents and incidents. Sometimes it is the last device that remains alive in crucial situations. These devices are placed in a hidden place and are very rare to be destroyed. Similarly, in computer programming, any algorithm consists of various functions that can not be accessed, but we can observe that its output depends on the given inputs; these types of functions are called black-box functions. Optimizing these types of functions is called black-box optimization (BBO). For example, tuning of a large neural network is considered as an application of black-box optimization.', 'The basic goal of BBO is to find a configuration for those black-box functions so that configuration can improve the performance of the whole algorithm by implying them as rapidly as possible.', 'There are some basic applications of traditional BBO are:', 'As the generalized BBO has emerged, the application of BBO has increased and has been applied to areas like:', 'Generalized BBO requires more functionalities like multiple objectives and constraints, which a single objective or traditional BBO may not support. There are various open-source BBO systems available. some of the systems are:', 'OpenBox is also an open-source BBO tool, which can help us in BBO for different tasks.', 'OpenBox', 'OpenBox is a standalone python package as an open source for efficient generalized Block-Box optimization. It is designed to satisfy the following demands in the field of BBO and generalized BBO :', 'With these all, it gives visualization functions for tracking and managing BBO tasks, allowing users to choose the SOTA algorithms after optimizing them, \xa0effective use of system optimization with transfer-learning and multi-fidelities, etc.', 'The following image can show the architecture of Open-Box.', 'Image source', 'There are a few main components of the Open-Box:', '\xa0Where service master helps in the management of nodes and load balance, task database stores the history of all tasks, suggestion server generates the configuration, and these configurations are connected by the worker using rest API.', 'Next in the article, we are going to tune a LightGBM model using OpenBox.', 'Installing the OpenBox', 'After completion of installation, we can start using the system.', 'Importing some basic libraries and OpenBox functions.', 'Preparing the data', 'Here we have called a data set provided by SK-Learn. In the data, we have 10 classes, and the features are an integer between 0-16. To know more about the data set readers can visit this link.', 'Splitting the data into train and test:', 'Defining a configuration space using OpenBox package provided get_config_space function.', 'Or we can define a function for configuration space using following code:', 'Defining the objective function using get_objective_function provided by the package of OpenBox.', ""objective_function = get_objective_function('lightgbm', x_train, x_val, y_train, y_val)"", 'Or if you want to edit the function you can define the function using following code:', 'The input of the objective function is a Configuration instance sampled from the space.', 'After defining those functions, we can define an optimization instance to run the optimization process.', 'This instance will take 00 rounds or optimize the objective function 100 times.', 'Running the optimizing function:', 'Here I have saved all the information of the function in the history instance to extract and visualize the information about the optimization.', 'After the execution of the opt.run function, we can check for the history and also visualize the optimization:', 'print(history)', 'Here we can see the learning rates and the optimal objective value for every optimization and compare them to select the best optimization values.', 'We can also visualize the minimum number of optimization after n iteration.']","'!pip install openbox'
 'import matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_digits\nfrom openbox import get_config_space, get_objective_function\nfrom openbox import Optimizer', 'X, y = load_digits(return_X_y=True)\nprint(X,y)\nprint(X.shape,y.shape)', 'x_train, x_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)', ""config_space = get_config_space('lightgbm')"", 'def get_configspace():\n\n\xa0\xa0\xa0\xa0space = sp.Space()\n\n\xa0\xa0\xa0\xa0n_estimators = sp.Int(""n_estimators"", 100, 1000, default_value=500, q=50)\n\n\xa0\xa0\xa0\xa0num_leaves = sp.Int(""num_leaves"", 31, 2047, default_value=128)\n\xa0\xa0\xa0\xa0max_depth = sp.Constant(\'max_depth\', 15)\n\xa0\xa0\xa0\xa0learning_rate = sp.Real(""learning_rate"", 1e-3, 0.3, default_value=0.1, log=True)\n\xa0\xa0\xa0\xa0min_child_samples = sp.Int(""min_child_samples"", 5, 30, default_value=20)\n\xa0\xa0\xa0\xa0subsample = sp.Real(""subsample"", 0.7, 1, default_value=1, q=0.1)\n\n\xa0\xa0\xa0\xa0colsample_bytree = sp.Real(""colsample_bytree"", 0.7, 1, default_value=1, q=0.1)\n\xa0\xa0\xa0\xa0space.add_variables([n_estimators, num_leaves, max_depth, learning_rate, min_child_samples, subsample,colsample_bytree])\n\xa0\xa0\xa0\xa0return space', ""objective_function = get_objective_function('lightgbm', x_train, x_val, y_train, y_val)"", ""def objective_function(config: sp.Configuration):\n\n\xa0\xa0\xa0\xa0params = config.get_dictionary()\n\xa0\xa0\xa0\xa0params['n_jobs'] = 2\n\xa0\xa0\xa0\xa0params['random_state'] = 47\n\xa0\xa0\xa0\xa0model = LGBMClassifier(**params)\n\xa0\xa0\xa0\xa0model.fit(x_train, y_train)\n\xa0\xa0\xa0\xa0y_pred = model.predict(x_test)\n\xa0\xa0\xa0\xa0loss = 1 - balanced_accuracy_score(y_test, y_pred)\xa0 # minimize\n\xa0\xa0\xa0\xa0return dict(objs=(loss, ))"", ""opt = Optimizer(\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0objective_function,\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0config_space,\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0max_runs=100,\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0surrogate_type='prf',\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0time_limit_per_trial=180,\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0task_id='tuning_lightgbm',\n\xa0\xa0\xa0\xa0)"", 'history = opt.run()\xa0', 'print(history)', 'history.plot_convergence()\nplt.show()'"
105,algorithm_configuration_using_smac/,https://analyticsindiamag.com/hands-on-guide-to-algorithm-configuration-using-smac/,"['Many algorithms belong to the family of tree and ensemble, which are hard for computational problems and exposed to many hyperparameters that can be modified to improve the performance.\xa0 However, manually exploring those parameters and setting those for optimized solutions is a rigorous task and often leads to unsatisfactory results. Therefore, recently automated approaches for solving this algorithm configuration problem have led to substantial improvements in the State-of-the-art for solving various problems.\xa0\xa0', 'Sequential Model-Based Algorithm Configuration, in short, referred to as SMAC, is a versatile tool for optimizing algorithm parameters or parameters of some other automated process or a user-defined function we can evaluate, such as simulation. SMAC is to be very effective for the hyperparameter optimization of machine learning algorithms. Eventually, it scales better to high dimensions and discrete input dimensions than other techniques. It also helps to capture the important information about the model domain, such as which input variables are important.\xa0\xa0\xa0\xa0\xa0', 'Today in this article, we will see the use case of SMAC for optimizing the hyperparameter of the support vector machine. Later the same algorithm will be used to optimize the set of input data. All this can be done by leveraging the Python programming language.\xa0', 'To run SMAC, you need to install some dependencies before making sure you have installed SciPy of version 1.7.0 or greater. To install SciPy, use the below command;\xa0', '! pip install scipy==1.7.0', 'After installing SciPy, make sure you restarted the runtime; by this, the changes will take place.\xa0', 'As explained earlier, SMAC can optimize the user-defined function; in this section, we will configure the SVM, and here we will optimize the solution quality, i.e., we will optimize the accuracy. We are configuring SVM to achieve high accuracy on the iris dataset;', 'iris = load_iris()', 'First, we need to inform SMAC about the possible hyperparameters and their values to optimize; the ConfigSpace package can do this; below, we structurally add the hyperparameters that SVM supports along with default values.\xa0', 'Let us first build the configuration space which holds all the states of\xa0 parameters;', 'We define few types of SVM kernel and adding them as kernel to our configuration space;', 'There are other common parameters such as ‘C’\xa0 Regularization parameter and shrinking; the below code add these two parameters along with range;\xa0', 'Below are the kernel specific parameters such degree, coefficient, and gamma we are adding them with specified conditions;\xa0', 'For SMAC to configure the SVM, we need to add an interface that accepts the configuration as input and outputs a cost value.\xa0', 'To finalize the scenario we need to tell SMAC about the whole method and configuration space using the Scenario object. This method looks similar to the compilation of Neural Networks.\xa0\xa0\xa0\xa0', 'We can now use SMAC to optimize the hyperparameters; SMAC has different instantiations that set its hyperparameter to work well for different scenarios.\xa0', 'Now let’s check the what was default hyperparameters at the beginning of optimization and the end of the optimization;', 'print(cs.get_default_configuration())', 'print(incumbent)', 'The above optimization should result in reduced cost value; let’s validate the result;']","'! pip install scipy==1.7.0'
 '! apt-get install swig -y\n! pip install pyrfr\n# install smac\n! pip install smac\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nfrom smac.configspace import ConfigurationSpace\nfrom smac.configspace import UniformFloatHyperparameter\nfrom smac.scenario.scenario import Scenario\n# main algo used to optimize\nfrom smac.facade.smac_hpo_facade import SMAC4HPO\xa0\nfrom smac.configspace import CategoricalHyperparameter,UniformFloatHyperparameter,\\\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0UniformIntegerHyperparameter, InCondition', 'iris = load_iris()', '# build configuration space\ncs = ConfigurationSpace()', ""# kernels\nkernel = CategoricalHyperparameter('kernel',['linear','rbf','poly','sigmoid'],default_value='sigmoid')\ncs.add_hyperparameter(kernel)"", ""# common hyperparameters\nC = UniformFloatHyperparameter('C',0.001, 1000.0, default_value=1.0)\nshrinking = CategoricalHyperparameter('shrinking',['true','false'],default_value='true')\ncs.add_hyperparameters([C,shrinking])"", ""# Others\ndegree = UniformIntegerHyperparameter('degree', 1, 5, default_value=1) # used by poly\ncoef0 = UniformFloatHyperparameter('coef0', 0.0, 10.0, default_value=5.0) # poly, sigmoid\ncs.add_hyperparameters([degree, coef0])\nuse_degree = InCondition(child=degree, parent=kernel, values=['poly'])\nuse_coef0 = InCondition(child=coef0, parent=kernel, values=['poly','sigmoid'])\ncs.add_conditions([use_degree, use_coef0])\n## only for rbf, ploy, sigmoid\ngamma = CategoricalHyperparameter('gamma',['auto','value'],default_value='auto')\xa0\ngamma_value = UniformFloatHyperparameter('gamma_value', 0.0001, 8, default_value=8)\ncs.add_hyperparameters([gamma, gamma_value])\n# activate gamma_value only if gamma is set to value\ncs.add_condition(InCondition(child=gamma_value, parent=gamma, values=['value']))\n# restrict the use of gamma in general to choice of kernel\xa0\ncs.add_condition(InCondition(child=gamma, parent=kernel, values=['rbf','poly','sigmoid']))"", 'def cfg_svm(cfg):\n\xa0\xa0"""""" a SVM based configuration and evaluates it\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0using cross-validation """"""\n\n\xa0\xa0# deactivated parameters stored as None this is not\n\xa0\xa0# accepted by SVM below we removing them\n\xa0\xa0cfg = {k: cfg[k] for k in cfg if cfg[k]}\n\xa0\xa0# translate the boolean values\n\xa0\xa0cfg[\'shrinking\'] = True if cfg[\'shrinking\'] == \'true\' else False\n\xa0\xa0# for gamma set it to fixed value or auto\n\xa0\xa0if \'gamma\' in cfg:\n\xa0\xa0\xa0\xa0cfg[\'gamma\'] = cfg[\'gamma_value\'] if cfg[\'gamma\']==\'value\' else \'auto\'\n\xa0\xa0\xa0\xa0cfg.pop(\'gamma_value\',None)\n\xa0\xa0model = svm.SVC(**cfg, random_state=42)\n\xa0\xa0scores = cross_val_score(model, iris.data,iris.target, cv=5)\n\xa0\xa0return (1 - np.mean(scores)) # as SMAC minimizes the cost', ""# scenario object\nscenario = Scenario({'run_obj':'quality',\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'runcount-limit':120,\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'cs':cs,\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0'deterministic':'true'})"", 'smac = SMAC4HPO(scenario=scenario, rng=42,\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0tae_runner=cfg_svm)\nincumbent = smac.optimize()', 'print(cs.get_default_configuration())', 'print(incumbent)', ""def_value = cfg_svm(cs.get_default_configuration())\ninc_value = cfg_svm(incumbent)\nprint('Default Value: {:.4f}'.format(def_value))\nprint('Optimized Value: {:.4f}'.format(inc_value))"""
106,ml_models_using_lazypredict/,https://analyticsindiamag.com/visualizing-and-comparing-ml-models-using-lazypredict/,"['We live in a period of computational and technological supremacy, where computing has moved from large mainframes with tangled wires to PCs to an era of cloud computing. The world around us is changing quickly, and what makes it even remarkable is not what has happened until now but what is yet to come. It is an exciting time to live where various tools and techniques are being developed, followed by a major boost in computing, which can truly be called the world of Data Science!. Machine Learning, or ML for short, has proven to be one of the most game-changing technological advancements of the current decade. In an increasingly competitive corporate world, ML enables organizations to fast-track digital transformation and move swiftly into an age of automation. AI/ML are here to stay relevant due to their demand of usage in everyday life, such as digital payments and fraud detection in banking or providing product recommendations to customers.\xa0', 'The adoption of machine learning algorithms and methods to learn them are well-documented and readily accessible, with different companies moving in to adopt machine learning at scale across the verticals. Every other app and software available today all over the Internet uses machine learning in some way or the other. Machine Learning has now become the go-to solution for companies to solve problems. Today, one can use machine learning to process current and past data to predict future data. Other real-world applications vary from finding the shortest way in a map to reach a destination to identifying types of cancer cells.', 'The process of developing a machine learning model can be complicated, and the model that is developed must be constructed in such a manner that it fits the problem perfectly. Machine learning is used to either solve a problem or offer insights that can lead to better decision making, wherever data is present. In the case of machine learning models, however, no one method can be used to solve all problems. Distinct types of algorithms are developed to solve different issues using entirely different techniques. For each class, the inputs supplied, the task completed, and the results achieved are all extremely distinct.\xa0', 'Some of the major types are Supervised Learning, Semi-Supervised Learning and Reinforcement Learning. Supervised learning is a machine learning algorithm where a model or a function is being developed to map the input from the test data to their respective output. Here, the training dataset is a data bank of labelled data, and the test data is a set of inputs having no labels. Unsupervised learning is a type of machine learning that uses the inferences drawn from a dataset without labels. Reinforcement learning algorithms are the type of machine learning model where tasks are being performed by an agent in a particular simulation environment. During this, the agent either receives a reward or punishment for each task that is performed. Unlike other machine learning approaches, the algorithm is not given any instructions and learns by itself.\xa0', 'Machine learning methods depend upon the type of task and can be further categorized as Classification models, Regression models, Clustering etc. Classification is the task of predicting a type or class of an object from a finite number of options. The output variable generated by classification is usually categorical, but regression may be used to solve a collection of issues using a continuous output variable. Predicting the price of airline tickets, for example, is a classic regression assignment. Clustering, on the other hand, is the challenge of grouping items that are related in some way. It assists in the automated identification of similar items without the need for human interaction.', 'LazyPredict is an open-source python library that helps you to semi-automate your Machine Learning Task. It can build multiple models without writing much code and helps understand which models work better for the processed dataset without requiring any parameter tuning. Using LazyPredict, one can apply all the models on that dataset to compare and analyze how our basic model is performing. Here a basic model means a “Model without parameters”. It can help deriving accuracies, and after getting accuracy for all the models, one can choose the top 5 models and then apply hyperparameter tuning to them. It comes with a Lazy Classifier to solve classification problems and Lazy Regressor to solve the regression problems.\xa0', 'While building machine learning models, one cannot be sure which algorithm will work well on the given dataset; hence, it ends up trying many models and keeping iterating until proper accuracy is synthesized. LazyPredict comes to the rescue for such use cases, generating all the basic machine learning algorithms’ performances on your model. Along with the accuracy score, LazyPredict also provides certain evaluation metrics and describes the time taken by each model.', 'In this article, we will implement a model using the LazyPredict library, which will help us find the best-suited model for Classification and Regression to be used for our dataset and each model’s accuracy score. In addition, we will also be visualizing the accuracy scores to compare and choose the best-suited model for the dataset being processed. The following code is inspired by the documentation provided by the creators of LazyPredict, whose link can be found here.\xa0', 'The first step will be to install the LazyPredict library to set up our model; you can use the following code to do so,', 'We are also installing the latest version of SciPy, which will help us process the data better.', 'Now we will be importing the required essential dependencies for our model,', 'Let us now load our dataset; we use the inbuilt Breast Cancer dataset for our first problem.', 'Splitting the dataset into train and test,', 'With the dataset loaded and everything else set up, now let’s perform our classification task. For this, we will be setting up our classification pipeline using the LazyClassifier.\xa0', 'As you can see, the LazyClassifier has provided us with scores for all the possible models according to our dataset!', 'Let us now visualize the scores to derive a better understanding,', 'Using such visualizations, we can now easily understand the best model to be used for optimal accuracy.', 'We can also perform Regression using the LazyRegressor module on another dataset!', 'Creating Visualization for a particular column, such as for R-squared Scores only,', 'In this article, we learnt about the importance of different machine learning models and their uses. We also created a model using the LazyPredict library that helps us understand the best-suited model for our dataset for optimal results and accuracy. The Following implementation can be found as a Colab notebook, using the link here.\xa0']","'!pip install lazypredict\n!pip install scipy==1.7.1'
 '#cloning the model\n!git clone https://github.com/shankarpandala/lazypredict.git', '# Import libraries\nimport lazypredict\nfrom lazypredict.Supervised import LazyClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split', '# Load dataset\ndata = load_breast_cancer()\nX = data.data\ny= data.target\n', '# Splitting Dataset\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.2,random_state =42)\n', '# Defining parameters for lazyclassifier\nclf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\nmodels_train,predictions_train = clf.fit(X_train, X_train, y_train, y_train)\nmodels_test,predictions_test = clf.fit(X_train, X_test, y_train, y_test)\n\n \n# Printing all the model performances\nmodels_train\n', '#plotting the accuracy scores\n \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n \nplt.figure(figsize=(10, 5))\nsns.set_theme(style=""whitegrid"")\nax = sns.barplot(x=models_train.index, y=""Accuracy"", data=models_train)\nplt.xticks(rotation=90)', 'import matplotlib.pyplot as plt\nimport seaborn as sns\n \nplt.figure(figsize=(5, 10))\nsns.set_theme(style=""whitegrid"")\nax = sns.barplot(y=models_train.index, x=""Accuracy"", data=models_train)\n', '# Importing the libraries\nfrom lazypredict.Supervised import LazyRegressor\nfrom sklearn import datasets\nfrom sklearn.utils import shuffle\nimport numpy as np\n \n# Loading the Boston dataset\nboston = datasets.load_boston()\nX, y = shuffle(boston.data, boston.target, random_state=42)\n \n#Splitting Data\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=.2,random_state =42)\n \n# building the pipeline\nreg = LazyRegressor(verbose=0,ignore_warnings=False, custom_metric=None)\nmodels_train,predictions_train = reg.fit(X_train, X_train, y_train, y_train)\nmodels_test,predictions_test = reg.fit(X_train, X_test, y_train, y_test)\n \n# Printing all model performances\nmodels_train\n', 'import matplotlib.pyplot as plt\nimport seaborn as sns\n \nplt.figure(figsize=(10, 5))\nsns.set_theme(style=""whitegrid"")\nax = sns.barplot(x=models_train.index, y=""R-Squared"", data=models_train)\nax.set(ylim=(0, 1))\nplt.xticks(rotation=90)', 'import matplotlib.pyplot as plt\nimport seaborn as sns\n \nmodels_train[""R-Squared""] = [0 if i < 0 else i for i in models_train.iloc[:,0] ]\n \nplt.figure(figsize=(5, 10))\nsns.set_theme(style=""whitegrid"")\nax = sns.barplot(y=models_train.index, x=""R-Squared"", data=models_train)\nax.set(xlim=(0, 1))'"
107,interpretability_library_for_pytorch/,https://analyticsindiamag.com/facebook-gives-away-captum-for-free-its-model-interpretability-library-for-pytorch/,"['Concept-based interpretability tools assist artificial intelligence researchers and engineers in designing, developing and debugging AI models. Additionally, these tools help understand the working of AI models, helping businesses assess if the models deliver accurate results and reflect their values. One such interpretability tool is Facebook’s Captum.\xa0', 'Captum is a powerful and flexible interpretability library for PyTorch, which makes algorithms for interpretability readily available for access to the PyTorch community. Captum supports model interpretability across modalities — vision, text; additionally allowing researchers to add new algorithms and benchmark their work against existing algorithms available in the library. Finally, it offers tools to help developers uncover vulnerabilities using metrics and adversarial attacks.\xa0', 'Recently, Facebook released its latest version of Captum — Captum 0.4, which has new functionality for model understanding. Facebook AI has added tools to evaluate model robustness, improvements to its existing attribution models, and new attribution methods in the latest version.\xa0', 'Captum 0.4 adds testing with Concept Activation Vectors (TCAV) that allows researchers and engineers to assess how different user-defined concepts affect a model’s prediction. It can also be used for checking algorithmic and label bias, which might be embedded in networks.\xa0', 'Additionally, TCAV’s capabilities expand beyond currently available attribution methods, enabling researchers to quantify the importance of the various inputs and quantify the impact of concepts like gender and race on a model’s prediction.\xa0', 'Captum 4.0 comes with generically implemented TCAV, allowing users to define custom concepts with example inputs for different modalities — vision and text.\xa0', 'Source: Facebook AI', 'The graphs above showcase visualised distributions of TCAV scores for the sensitivity analysis model implemented in a Captum tutorial. As a data set, Facebook AI researchers have used movie ratings with positive sentiment. The graphs visualise TCAV scores for positive adjectives concepts along with five sets of neutral terms concepts. The positive adjectives concept is more important than for both convolutional layers across all the five neutral concept sets, indicating the importance of positive adjectives in predicting positive sentiment.\xa0', 'Deep learning techniques are often vulnerable to adversarial inputs that, in turn, can fool the AI model and be imperceptible to humans. Captum 0.4 comes with tooling to support the improved understanding of the limitations and vulnerabilities of a model. As a result, the AI system will react to unforeseen issues and make necessary changes to avoid harming or otherwise negatively affecting people.\xa0', 'Captum 0.4 also comes with tools to understand the robustness of the model, including implementations of adversarial attacks and robustness metrics to evaluate the impact of different attacks or perturbations on a model. The robustness metrics included in its latest version are:\xa0', 'This new tooling enables developers to understand potential model vulnerabilities better and analyse counterfactual examples to comprehend a model’s decision boundary better.\xa0', 'Source: Facebook AI', 'Facebook AI has implemented a new attribution algorithm, in collaboration with Technische Universitat Berlin, to offer a new perspective for explaining model predictions. Captum 0.4 adds both LRP and a layer-attribution variant — layer LRP.\xa0', 'The Layer-wise Relevance Propagation (LRP) algorithm is based on a backward propagation mechanism applied sequentially to all layers of the model. The model output score represents the initial relevance which is then decomposed into values for each neuron of the underlying layers.\xa0']",
108,for_interactive_machine_learning/,https://analyticsindiamag.com/guide-to-vowpal-wabbit-a-state-of-the-art-library-for-interactive-machine-learning/,"['Vowpal Wabbit is a flexible open-source project designed to tackle complex interactive machine learning tasks. With Microsoft Research and (earlier) Yahoo! Research as major project contributors, Vowpal Wabbit results from intensive community research and contributions since 2007. It provides you with rapid, online and active machine learning solutions for supervised learning and reinforcement learning.\xa0', '\xa0Vowpal Wabbit supports Windows, macOS and Ubuntu operating systems. To date, C#, command line and Python packages of Vowpal Wabbit are available for Windows OS, while Java configuration is yet to be released. For macOS and Ubuntu, C# and Java packages will be out soon.', 'Image source: Official website', '2. Supervised learning', '3. Interactive learning: Vowpal Wabbit enables online machine learning which does not require all the input data to be available before the algorithm learns to infer. It allows learning from an expanding data source for problems that vary.', '4. Efficient learning: Vowpal Wabbit can handle problems with a huge number of sparse features. Also, it achieves scalability by allowing the feature set to be independent of the training data size.', '5. Versatile learning', 'Here’s a demonstration of solving a contextual bandits problem using Vowpal Wabbit The code has been implemented in Google colab with Python 3.7.10 and vowpalwabbit 8.9.0 versions. Step-wise explanation of the code is as follows:', 'Where ‘prob’ denotes the probability of the actions’ occurrence, and ‘f’ denotes feature.', 'training_df = pd.DataFrame(training_data)', 'Training data:', 'Test data:', 'vw = pyvw.vw(""--cb 4"")', '‘pyvw’ is a Python binding for pylibvw class. –cb is the contextual bandit module for optimizing the predictor based on already existing data without further exploration. ‘4’ in “–cb 4” above denotes the number of possible actions.', 'According to the training data’s cost structure, contextual bandit assigns each test instance to action 4 as can be seen from the above output.', 'For more applications and a detailed understanding of Vowpal Wabbit, refer to the following sources:']","'training_df = pd.DataFrame(training_data)'
 'vw = pyvw.vw(""--cb 4"")'"
109,new_hyperparameter_optimization_tool/,https://analyticsindiamag.com/hands-on-python-guide-to-optuna-a-new-hyperparameter-optimization-tool/,"['Hyperparameter Optimization is getting deeper and deeper as the complexity in deep learning models increases. Many handy tools have been developed to tune the parameters like HyperOpt, SMAC, Spearmint, etc. However, these existing tool kits have some serious issues that can’t be neglected as we progress.\xa0', 'To address the above concerns, The Japanese AI company Preferred Networks, has developed an advanced self-contained hyperparameter optimization framework called Optuna. Optuna is an open-source hyperparameter optimization toolkit designed to deal with machine learning and non-machine learning(as long as we can define the objective function). It provides a very imperative interface to fully support Python language with the highest modularity level in code.', 'Features of Optuna', 'Install Optuna toolkit in Python via Pip', '!pip install optuna', 'In the undermentioned image, import the optuna package, create an objective function with parameter trial(specifies the number of trials), write your machine learning model within the function and return the trained model’s evaluation. Now, create an optuna study and specify that particular objective function(minimize/maximize). Finally, optimize your created study by passing the objective function and number of trials.', 'You can check the full demo here.', 'Suppose, you are confused about which regularization method is better “Lasso” or “Ridge”. In that case, you have to optimize ridge and lasso both and compare them. Creating this specific pipeline from scratch is time-consuming but Optuna can do this task in only 4 lines of code. Optuna can deal with conditional hyperparameters with its imperative (define-by-run) interface.', 'study.trials_dataframe()', 'The output of this is shown below.', 'You can check the whole demo here.', 'We have already listed out all the sampling and pruning algorithms available in Optuna in above section. Optuna uses TPE as its default sampling algorithm. In this demo, we will see how pruning algorithms can be implemented using Optuna.', 'You can check the full demo, here.', 'Optuna is suitable for Non-ML tasks as long as we are able to define an objective function. The image shown below, describes the whole process.', 'An example of it(in python) is shown below.', 'You can check the full demo, here.', 'In this session, we have discussed a next-generation Hyperparameter optimization framework Optuna. Not only it optimizes the parameters, but also provides visualization plots and dashboard. Four demos of using this library is shown above. Colab notebooks are available at:', 'You can check other hyperparameter related articles, here.']","'!pip install optuna'
 '## In optuna, A Trial represents a single call of the objective function\n## Study shows an optimization session which contains a set of trials\n## In this demo, ""alpha"" is the hyperparameter which is need to be optimized\ndef objective(trial):\n   \n    # hyperparameter setting, trial.suggest_uniform will suggest uniform hyperparameter\n    #alpha between the range of 0.0 to 2.0, lowest value of interval is closed and \n    #when low=high, it will return low value\n    alpha = trial.suggest_uniform(\'alpha\', 0.0, 2.0)\n    \n    # data loading and train-test split\n    X, y = load_iris(return_X_y=True)\n    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n    \n    # model training and evaluation\n    model = sklearn.linear_model.Lasso(alpha=alpha)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    error = sklearn.metrics.mean_squared_error(y_val, y_pred)\n \n    # output: evaluation score\n    return error', ""# In Optuna, we use the study object to manage optimization.\n# Method :func:`~optuna.create_study` returns a study object.\n# A study object has useful properties for analyzing the optimization outcome.\nstudy = optuna.create_study(direction='minimize') #Set minimize for minimization and maximize for maximization.\n#To start the optimization, we create a study object and pass the objective function to method\nstudy.optimize(objective, n_trials=50)"", '# To get the dictionary of parameter name and parameter values:\nprint(""Return a dictionary of parameter name and parameter values:"",study.best_params)\n \n# To get the best observed value of the objective function:\nprint(""Return the best observed value of the objective function:"",study.best_value)\n \n# To get the best trial:\nprint(""Return the best trial:"",study.best_trial)\n \n# To get all trials:\nprint(""Return all the trials:"", study.trials)', '## In optuna, A Trial represents a single call of the objective function\n## Study shows an optimization session which contains a set of trials\n## In this demo, ""alpha"" is the hyperparameter which is need to be optimized\n \n##ridge_alpha: the regularization constant of ridge\n##lasso_alpha: the regularization constant of lasso\n \ndef objective(trial):\n   \n    # hyperparameter setting\n    #:func:`optuna.trial.Trial.suggest_categorical` for categorical parameters\n    #The line below will optimize both ridge and lasso and compares them and find the best hyperparameter\n    #between ridge_alpha and lasso_alpha\n    regression_method = trial.suggest_categorical(\'regression_method\', (\'ridge\', \'lasso\'))\n    if regression_method == \'ridge\':\n        ridge_alpha = trial.suggest_uniform(\'ridge_alpha\', 0.0, 2.0)\n        model = Ridge(alpha=ridge_alpha)\n    else:\n        lasso_alpha = trial.suggest_uniform(\'lasso_alpha\', 0.0, 2.0)\n        model = Lasso(alpha=lasso_alpha)\n    \n    # data loading and train-test split\n    X, y = load_iris(return_X_y=True)\n    X_train, X_val, y_train, y_val = train_test_split(X, y, random_state=0)\n    \n    # model training and evaluation\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_val)\n    error = mean_squared_error(y_val, y_pred)\n \n    # output: evaluation score\n    return error', 'study.trials_dataframe()', '# Define the objective function.\ndef objective(trial):\n    #load the dataset\n    data, target = load_iris(return_X_y=True)\n    #split the dataset in train and test\n    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n    dvalid = lgb.Dataset(valid_x, label=valid_y)\n    #default parameters dictionary for optimizations(search space)\n    param = {\n        ""objective"": ""binary"",\n        ""metric"": ""auc"",\n        ""verbosity"": -1,\n        ""boosting_type"": ""gbdt"",\n        ""bagging_fraction"": trial.suggest_float(""bagging_fraction"", 0.4, 1.0),\n        ""bagging_freq"": trial.suggest_int(""bagging_freq"", 1, 7),\n        ""min_child_samples"": trial.suggest_int(""min_child_samples"", 5, 100),\n    }\n \n    # Add a callback for pruning.\n    # To turn on the pruning feature, you need to call :func:`~optuna.trial.Trial.report` and :func:`~optuna.trial.Trial.should_prune` after each step of the iterative training.\n    # :func:`~optuna.trial.Trial.report` periodically monitors the intermediate objective values.\n    # :func:`~optuna.trial.Trial.should_prune` decides termination of the trial that does not meet a predefined condition.\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, ""auc"")\n    gbm = lgb.train(\n        param, dtrain, valid_sets=[dvalid], verbose_eval=False, callbacks=[pruning_callback]\n    )\n    #Predict the valid data\n    preds = gbm.predict(valid_x)\n    #rounding values to its nearest integers\n    pred_labels = np.rint(preds)\n    #calculating accuracy\n    accuracy = accuracy_score(valid_y, pred_labels)\n    return accuracy', '# Add stream handler of stdout to show the messages\noptuna.logging.get_logger(""optuna"").addHandler(logging.StreamHandler(sys.stdout))\n# Method :func:`~optuna.create_study` returns a study object.\n# A study object has useful properties for analyzing the optimization outcome.\nstudy = optuna.create_study(\n    direction=""maximize"",\n    sampler=optuna.samplers.TPESampler(),\n    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),\n) #Set minimize for minimization and maximize for maximization.\n#To start the optimization, we create a study object and pass the objective function to method\nstudy.optimize(objective, n_trials=100, timeout=600) #add timeout for model not to exceed this time limit', ""def objective(trial):\n    #trial suggesting the values for x between -100 to 100\n    x = trial.suggest_uniform('x', -100, 100)\n    #return the funciton value\n    return (x - 2) ** 2"""
110,for_machine_learning_visualizations/,https://analyticsindiamag.com/yellowbrick-hands-on-guide-a-python-tool-for-machine-learning-visualizations/,"['Yellowbrick is mainly designed to visualize and Diagnose the machine learning models. It is a visualization suite built on top of Scikit-Learn and Matplotlib. It helps in the model selection process, hyperparameter tuning, and algorithm selection.\xa0', 'Yellowbrick calls an API using the visualizer which is a scikit-learn estimator, the visualizer learns from data by creating the visualization of the workflow of the model selected.\xa0 These visualizations allow us to draw insights into the model selection process.\xa0\xa0', 'In this article, we will explore different types of visualizations that are provided by Yellowbrick and how we can create them according to our requirements.', 'Yellowbrick is based on scikit-learn and matplotlib so we need to install both and then install yellowbrick. The command for installing all three libraries is given below:', 'pip install scikit-learn', 'pip install matplotlib', 'pip install yellowbrick', 'We will import different functions defined in yellowbrick and scikit-learn for model selection as and when required. We will start by visualizing an advertising dataset that contains 3 features and one target variable ‘Sales’.', 'a. Loading the Dataset', 'import pandas as pd', 'df = pd.read_csv(‘Advertising.csv’)', 'df', 'b. Defining Target and Feature variables', ""x = df[['TV', 'Radio', ‘Newspaper’]]"", ""y= df['Sales']"", 'c. Visualizing Features', 'from yellowbrick.features import Rank1D', 'visual = Rank1D()', 'visual.fit(x, y)', 'visual.transform(x)', 'visual.show()\xa0', '\xa0\xa0\xa02. Linear Regression Visualization', 'We will create a linear regression model using Scikit-Learn to visualize the Linear Regression using Yellowbrick.', 'a. Creating the model', 'We will create a linear regression model to visualize.', 'from sklearn.model_selection import train_test_split', 'from sklearn.linear_model import LinearRegression', 'x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)', 'model = LinearRegression().fit(x_train, y_train)', 'model_pred = model.predict(x_test) ', 'b. Visualizing the Model', 'Using yellowbrick to visualize the model.\xa0', 'from yellowbrick.regressor import PredictionError , ResidualsPlot', 'visual = PredictionError(model).fit(x_train, y_train)', 'visual.score(x_test, y_test)', 'visual.poof()', '\xa0\xa0\xa0\xa03. Model Selection Visualization', 'The model selection visualizer helps us in inspecting the performance of cross-validation and hyperparameter tuning.\xa0', 'Let us visualize the feature importance using Random Forest Classifier and Yellowbrick.', 'from sklearn.ensemble import RandomForestClassifier', 'from yellowbrick.model_selection import FeatureImportances', 'model = RandomForestClassifier()', 'viz = FeatureImportances(model)', 'viz.fit(x, y)', 'viz.show()', 'Similarly, we can visualize feature importance using Logistic Regression and yellowbrick.\xa0', 'model = LogisticRegression(multi_class=""auto"", solver=""liblinear"")', 'visual = FeatureImportances(model, stack=False, relative=False)', 'visual.fit(x, y)', 'visual.show()', '\xa0\xa04.  Textual Data Visualization', 'Yellowbrick can help us analyze the textual data properties also. For analyzing textual data we can read any textual data using the open function and visualize the frequency of the word using Frequency Distribution Visualizer.', 'a. Importing Library and loading dataset', 'from sklearn.feature_extraction.text import CountVectorizer', 'from yellowbrick.text import FreqDistVisualizer', ""corpus = open('text.txt', 'r')"", 'vectorizer = CountVectorizer()', 'docs \xa0 \xa0 \xa0 = vectorizer.fit_transform(corpus)', 'features \xa0 = vectorizer.get_feature_names()', 'b. Visualizing The frequency and features or words', ""visualizer = FreqDistVisualizer(features=features, orient='v')"", 'visualizer.fit(docs)', 'visualizer.show()', '\xa0\xa0\xa0\xa05. Anscombe’s Quartet\xa0', 'In the end, let us visualize the Anscombe’s Quartet which is a collection of four datasets that have similar statistical properties in the description format but are very different in the visual format. Anscombe’s Quartet clearly describes why we need to visualize data is an example of why Visualization is important for machine learning.\xa0', 'import yellowbrick as yb', 'import matplotlib.pyplot as plt', 'ans = yb.anscombe()', 'plt.show()', 'We can clearly visualize how different these four datasets are irrespective of their similar statistical properties.']","'import pandas as pd'
 'df = pd.read_csv(‘Advertising.csv’)', 'df', ""x = df[['TV', 'Radio', ‘Newspaper’]]"", ""y= df['Sales']"", 'from yellowbrick.features import Rank1D', 'visual = Rank1D()', 'visual.fit(x, y)', 'visual.transform(x)', 'visual.show()\xa0', 'from sklearn.model_selection import train_test_split', 'from sklearn.linear_model import LinearRegression', 'x_train, x_test, y_train, y_test = train_test_split(x,y, random_state=1)', 'model = LinearRegression().fit(x_train, y_train)', 'model_pred = model.predict(x_test)', 'from yellowbrick.regressor import PredictionError , ResidualsPlot', 'visual = PredictionError(model).fit(x_train, y_train)', 'visual.score(x_test, y_test)', 'visual.poof()', 'from sklearn.ensemble import RandomForestClassifier', 'from yellowbrick.model_selection import FeatureImportances', 'model = RandomForestClassifier()', 'viz = FeatureImportances(model)', 'viz.fit(x, y)', 'viz.show()', 'model = LogisticRegression(multi_class=""auto"", solver=""liblinear"")', 'visual = FeatureImportances(model, stack=False, relative=False)', 'visual.fit(x, y)', 'visual.show()', 'from sklearn.feature_extraction.text import CountVectorizer', 'from yellowbrick.text import FreqDistVisualizer', ""corpus = open('text.txt', 'r')"", 'vectorizer = CountVectorizer()', 'docs \xa0 \xa0 \xa0 = vectorizer.fit_transform(corpus)', 'features \xa0 = vectorizer.get_feature_names()', ""visualizer = FreqDistVisualizer(features=features, orient='v')"", 'visualizer.fit(docs)', 'visualizer.show()', 'import yellowbrick as yb', 'import matplotlib.pyplot as plt', 'ans = yb.anscombe()', 'plt.show()'"
111,https://analyticsindiamag.com/adversarial_robustness_toolbox_art/,https://analyticsindiamag.com/adversarial-robustness-toolbox-art/,"['Machine Learning models can be exposed to the threat to jeopardise with the predictions. Such attacks on deployment ends have been seen time and again and thus needed to be addressed accurately. AI security is most necessary for enterprise AI systems where data storage is mostly in tabular forms, and data privacy policies are at stake. The Adversarial Robustness Toolbox(ART) is a Python library which is one of the complete resources providing developers and researchers for evaluating the robustness of deep neural networks against adversarial attacks. Open-sourced by IBM, ART provides support to incorporate techniques to prevent adversarial attacks for deep neural networks written in TensorFlow, Keras, PyTorch, sci-kit-learn, MxNet, XGBoost, LightGBM, CatBoost and many more deep learning frameworks. It can be applied to all kinds of data from images, video, tables, to audio, and many more. It is cross-platform and supports various machine learning tasks such as classification, speech recognition, object detection, generation, certification, etc.', 'ART has attracted many developers since its first release. Its latest version v1.5 allows it to evaluate and defend AI models and applications against the 4 adversarial threats of inference, extraction, poisoning and evasion with a single unified library.', 'The new version extends the supported ML tasks to include object detection, automatic speech recognition (ASR), generative-adversarial networks (GAN), and robustness certification in addition to simple classification models and is compatible with more popular ML frameworks to prevent users from being under the hood of one framework. The threats of extraction, where the attacker is liable to steal a model via model queries, and inference, thereby allowing the attacker to acquire private information in a model’s training data. There are three different types of inference attacks that can disrupt different features of the privacy of data in training. In membership inference, ART allows reproducing a malicious attacker attempting to acquire information of a specific record, e.g. of a person, has been part of training data in an ML model or not. Such attacks can be harmful as they expose sensitive private information from just having access to a trained ML model. The attribute inference attack aims at extracting the original attribute values of an existing record in the training data, which can be only accessed by the trained model and knowing a few of the other features. For example, an ML model trained on demographic data is attacked with attribute inference could expose information about a person’s exact DOB and wages. Lastly, the model invasion where attackers can invert a trained ML model by reconstructing representative averages of features from the record.', 'Adversarial Action Recognition Attack', 'This demonstrates the usage of ART library to impose an adversarial attack on video action recognition. First, it uses GluonCV and MXNet for video action recognition. MXNet pre-trained models are used for classification tasks. Specifically, the pre-trained i3d_resnet50_v1_ucf101 model is used. The video clip of a basketball action taken from the UCF101 dataset. To show how to classify the following short video clip correctly.', '# Initial working stages\xa0\xa0', '# Loading Model and Basketball Sample', '# setting global variables', '# setting seed', '# downloading sample video', '# preprocessing the benign video sample', '`/home/hessel/.art/data/v_Basketball_g01_c01.avi` has been downloaded and preprocessed.', '# loading pretrained model', '`i3d_resnet50_v1_ucf101` model was successfully loaded.', '# evaluating model on basketball video sample', '_ = predict_top_k(sample, model)', 'For the given video sample, it is seen that the model correctly classified it as playing basketball.', '# Creating Adversarial Attack', 'Now we can include the ART library for the adversarial attack via the Fast Gradient Method. The attack is incorporated to corrupt the video sample so that it could be misclassified. Also, the adversarial example is converted into a GIF\xa0', 'Adversarial Basketball', '# preprocessing the adversarial sample video input', '# wrapping model in a ART classifier', '# preparing the mean and std arrays for ART classifier preprocessing', '# verifying whether the ART classifier predictions are consistent with the original model:', '# crafting adversarial attack with FGM', 'CPU times: user 2min 44s, sys: 826 ms, total: 2min 45s', 'Wall time: 2min 45s', '# printing results', '_ = predict_top_k((adv_sample-mean)/std, model)', '# saving adversarial example to gif:', '`/home/hessel/.art/data/adversarial_basketball.gif` has been successfully created.', '# Creating Sparse Adversarial Attack', 'Using the Frame Saliency Attack, now it’s time to create a sparse adversarial example. The final result is shown in the GIF. Here only one frame is needed to be perturbed to achieve a misclassification.', 'adversarial_basketball_sparse.gif', '# Frame Saliency Attack. Note: we specify here the frame axis, which is 2.', 'CPU times: user 5min 54s, sys: 700 ms, total: 5min 55s', 'Wall time: 5min 56s', '# printing the resulting predictions:', '_ = predict_top_k((adv_sample_sparse-mean)/std, model)', '# Again saving the adversarial example to gif:', '`/home/hessel/.art/data/adversarial_basketball_sparse.gif` has been successfully created.', '# counting the number of perturbed frames:', 'Number of perturbed frames: 1', '# Applying H.264 compression defence', 'Next VideoCompression is applied as a simple input preprocessing defence mechanism. This defence is intended to correct predictions when applied to both the original and the adversarial video input.', '# initializing VideoCompression defense', 'video_compression = VideoCompression(video_format=""avi"", constant_rate_factor=30, channels_first=True)', '# applying defense to the original input', 'adv_sample_input_compressed = video_compression(adv_sample_input * 255)[0] / 255', '# applying defense to the sparse adversarial sample', 'adv_sample_sparse_compressed = video_compression(adv_sample_sparse * 255)[0] / 255', '# printing the resulting predictions on compressed original input', '_ = predict_top_k((adv_sample_input_compressed-mean)/std, model)', '# printing the resulting predictions on sparse adversarial sample', '_ = predict_top_k((adv_sample_sparse_compressed-mean)/std, model)', 'EndNotes']","'`/home/hessel/.art/data/v_Basketball_g01_c01.avi` has been downloaded and preprocessed.'
 '`i3d_resnet50_v1_ucf101` model was successfully loaded.', '_ = predict_top_k(sample, model)', '_ = predict_top_k((adv_sample-mean)/std, model)', '`/home/hessel/.art/data/adversarial_basketball.gif` has been successfully created.', '_ = predict_top_k((adv_sample_sparse-mean)/std, model)', 'Number of perturbed frames: 1', 'video_compression = VideoCompression(video_format=""avi"", constant_rate_factor=30, channels_first=True)', 'adv_sample_input_compressed = video_compression(adv_sample_input * 255)[0] / 255', 'adv_sample_sparse_compressed = video_compression(adv_sample_sparse * 255)[0] / 255', '_ = predict_top_k((adv_sample_input_compressed-mean)/std, model)', '_ = predict_top_k((adv_sample_sparse_compressed-mean)/std, model)'"
112,https://analyticsindiamag.com/guide_to_microsofts_flaml/,https://analyticsindiamag.com/guide-to-microsofts-flaml/,"['The current data science scenario raises a big question: how and what to select as a machine learning model to predict all best. When selecting, we use conventional ways like hyperparameter tuning, GridSearchCV and Random search to choose the best-fit parameters. These conventional techniques help us a lot, but they are time-consuming, take high computation power, and are a huge weight for our working environment.', 'What if a lightweight model can tell us the best fit model and the best parameters to tune for our dataset? Imagine how much research and time it can save. For this reason, Microsoft FLAML\xa0 comes into the picture.', 'FLAML is a python package that can tell us the best-fit machine learning model for low computation. Thus, it removes the burden of the manual process of choosing the best model and best parameter.\xa0', 'Nowadays, many businesses started building machine learning embedded applications, and it costs a lot to select a single machine learning model from a variety of machine learning models. After choosing a model, it is also time-consuming to select the best parameters for every dataset. To solve this problem, Microsoft built an AutoML system which is mainly focused on:', 'This is an ongoing project for model selection, feature engineering, and hyperparameter tuning. Microsoft has built the Microsoft FLAML library package using python development. In the next section, we will start with the basics of Microsoft FLAML(a fast and lightweight autoML library.\xa0', 'Setting up the environment in google colab.', 'Requirements : python version 3.6 or above, scikit-learn 0.23.2 or above, xgboost 0.90 or above, catboost 0.26 or above,\xa0 lightgbm 3.2.1 or above scikit-learn 0.24.2 or above and\xa0 threadpoolctl 2.1.0 or above', 'We can install them by using pip.\xa0', '!pip install flaml', 'Importing required libraries', 'We are going to make a classification model on the iris dataset, which is present in sklean.dataset library. We can learn about the data set from here.', 'Loading the iris data set\xa0', 'These are feature and the target column of the dataset in the dataset\xa0 target value names are setosa , versicolor and virginica. Next we will split the dataset into training and testing. We can do this by using following code:', 'In this section,\xa0 we will implement autoML engine to get the best classification model. This is very important that we give the select ‘classifier’ in the task parameter.', 'We can see it started iterating between the different models and using Strathfield k fold cross-validation for evaluation in the image.', 'In the image, we can see the result of the automl_clf model, and we got to know which model with parameter is the best fit for the data set. It is already fitted and ready to predict in just 60.0 seconds, pretty less than the time consumed by any other hyperparameter tuning technique we use in general. Finally, we would see the accuracy score of the automl_clf model.', 'We are predicting x_test.', 'y_pred = automl_clf.predict(x_test)', 'Now we call accuracy_score and find the accuracy score between y_test and y_true.', 'We can see this is a satisfactory result; now we would go with the model itself, which AutoML suggests; it suggested an ExtraTreesClassifier classification model with some parameters picking up that model from sklearn.ensemble library.', 'Calling and defining the model.\xa0', 'Let’s check for the accuracy score by the following command.\xa0', 'accuracy_score(y_test,y_pred)', '0.9333333333333333', 'We got the same accuracy for this model also; this is fascinating that without wasting so much time and hard work, we have got a pretty decent model, and everything happened with only a few lines of code where it searched for a better model to best fit parameters and gave the result.', 'Next, we try the same thing with a regression model also. To perform this, we are again going to use a data set from sklearn.datasets library, but this would be the Boston Housing data set. Two know more about the data set; you can click here.', 'Loading the data set', 'Dividing the dataset into predictor variable (x) and target variable (y).', 'Now we are going to fit the model with the x_train and y_train, but this time we are seeking results in regression, so in this case, we will provide “regression” as the value to task parameter :', 'As we can see, the output model started finding for best fit model and using repeatedKFold cross-validation methods for evaluation.', 'As the final results, we got the ExtraTreeRegression model as the best fit model and some parameter; with the same parameters, we will be performing the model fitting task again, but in the next step, we will see R2 score, mean squared log error, mean squared error, mean absolute error, the max error between test and predicted data.', 'Let’s check with the ExtraTreesRegressor model, which the AutoML model suggests and what result it will give. First, let’s call the model from sklearn.ensemble library, and after giving the same parameters to the model suggested by the AutoML model, we will check for different errors.', 'Making the prediction-', 'y_pred=sug_reg.predict(x_test)', 'Majoring the error values and r2 value bet y_test and y_pred-', 'So here, we can see a slight deviation between AutoMl and ExtraTreeRegressor, but both are satisfying. AutoML model can also be used as a hyperparameter tuning in very simple steps, lets see how it works and how it is going to perform, in modeling we have seen that it goes through different algorithms tried to change the parameters and try to give the best fit among all the model, but what if we want best-fit parameter of any single algorithm or model. for example, what if we want to know about the best-fit parameters of a random forest classification model for iris dataset using AutoML.', 'Let’s go straight to the hyperparameter tuning using FLAML’s AutoML package.\xa0', 'We can restrict our automl_reg learning model and use it as a hyperparameter tuning tool for random forest regression.\xa0', 'automl_reg.fit(x_train, y_train, task=""regression"", estimator_list=[\'rf\'])', 'And this is how it succeeded to give the best fit for random forest regressor parameters as a hyperparameter tuning tool; now, in the next step, we would see the results of the errors and r2 score for prediction of this model.', 'Codes are following:', 'In this case, we can see we improved the accuracy level of the predictions compared to the last two models. From the basics, we got to know how to use Microsoft FLAML’s Automl package. As of now,\xa0 I am personally impressed by this package.', 'Some of the advantages of using this package are:', 'Let’s sum it all up into the steps we have followed in this article :']","'!pip install flaml'
 'y_pred = automl_clf.predict(x_test)', 'accuracy_score(y_test,y_pred)', '0.9333333333333333', 'y_pred=sug_reg.predict(x_test)', 'automl_reg.fit(x_train, y_train, task=""regression"", estimator_list=[\'rf\'])'"
113,learning_framework_for_nlp/,https://analyticsindiamag.com/a-guide-to-gluonnlp-deep-learning-framework-for-nlp/,"['Natural language processing is one of the most explored and currently trending topics in machine learning. By the NLP daily digital needs such as smart assistance, language translation, text prediction, etc are being addressed. In context to the various libraries used in this field, today in this post we are going to discuss a GluonNLP Natural language processing Deep learning-based toolkit. This toolkit includes cutting-edge pre-trained models, training scripts, and training logs to help with rapid prototyping and reproducible research. We also offer modular APIs with flexible building pieces for easy customization. Following are the major points that we are going to discuss in this post.\xa0\xa0\xa0\xa0\xa0', 'Let’s first understand the library structure.', 'Deep learning has spurred rapid progress in artificial intelligence research, resulting in remarkable discoveries on long-standing problems in a wide range of natural language processing areas. Deep learning frameworks like MXNet, PyTorch, TensorFlow, Caffe, Apache, and Theano make this possible.\xa0', 'These frameworks have been crucial in the transmission of ideas in the field.\xa0 In particular, imperative tools, which were perhaps popularized by Chainer, are straightforward to develop,', 'learn, read, and debug. Such benefits hasten the imperative programming interface.\xa0', 'Jian Guo et al create and develop the GluonNLP toolkits for deep learning in natural language processing using MXNet’s imperative Gluon API. GluonNLP simultaneously provides modular APIs to allow customization by reusing efficient building blocks; pretrained state-of-the-art models, training scripts, and training logs to enable fast prototyping and promote reproducible research; and models that can be deployed in a wide variety of programming languages, including C++, Clojure, Java, Julia, Perl, Python, R, and Scala.', 'Here we’ll discuss the major highlights of this library.\xa0', 'Users may tailor their model design, training, and inference by reusing efficient components across various models with GluonNLP’s modular APIs. Data processing tools, models with individual components, initialization procedures, and loss functions are examples of common components.', 'Take the data API of GluonNLP, which is used to design efficient data pipelines, as an example of how the modular API supports efficient implementation.', 'with data provided by users In natural language processing jobs, inputs are frequently of various shapes, such as sentences of various lengths. As a result, the data API includes a set of utilities for sampling inputs and converting them into mini-batches that may be computed quickly.', 'Building on such modular APIs, GluonCV/NLP provides pre-trained state-of-the-art models, training scripts, and training logs via the model zoo, enabling fast prototyping and encouraging repeatable research. Over 200 models have been supplied by GluonNLP for natural languages processing tasks such as word embedding, language modelling, machine translation, sentiment analysis, natural language inference, dependency parsing, and question answering.', 'In this section by leveraging this library API,\xa0 how to sample and generate a text sequence using a pre-trained language model.\xa0 Using a language model, we can sample sequences based on the likelihood that they will appear in our model for a particular vocabulary size and sequence length.\xa0', 'Given the context from previous time steps, a language model predicts the likelihood of each word happening at each time step.GluonNLP provides two samplers for generating from a language model for this purpose: BeamSearchSampler and SequenceSampler, of which we will use SequenceSampler.', 'Let’s now quickly install the dependencies.\xa0\xa0', 'To begin, load an AWD LSTM language model, which is a state-of-the-art RNN language pre-trained language model from which we will sample sequences.', 'A scorer function is required for Sequence Sampler to function. As the scorer function, we will utilize the BeamSearchScorer, which implements the scoring function with a length penalty.', 'Next, we need to define a decoder based on the pre-trained language model.', 'Now that we have a scorer and a decoder, we’re ready to construct a sampler. The example code below shows how to make a sequence sampler. We’ll make a sampler with 5 beams and a maximum sample length of 100 to control softmax activation.', 'Next, we’ll produce sentences that begin with “I enjoy swimming.” We feed the language model [‘I,’ ‘love,’ ‘to’] to retrieve the starting states and set the initial input to be the word ‘swim’.\xa0', 'All this can be combined with a helper function by which using a single line we can generate the sequence.\xa0', 'Below now we can generate the sequence.', 'generate_sequences(seq_sampler, inputs, begin_states, 5)', 'Here is the output of the function,', 'As we can see the generated context is quite suitable for our original sentence.']","'generate_sequences(seq_sampler
 inputs, begin_states, 5)'"
114,for_basic_nlp_tasks/,https://analyticsindiamag.com/getting-started-with-gensim-for-basic-nlp-tasks/,"['Gensim is an open-source python package for natural language processing with a special focus on topic modelling. It is designed as a topic modelling library, allowing users to apply common academic-based models in production or projects. So, in this article, we will talk about this library and its main functions and features, as well as various NLP-related tasks. Below are the major points that we are going to discuss throughout this post.\xa0', 'Let’s first discuss the Gensim library.', 'Gensim is open-source software that performs unsupervised topic modelling and natural language processing using modern statistical machine learning. Gensim is written in Python and Cython for performance. It is designed to handle large text collections using data streaming and incremental online algorithms, which sets it apart from most other machine learning software packages that are only designed for in-memory processing.\xa0', 'Gensim is not an all-encompassing NLP research library (like NLTK); rather, it is a mature, targeted, and efficient collection of NLP tools for subject modelling.\xa0 It also includes tools for loading pre-trained word embeddings in a variety of formats, as well as using and querying a loaded embedding.', 'Following are some of the features of the gensim.', 'Gensim provides efficient multicore implementations of common techniques including Latent Semantic Analysis (LSA), Latent Dirichlet Allocation (LDA), Random Projections (RP), and Hierarchical Dirichlet Process to speed up processing and retrieval on machine clusters (HDP).', 'Using its incremental online training algorithms, Gensim can easily process massive and web-scale corpora. It is scalable since there is no need for the entire input corpus to be fully stored in Random Access Memory (RAM) at any given time. In other words, regardless of the size of the corpus, all of its methods are memory-independent.', 'Gensim is a strong system that has been used in a variety of systems by a variety of people. Our own input corpus or data stream can be easily plugged in. It’s also simple to add other Vector Space Algorithms to it.', 'In this section, we’ll address some of the basic NLP tasks by using Gensim. Let’s first start with creating the dictionary.\xa0', 'Gensim requires that words (aka tokens) be translated to unique ids in order to work on text documents. To accomplish this, Gensim allows you to create a Dictionary object that maps each word to a unique id. We may do this by transforming our text/sentences to a list of words and passing it to the corpora.Dictionary() method.\xa0', 'In the following part, we’ll look at how to really do this. The dictionary object is often used to generate a Corpus of ‘bag of words.’ This Dictionary, as well as the bag-of-words (Corpus), are utilized as inputs to Gensim’s topic modelling and other models.', 'Here is the snippet that creates the dictionary for a given text.', 'The Corpus is the next important item to learn if you want to use gensim effectively (a Bag of Words). It is a corpus object that contains both the word id and the frequency with which it appears in each document.\xa0', 'To create a bag of word corpus, all that is required is to feed the tokenized list of words to the Dictionary after it has been updated. doc2bow(). To generate BOW, we’ll continue from the tokenized text from the previous example.', 'The (0, 1) in line 1 indicates that the id=0 word appears just once in the first sentence. Similarly, the (10, 1) in the third list item indicates that the word with the id 10 appears in the third phrase once. And so forth.', 'Certain words in paragraphs invariably appear in pairs (bigram) or in groups of threes (trigram). Because the two terms when joined make the actual entity. Forming bigrams and trigrams from phrases is critical, especially when working with bag-of-words models. It’s simple and quick with Gensim’s Phrases model. Because the built Phrases model supports indexing, simply send the original text (list) to the built Phrases model to generate the bigrams.', 'Like the regular corpus model, the Term Frequency – Inverse Document Frequency (TF-IDF) model reduces the weight of tokens (words) that appear frequently across texts. Tf-Idf is calculated by dividing a local component, such as term frequency (TF), by a global component, such as inverse document frequency (IDF), and then normalizing the result to unit length. As a result, phrases that appear frequently in publications will receive less weight.\xa0', 'There are various formula modifications for TF and IDF. Below is the way by which we can obtain the TF-IDF matrix. The blow snippets first obtain the frequency given by the BOW and later by the TF-IDF.', 'Now moving with TF-IDF, we just need to fit the model and access the weights by loops and conditions for each word.\xa0', 'Here is the output.']","'text = [\n   ""Gensim is an open-source library for""
\n   ""unsupervised topic modeling and"",\n   ""natural language processing.""\n]\n# get the separate words\ntext_tokens = [[tok for tok in doc.split()] for doc in text]\n# create dictionary\ndict_ = corpora.Dictionary(text_tokens)\n# get the tkens and ids\npprint(dict_.token2id)\n', '# tokens\ntext_tokens = [[tok for tok in doc.split()] for doc in text]\n# create dict\ndict_ = corpora.Dictionary()\n#BOW\nBoW_corpus = [dict_.doc2bow(doc, allow_update=True) for doc in text_tokens]\npprint(BoW_corpus)\n', 'from gensim.models.phrases import Phrases\n# Build the bigram models\nbigram = gensim.models.phrases.Phrases(text_tokens, min_count=3, threshold=10)\n#Construct bigram\npprint(bigram[text_tokens[0]])\n', 'from gensim.utils import simple_preprocess\nfrom gensim import models\nimport numpy as np\n# data to be processed\ndoc = [\n   ""Gensim is an open-source library for  "",\n   ""unsupervised topic modeling and"",\n   ""natural language processing.""]\n \n# Create the Dictionary and Corpus\nmydict = corpora.Dictionary([simple_preprocess(line) for line in doc])\ncorpus = [mydict.doc2bow(simple_preprocess(line)) for line in doc]\n \n# Show the Word Weights in Corpus\nfor doc in corpus:\n    print([[mydict[id], freq] for id, freq in doc])\n', ""# Create the TF-IDF model\ntfidf = models.TfidfModel(corpus, smartirs='ntc')\n \n# Show the TF-IDF weights\nfor doc in tfidf[corpus]:\n    print([[mydict[id], np.around(freq, decimals=2)] for id, freq in doc])"""
115,for_explainability_of_transformers/,https://analyticsindiamag.com/a-guide-to-ecco-python-based-tool-for-explainability-of-transformers/,"['Accountability is required for any decision-making tool in an organization. Machine learning models are already being used to automate time-consuming administrative tasks and to make complex business decisions. To ensure proper security of the model and business decisions, scientists and engineers must understand the inner mechanics of their models, which is commonly referred to as a black box. This is no longer the case, as various tools, such as ELI5, are available to track the inner mechanics of the model. In this article, we’ll look at how to explain the inner workings of language models like transformers using a toolbox called ECCO. The main points to be covered in this article are listed below.', 'Let’s start the discussion by understanding the explainability of machine learning models.', 'Explainability in machine learning refers to the process of explaining a machine learning model’s decision to a human. The term “model explainability” refers to the ability of a human to understand an algorithm’s decision or output. It’s the process of deciphering the reasoning behind a machine learning model’s decisions and outcomes. With ‘black box’ machine learning models, which develop and learn directly from data without human supervision or guidance, this is an important concept to understand.', 'A human developer would traditionally write the code for a system or model. The system evolves from the data with machine learning. Machine learning will be used to improve the algorithm’s ability to perform a specific task or action by learning from data. Because the underlying functionality of the machine learning model was developed by the system itself, it can be difficult to understand why the system made a particular decision once it is deployed.', 'Machine learning models are used to classify new data or predict trends by learning relationships between input and output data. The model will identify these patterns and relationships within the dataset. This means that the deployed model will make decisions based on patterns and relationships that human developers may not be aware of. The explainability process aids human specialists in comprehending the decision’s algorithm. After that, the model can be explained to non-technical stakeholders.', 'Machine learning explainability can be achieved using a variety of tools and techniques that vary in approach and machine learning model type. Traditional machine learning models may be simpler to comprehend and explain, but more complex models, such as deep neural networks, can be extremely difficult to grasp.', 'When machine learning has a negative impact on business profits, it earns a bad reputation. This is frequently the result of a misalignment between the data science and business teams. There are a few areas where Explainability heals based on this, such as,', 'Understanding how your models make decisions reveals previously unknown vulnerabilities and flaws. Control is simple with these insights. When applied across all models in production, the ability to quickly identify and correct mistakes in low-risk situations adds up.', 'In high-risk industries like healthcare and finance, trust is critical. Before ML solutions can be used and trusted, all stakeholders must have a thorough understanding of what the model does. If you claim that your model is better at making decisions and detecting patterns than humans, you must be able to back it up with evidence. Experts in the field are understandably skeptical of any technology that claims to be able to see more than they can.', 'When a model makes a bad or rogue decision, it’s critical to understand the factors that led to that decision, as well as who is to blame for the failure, in order to avoid similar issues in the future. Data science teams can use explainability to give organizations more control over AI tools.', 'The terms explainability and interpretability are frequently used interchangeably in the disciplines of machine learning and artificial intelligence. While they are very similar, it is instructive to note the distinctions, if only to get a sense of how tough things may become as you advance deeper into machine learning systems.', 'The degree to which a cause and effect may be observed inside a system is referred to as interpretability. To put it another way, it’s your capacity to predict what will happen if the input or computational parameters are changed.\xa0', 'Explainability, on the other hand, relates to how well a machine’s or deep learning system’s internal mechanics can be articulated in human terms. It’s easy to ignore the subtle contrast between interpretability and comprehension, but consider this: interpretability is the ability to comprehend mechanics without necessarily knowing why. The ability to explain what is happening in depth is referred to as explainability.', 'Many recent advances in NLP have been powered by the transformer architecture, and until now, we had no idea why Transformer-based NLP models have been so successful in recent years. To improve the transparency of Transformer-based language models, ECCO an open-source library for the explainability of Transformer-based NLP models was created.', 'ECCO offers tools and interactive explorable explanations to help with the examination and intuition of terms, such as Input Saliency, which visualizes the token importance for a given sentence. Hidden State Evaluation is applied to all layers of a model to determine the role of each layer. Non-negative matrix factorization of neuron activations was used to uncover underlying patterns of neuron firings, revealing firing patterns of linguistic properties of input tokens, and neuron activation tell us how a group of neurons spikes or responds while making a prediction.', 'Now in this section, we will take a look at how ECCO can be used to understand the working of various transformers models while predicting the sequence-based output. Majorly we’ll see how weights are distributed at the final layer while predicting the next sequence and will also analyze all layers of the selected model.\xa0', 'To start with ECCO we can install it using the pip command as! pip install Ecco\xa0', 'And also make sure you have also installed the Pytorch.', 'First, we will start with generating a single token by passing a random string to the model. The GPT2 is used due to its superiority for generating the next sequence as a human does. The below code shows how we load the pre-trained model and how to use it for the prediction. The below generate method takes an input sequence and additionally there we can pass how many tokens we need to generate from the model by specifying generate= some number.\xa0\xa0\xa0', 'While initializing the pre-trained model we set activation=True that we capture all the firing status of the neurons.\xa0\xa0\xa0', 'Now we’ll generate a token using the generate method.', 'From the method, token 6 and 5 is generated as first and of respectively.', 'The model has a total of 6 decoder layers and the last layer is the decision layer where the appropriate token is chosen.', 'Now we will observe the status of the last layers and see what are the top 15 tokens that the model has considered. Here we observe the status for position / token 6 and this can be achieved by output.layer_predictions method as below.\xa0\xa0', 'output.layer_predictions(position=6, layer=5, topk=15)', 'As we can see, the token first comes up with a higher contribution.\xa0', 'Similarly, we can check how different tokens would perform at the output layer. This can be done by explicitly passing the token numbers inside the method ranking_watch. However, tokens can be easily generated by using the pre-trained model that we have selected initially.\xa0 \xa0', 'Below are the generated token IDs.', 'Now we’ll supply these IDs to see the rankings.', 'output.rankings_watch(watch=[262, 717, 621], position=6)', 'At the decision layer, we can see the first rank is achieved by token first and the rest not even closer to it. Thus we can say the model has correctly identified the next token and did assign proper weights for possible tokens.']","'output.layer_predictions(position=6
 layer=5, topk=15)', 'output.rankings_watch(watch=[262, 717, 621], position=6)'"
116,perceiverio_for_text_classification/,https://analyticsindiamag.com/hands-on-guide-to-hugging-face-perceiverio-for-text-classification/,"['Nowadays, most deep learning models are highly optimized for a specific type of dataset. Computer vision and audio analysis can not use architectures that are good at processing textual data. This level of specialization naturally influences the development of models that are highly specialized in one task and unable to adapt to other tasks. So, in contrast to the General Purpose model, we will talk about PerceiverIO, which is designed to address a wide range of tasks with a single architecture. The following are the main points to be discussed in this article.', 'Let’s start the discussion by understanding the PerceiverIO.', 'A perceiver is a transformer that can handle non-textual data like images, sounds, and video, as well as spatial data. Other significant systems that came before Perceiver, such as BERT and GPT-3, are based on transformers. It uses an asymmetric attention technique to condense inputs into a latent bottleneck, allowing it to learn from a great amount of disparate data. On classification challenges, Perceiver matches or outperforms specialized models.', 'The perceiver is free of modality-specific components. It lacks components dedicated to handling photos, text, or audio, for example. It can also handle several associated input streams of varying sorts. It takes advantage of a small number of latent units to create an attention bottleneck through which inputs must pass. One advantage is that it eliminates the quadratic scaling issue that plagued early transformers. For each modality, specialized feature extractors were employed previously.', 'Perceiver IO can query the model’s latent space in a variety of ways to generate outputs of any size and semantics. It excels at activities that need structured output spaces, such as natural language and visual comprehension and multitasking. Perceiver IO matches a Transformer-based BERT baseline without the need for input tokenization on the GLUE language benchmark and achieves state-of-the-art performance on Sintel optical flow estimation.', 'The latent array is attended to using a specific output query associated with that particular output to produce outputs. To predict optical flow on a single pixel, for example, a query would use the pixel’s XY coordinates along with an optical flow task embedding to generate a single flow vector. It’s a spin-off of the encoder/decoder architecture seen in other projects.', 'The Perceiver IO model is based on the Perceiver architecture, which achieves cross-domain generality by assuming a simple 2D byte array as input: a set of elements (which could be pixels or patches in vision, characters or words in a language or some form of learned or unlearned embedding), each described by a feature vector. The model then uses Transformer-style attention to encode information about the input array using a smaller number of latent feature vectors, followed by iterative processing and a final aggregation down to a category label.', 'HuggingFace Transformers’ PerceiverModel class serves as the foundation for all Perceiver variants. To initialize a PerceiverModel, three further instances can be specified – a preprocessor, a decoder, and a postprocessor.', 'A preprocessor is optionally used to preprocess the inputs (which might be any modality or a mix of modalities). The preprocessed inputs are then utilized to execute a cross-attention operation utilizing the latent variables of the Perceiver encoder.\xa0', 'Source', 'Perceiver IO is a domain-agnostic process that maps arbitrary input arrays to arbitrary output arrays. The majority of the computation takes place in a latent space that is typically smaller than the inputs and outputs, making the process computationally tractable even when the inputs and outputs are very large.', 'In this technique (Referring to the above architecture), the latent variables create queries (Q), whilst the preprocessed inputs generate keys and values (KV). Following this, the Perceiver encoder updates the latent embeddings with a (repeatable) block of self-attention layers. Finally, the encoder will create a shape tensor (batch size, num latents, d latents) containing the latents’ most recently concealed states. Then there’s an optional decoder, which may be used to turn the final concealed states of the latent into something more helpful, like classification logits. This is performed by a cross-attention operation in which trainable embeddings create queries (Q) and latent generate keys and values (KV).', 'In this section, we will see how perceiver can be used to do the text classification. Now let’s install the Transformer and datasets module of Huggingface.', 'Next, we will prepare the data from the module. The dataset is about IMDB movie reviews and we are using a chunk of it. Later after loading the dataset, we will make it handy when doing the inference.\xa0\xa0', 'In this step, we will preprocess the dataset for tokenization. For that, we are using PerceiverTokenizer on both train and test datasets.', 'We are going to use PyTorch for further modelling and for that we need to set the format of our data compatible with the PyTorch.', 'Next, we will define and train the model.', 'Now, let’s do the inference with the model.']","'! pip install -q git+https://github.com/huggingface/transformers.git\n! pip install -q datasets'
 'from datasets import load_dataset\n# load the dataset\ntrain_ds, test_ds = load_dataset(""imdb"", split=[\'train[:100]+train[-100:]\', \'test[:5]+test[-5:]\'])\n\n# making the dataset handy\nlabels = train_ds.features[\'label\'].names\nprint(labels)\n \nid2label = {idx:label for idx, label in enumerate(labels)}\nlabel2id = {label:idx for idx, label in enumerate(labels)}\nprint(id2label)', '# Tikenization\nfrom transformers import PerceiverTokenizer\n \ntokenizer = PerceiverTokenizer.from_pretrained(""deepmind/language-perceiver"")\n \ntrain_ds = train_ds.map(lambda examples: tokenizer(examples[\'text\'], padding=""max_length"", truncation=True),\n                        batched=True)\ntest_ds = test_ds.map(lambda examples: tokenizer(examples[\'text\'], padding=""max_length"", truncation=True),\n                      batched=True)\n', '# campatible with torch\nfrom torch.utils.data import DataLoader\n \ntrain_ds.set_format(type=""torch"", columns=[\'input_ids\', \'attention_mask\', \'label\'])\ntest_ds.set_format(type=""torch"", columns=[\'input_ids\', \'attention_mask\', \'label\'])\n \ntrain_dataloader = DataLoader(train_ds, batch_size=4, shuffle=True)\ntest_dataloader = DataLoader(test_ds, batch_size=4)\n', 'from transformers import PerceiverForSequenceClassification\nimport torch\nfrom transformers import AdamW\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import accuracy_score\n \n# Define model\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\nmodel = PerceiverForSequenceClassification.from_pretrained(""deepmind/language-perceiver"",\n                                                               num_labels=2,\n                                                               id2label=id2label,\n                                                               label2id=label2id)\nmodel.to(device)\n \n# Train the model\noptimizer = AdamW(model.parameters(), lr=5e-5)\nmodel.train()\nfor epoch in range(20):  # loop over the dataset multiple times\n    print(""Epoch:"", epoch)\n    for batch in tqdm(train_dataloader):\n         # get the inputs; \n         inputs = batch[""input_ids""].to(device)\n         attention_mask = batch[""attention_mask""].to(device)\n         labels = batch[""label""].to(device)\n \n         # zero the parameter gradients\n         optimizer.zero_grad()\n \n         # forward + backward + optimize\n         outputs = model(inputs=inputs, attention_mask=attention_mask, labels=labels)\n         loss = outputs.loss\n         loss.backward()\n         optimizer.step()\n \n         # evaluate\n         predictions = outputs.logits.argmax(-1).cpu().detach().numpy()\n         accuracy = accuracy_score(y_true=batch[""label""].numpy(), y_pred=predictions)\n         print(f""Loss: {loss.item()}, Accuracy: {accuracy}"")\n', 'text = ""I loved this epic movie, the multiverse concept is mind-blowing and a bit confusing.""\n \ninput_ids = tokenizer(text, return_tensors=""pt"").input_ids\n \n# Forward pass\noutputs = model(inputs=input_ids.to(device))\nlogits = outputs.logits \npredicted_class_idx = logits.argmax(-1).item()\n \nprint(""Predicted:"", model.config.id2label[predicted_class_idx])'"
117,based_spelling_correction_toolkit/,https://analyticsindiamag.com/neuspell-a-neural-net-based-spelling-correction-toolkit/,"['Spell check features, or spell checkers, are software applications that check words against a digital dictionary to ensure they are correctly spelled. Words that are identified as misspelled by the spell checker are usually highlighted or underlined. Among the numerous spelling checking tools and applications available, this post will focus on NeuSpell, a neural network, and Python-based spelling checking toolbox. The following are the key points that will be addressed in this article.', 'Let’s start the discussion by understanding how various tools work for spell correction.', 'When presenting a document to clients, professors, or any other audience, saying something smart and valuable is crucial. However, if your content is riddled with typos, misspellings, and errors, most people are likely to overlook it. Perfect copy is a sign of professionalism, and most businesses expect nothing less from their documentation. A spell checker program or the spell checking functions provided by a word processor are two useful tools that computer users can use to edit their documents.', 'The most common type of error in written text is misspelt words. As a result, spell checkers are commonplace, appearing in a variety of applications such as search engines, productivity and collaboration tools, messaging platforms, and so on. Many high-performing spelling correction systems, on the other hand, are developed by businesses and trained on massive amounts of proprietary user data.\xa0', 'Many freely available off-the-shelf correctors, such as Enchant, GNU Aspell, and JamSpell, on the other hand, do not make effective use of the misspelt word’s context. For example, based on the context, they fail to distinguish between thaught and taught or thought: “Who thaught you calculus?” vs. “I never imagined I’d be given the fellowship.”', 'In their paper, Sai Muralidhar et al. propose a spelling checker toolkit called NeuSpell. They show a spelling correction toolkit that consists of several neural models that accurately capture context around misspellings. They use several text noising strategies to train these neural spell correctors by curating synthetic training data for spelling correction in context.\xa0', 'For word-level noising, these strategies use a lookup table, and for character-level noising, they use a context-based character-level confusion dictionary. Harvest isolated misspelling-correction pairs from various publicly available sources to populate this lookup table and confusion matrix.', 'NeuSpell is an open-source toolkit for English spelling correction. This toolkit includes ten different models that are tested against naturally occurring misspellings from a variety of sources. When models are trained on our synthetic examples, correction rates improve by 9% (absolute) when compared to training on randomly sampled character perturbations.\xa0', 'The correction rate is increased by another 3% when richer contextual representations are used. This toolkit allows users to use proposed and existing spelling correction systems through a unified command line and a web interface.', 'This toolkit includes ten different spelling correction models, (i) including two commercially available nonneural models, (ii) four published neural models for spelling correction, and (iii) four of our extensions. The following are the details of the first six systems:', 'It uses semi-character representations fed through a bi-LSTM network to correct misspelt words. The semi-character representations combine one-hot embeddings for the first, last, and bag of internal characters.', 'The model creates word representations by feeding each character into a bi-LSTM. These representations are then fed into a second biLSTM that has been trained to predict the corrective action.', 'This model, like the previous one, uses a convolutional network to create word-level representations from individual characters.', 'A pre-trained transformer network is used in the model. The word representations are obtained by averaging the sub-word representations, which are then fed to a classifier to predict its correction.', 'To score candidate words, it employs a combination of the Metaphone phonetic algorithm, Ispell’s near-miss strategy, and a weighted edit distance metric.', 'They enhanced the SC-LSTM model with deep contextual representations from pre-trained ELMo and BERT to better capture the context around a misspelt token. They append them to semi-character embeddings before feeding them to the biLSTM or to the biLSTM’s output because the best point to integrate such embeddings varies by task. Our toolkit currently includes four such trained models: ELMo/BERT coupled with a semi-character-based bi-LSTM model at input/output.', 'In NeuSpell, neural models are trained by treating spelling correction as a sequence labelling task, with a correct word labelled as itself and its correction labelled as to its correction. The abbreviation UNK refers to labels that aren’t in the dictionary. A softmax layer is used to train models to output a probability distribution over a finite vocabulary for each word in the input text sequence.', 'During training, they used 50,100,100,100 sized convolution filters with lengths of 2,3,4,5 in CNNs and set the hidden size of the bi-LSTM network in all models to 512. The bi-LSTM outputs were dropped out at 0.4, and the models were trained using cross-entropy loss.\xa0', 'For models with a BERT component, we used the BertAdam optimizer, and for the rest, we used the Adam optimizer. The default parameter settings are used with these optimizers. I used a batch size of 32 examples and trained for 3 epochs of patience.', 'Replace UNK predictions with their corresponding input words during inference, then evaluate the results. The accuracy (percentage of correct words among all words) and word correction rate of the models are then assessed (percentage of misspelt tokens corrected).\xa0', 'To use ELMo and BERT, the libraries AllenNLP and Huggingface were used. The Pytorch library is used to implement all of the neural models in this toolkit, and they are compatible with both CPU and GPU environments.', 'Now let’s see how we can implement NeuSpell.', 'To move further we need to install the NeuSpell from its official repository by cloning and installing the dependencies from the requirement.txt file as mentioned in the repository or we can directly install it by using pip command as pip install neuspell.', 'Import all the dependencies\xa0', 'Now instantiate the BertChecker Class and download the pre-trained model.', 'Now let’s take some samples of incorrectly spelled sentences and see how the model can correct them.\xa0', 'checker_bert.correct(""I luk foward to receving your reply"")', 'And here is the output.', 'Let’s take another example,', 'checker_bert.correct_strings([""Thee wors are often used together. You can go to the defition of spellig or the defintion of mistae. Or, see other combintions with mistke."", ])', 'The beautiful thing that I observed from this toolkit is that we can even pass our text file directly and it can return the cleaned version in the form of text file just like in the above example by using just a single line of code as below.', 'checker_bert.correct_from_file(src=""/content/History_100.txt"")', 'The above code returns a clean_version.txt in the local directory.', 'Further from this step, we can also evaluate our text files. For that under the checker_bert.evaluate() we need to pass the original clean file and the corrupted file as shown below.', 'checker_bert.evaluate(clean_file=""clean_version.txt"", corrupt_file=""History_100.txt"")']","'checker_bert.correct(""I luk foward to receving your reply"")'
 'checker_bert.correct_strings([""Thee wors are often used together. You can go to the defition of spellig or the defintion of mistae. Or, see other combintions with mistke."", ])', 'checker_bert.correct_from_file(src=""/content/History_100.txt"")', 'clean_version.txt', 'checker_bert.evaluate()', 'checker_bert.evaluate(clean_file=""clean_version.txt"", corrupt_file=""History_100.txt"")'"
118,nlp_by_hugging_face/,https://analyticsindiamag.com/datasets-a-community-library-for-nlp-by-hugging-face/,"['There has been tremendous research in NLP applications since its implementation. Nowadays we have powerful tools such as BERT which facilitates a robust NLP model on the fly. When preparing for such models we often spend plenty of time gathering appropriate data and for that, we have to go through various repositories such as Kaggle, UCI ML, etc. So is there any way where we can access a variety of such data in one place?\xa0 The answer is yes. A few months back, Hugging Face introduced its Community library called Datasets which facilitates more than 600 publicly available datasets in a standard format in 467 different languages. So in this post, we are going to discuss this framework and practically see how we can leverage it. The major points to be discussed are listed below.', 'Let’s start the discussion by understanding the need for this framework.', 'The size, variety, and number of publicly available NLP (Natural Language Processing) datasets have grown rapidly as researchers propose new goals, larger models, and unique benchmarks. For assessment and benchmarking, curated datasets are used; supervised datasets are used for training and fine-tuning models, and massive unsupervised datasets are required for pretraining and language modelling. Each dataset type has a different scale, granularity, and structure, in addition to the annotation approach.', 'In the past, new dataset paradigms have been critical in propelling NLP forward. Today’s NLP systems consist of a pipeline that includes a wide range of datasets with varying dimensions and levels of annotation. Several datasets are used for pretraining, fine-tuning, and benchmarking. As a result, the number of datasets available to the NLP community has skyrocketed. As the number of datasets grows, significant issues such as interface standardization, versioning, and documentation arise.', 'Without having to use multiple interfaces, one should be able to work with a variety of datasets. Furthermore, a group of people working on the same dataset should be aware that they are all using the same version. As a result of this magnitude, interfaces should not have to change.', 'This is where a Dataset comes into play. Datasets is a modern NLP community library that was created to help the NLP community. Datasets aim to standardize end-user interfaces, versioning, and documentation while also providing a lightweight front-end that can handle small datasets as well as large internet corpora.\xa0', 'The library was built with a distributed, community-driven approach to dataset addition and usage documentation in mind. The library now has over 650 unique datasets, over 250 contributors, and has supported many original cross-dataset research initiatives and shared tasks after a year of hard work.', 'Datasets is a community library dedicated to addressing data management and access issues while also promoting community culture and norms. The project has hundreds of contributors from all over the world, and each dataset is tagged and documented. Each dataset is expected to be in a standard tabular format that can be versioned and cited; datasets are computed- and memory-efficient by default, and they work well with tokenization and featurization.', 'The users can access the dataset by simply referring to a global variable. Each dataset has its own feature schema and metadata. For every dataset users need not load the whole dataset, Datasets has provided 3 folds for nearly all datasets, and users can load them separately and can access them by indexing. Additionally, we can apply various pre-processing steps directly to the corpus.\xa0\xa0', 'Datasets have divided all its procedures into simple four steps as follows,', 'The underlying raw datasets are not hosted by Datasets; instead, it uses a distributed approach to access hosted data from the original authors. Each dataset has a builder module contributed by the community. The builder module is in charge of converting unstructured data, such as text or CSV, into a standardized dataset interface.', 'Internally, each built dataset is represented as a table with typed columns. A variety of common and NLP-targeted dataset types are available in the Dataset type system. Aside from atomic values (ints, floats, strings, and binary blobs) and JSON-like dicts and lists, the library also includes named categorical class labels, sequences, paired translations, and higher dimension arrays for images, videos, or waveforms.', 'Datasets is built on Apache Arrow, a cross-language columnar data framework. Arrow includes a local caching system that allows datasets to be backed up by a memory-mapped on-disk cache for the quick lookup. This architecture allows large datasets to be used on machines with limited device memory. Arrow also allows for copy-free handoffs to popular machine learning tools such as NumPy, Pandas, Torch, and TensorFlow.', 'The library provides access to typed data with minimal preprocessing when downloaded. It includes sorting, shuffling, splitting, and filtering functions for manipulating datasets. It has a powerful map function for complex manipulations that supports arbitrary Python functions for creating new in-memory tables. The map can be run in batched, multi-process mode to apply processing in parallel to large datasets. Data processed by the same function is also cached automatically between sessions.', 'When you request a dataset, it is downloaded from its original host. This triggers the execution of dataset-specific builder code, which converts the text into a typed tabular format that conforms to the feature schema and caches the table. The user is given a memory-mapped table. The user can run any vectorized code and cache the results to perform additional data processing, such as tokenization.', 'Here in this section we practically see how we can leverage Datasets to Build NLP-related applications. In this implementation first, we will see how we can preview and load the dataset, pre-process it, and make it compatible for modelling it. Let’s start with installing and importing the dependencies.\xa0', 'It is often useful to quickly obtain all relevant information about a dataset before taking the time to download it. The datasets.load dataset builder() method allows you to inspect a dataset’s attributes without having to download it.\xa0\xa0', 'Once you’ve found the dataset you want, load it with datasets in a single line. With load_dataset(), you can see the entire schema by simply printing the variable. Or even you can convert it into a CSV version as shown below.', 'We’ve seen how to load a dataset from the Hugging Face Hub and access the data it contains so far. Now we’ll tokenize our data and use a framework like TensorFlow to analyze it. By default, all dataset columns are returned as Python objects. The columns are formatted to be compatible with TensorFlow types.', 'To begin, let’s take a look at tokenization. Tokenization is the process of separating the text into individual words known as tokens. Tokens are converted into numbers, which the model uses as input. Bring in a tokenizer. To ensure that the text is consistently split, we must use the tokenizer associated with the model. Because you’re using the BERT model in this example, load the BERT tokenizer.', 'Tensorflow and Pytorch are two widely used frameworks for model building. We’ll continue with the Tensorflow example. To wrap the dataset with tf.data, we can use to_tf_dataset(). This indicates a tf.data. The dataset object can be iterated over to produce batches of data, which can then be passed directly to methods such as model.fit(). to_tf_dataset() takes a number of arguments such as,', 'Now we have created a dataset that is ready to use in the training loop for Tensorflow models. Let’s take a look at it.', 'next(iter(train_dataset))\xa0']",'next(iter(train_dataset))\xa0'
119,a_sentence_using_textblob/,https://analyticsindiamag.com/how-to-obtain-a-sentiment-score-for-a-sentence-using-textblob/,"['When a product owner or service provider wants to know the feedback of the users, sentiment analysis gives a strong indication of how the users are satisfied with the product or service. Most of the feedback from the customers remains available as reviewed comments so there is always a need to quickly analyze the comments or sentences and find the sentiments of the customers. For this purpose, Python offers many features to quickly analyze a comment or sentence and find the sentiment and TextBlob is one of these. In this post, we will understand how the sentiment score can be obtained for a sentence using TextBlob. Along with this, we will also discuss the significance of sentiment analysis, its applications and different python packages used for this task. The major points to be discussed in this article are listed below.\xa0', 'Let’s start the discussion by understanding what sentiment analysis actually means.\xa0', 'The systematic identification, extraction, measurement, and study of affective states and subjective information utilizing natural language processing, text analysis, computational linguistics, and biometrics are known as sentiment analysis (also known as opinion mining or emotion AI). Sentiment analysis is often utilized in client-facing materials like reviews and survey responses, as well as in online and social media and healthcare papers for a variety of objectives ranging from marketing to customer service to clinical medicine.', 'A core problem in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level—whether the transmitted opinion in a document, sentence or entity feature/aspect is positive, negative, or neutral. Happiness, anger, contempt, sadness, fear, and surprise are all considered in advanced sentiment categorization, sometimes known as “beyond polarity.”', 'It’s essentially a multiclass text classification text in which the input text is categorized as a positive, neutral, or negative emotion. Depending on the nature of the training dataset, the number of classes can vary. It’s commonly expressed as a binary classification problem, with 1 denoting positive sentiment and 0 denoting negative sentiment.', 'Aspect-based sentiment analysis, grading sentiment analysis (positive, negative, neutral), multilingual sentiment analysis, and emotion detection are some of the other types of sentiment analysis.', 'The subjectivity/objectivity activity is typically defined as categorizing a text (usually a sentence) into one of two categories: objective or subjective. This problem can be more challenging than polarity categorization in some cases. Words and phrases’ subjectivity may be affected by their context, and an objective document may contain subjective sentences (for example, a news piece quoting people’s thoughts).', 'Sentiment analysis has several applications, including evaluating user reviews, tweet sentiment, and so on. Let’s have a look at a few of them:', 'Analyzing Movie Reviews: Analyzing online movie reviews to gather audience insights into the film.', 'News sentiment Analysis: The technique of analyzing news sentiments for a certain company to gain insights. Examine the emotions expressed in Facebook postings, Twitter tweets, and other social media posts. Online food reviews: analyzing user comments to discover how people feel about food.', 'E-Commerce and Social Networking: Users can submit text reviews, comments, or feedback on things on many social networking platforms or e-commerce websites. These user-generated texts are a great source of user sentiment opinions on a wide range of products and items. For an item, such language might potentially expose both the connected aspects of the item as well as the users’ opinions on each feature.', 'The NLTK platform includes interfaces to over fifty corpora and lexical sources that have been mapped to machine learning techniques, as well as a powerful set of parsers and utilities.', 'Apart from sentiment analysis, the NLTK algorithms support named entity recognition, tokenization, part-of-speech (POS), and subject segmentation. NLTK also has the most extensive language support of any of the libraries featured here, as well as a good range of third-party extensions.', 'Remember that NLTK was developed by and for academic researchers. It wasn’t built to support NLP models in a real-world setting. Even the how-tos are lacking in documentation. There is also no 64-bit binary; you must install the 32-bit edition of Python in order to use it. Finally, while NLTK isn’t the fastest library, it can be made faster with parallel processing.', 'The SpaCy Python library, which claims to provide “industrial-strength natural language processing,” is interesting for sentiment analysis applications that need to be performant at scale or can benefit from a strongly object-oriented programming style.', 'SpaCy is a multi-platform environment based on Cython, a Python superset that allows the creation of fast-running C-based Python frameworks. As a result, according to academics, SpaCy is currently the fastest-running solution.\xa0', 'In contrast to NLTK, SpaCy focuses on industrial application and maintains a limited effective toolkit, with updates replacing older versions and tools. Named entity recognition, part-of-speech (POS) tagging, and classification are all covered by SpaCy’s pre-built models.', 'Stanford CoreNLP is a suite of highly expandable Java libraries for natural language processing that uses wrappers to access Python. It is platform-independent, feature-rich, and efficient, and it is presently in use in many production systems. Non-English languages are well supported by CoreNLP in NLP flows. Arabic, Chinese, French, German, and Spanish are some of the current language models.', 'The suite is updated on a regular basis and includes APIs for a range of programming languages. It has an annotator for arbitrary texts that is both efficient and stable, as well as integration with annotation pipelines. NLTK modules are supported by some CoreNLP components.', 'CoreNLP includes a built-in sentiment analysis tool with its own set of third-party resources. Stanford offers a live demonstration that includes the source code for a sentiment analysis solution.', 'Around 2010, two students from the Czech Republic’s Natural Language Processing Laboratory built Gensim, which has since grown into one of the most scalable and sophisticated NLP solutions. Gensim, like NLTK, is comprehensive and powerful enough to be utilized as a remote resource in bigger pipelines, such as phrase modelling or in conjunction with other frameworks like SpaCy and TextaCy.', 'Gensim is a popular program for document similarity and topic and vector space modelling. It’s also a great tool for dimensionality reduction and multi-label classification. Gensim, on the other hand, is primarily concerned with the efficient initial distillation of data from documents and word clouds.', 'Gensim supports Cython implementations, with processing times comparable to SpaCy depending on the job at hand. The project published a fresh set of optimizations in March 2019 that give significant speed improvements across a variety of functions.', 'The further section of this article is focused on how to assess or assign a sentiment score to a given corpus or given chunk of words using a python toolkit called TextBlob.\xa0', 'TextBlob is an appealing and relatively lightweight Python 2/3 toolkit for NLP and sentiment analysis development that offers improved ease of use and a less harsh learning curve.', 'The project has a more user-friendly interface than NLTK, and it also makes use of the Pattern web mining module from the University of Antwerp. Combining these resources makes it simple to transition between the powerful Pattern library and a pre-trained NLTK model, for example.', 'The integrated sentiment analysis function in TextBlob has two properties: subjectivity and polarity. The most prevalent ways to sentiment analysis with TextBlob are workflows with TextBlob and VADER (Valence Aware Dictionary and sEntiment Reasoner).', 'It’s not unexpected that TextBlob has few functional qualities that set it apart from its competitors, given its design and purpose. It’s powerful and feature-rich, but in terms of speed, it’s still reliant on external resources, none of which are particularly impressive.', 'When we use TextBlob to calculate the sentiment of a text, we get numeric values for polarity and subjectivity. The polarity numeric number indicates how negative or positive a sentence is. Subjectivity, on the other hand, refers to how objective or subjective a text is.TextBlob employs a sentiment-calculating algorithm, with each word in the lexicon being rated as follows:', 'When computing a sentiment for a single word, TextBlob employs the “averaging” technique, which is applied to polarity values to calculate a polarity score for a single word, and thus a similar procedure applies to every single word, resulting in a combined polarity for larger texts.', 'TextBlob understands negations as well, and polarity is doubled by -0.5.', 'TextBlob has an intriguing feature in that it handles modifiers, also known as intensifiers, which intensify the meaning of the text based on its pattern. TextBlob ignores polarity and subjectivity when a modifier word is included, instead of relying solely on intensity to compute the sentiment of the text.', 'In this section, we are going to score a set sentence according to their sentiments using TextBlob.\xa0\xa0', 'Let’s have a look at how the TextBlob library functions. The first line of code below contains the text example, while the second line prints the text. In the third line, the sentiment function is utilized, which returns two properties: polarity and subjectivity.\xa0', 'The polarity of the statement is 0.0 in the above report, suggesting that the sentiment is neutral. In our example, the output also includes the text’s subjectivity, which is 0.3. Subjectivity is a float with a value between 0 and 1. The closer the value is to one, the more likely the statement is to be a public opinion rather than a true piece of information, and vice versa. We now understand how the TextBlob library operates.', 'Now let’s try to apply this to the dataset. The dataset contains the food review of datasets. Labelled the sentiment 1 as positive and 0 as negative.', 'We will directly apply the TextBlob functionality by iterating the text using the lambda function as below.']","'! pip install textblob\nfrom textblob import TextBlob\n'
 ""data['calculated'] = data['Text'].apply(lambda x: (TextBlob(x).sentiment.polarity))\ndata.head()\n"""
120,for_processing_textual_data/,https://analyticsindiamag.com/lets-learn-textblob-quickstart-a-python-library-for-processing-textual-data/,"['Processing text in such a way to extract useful information from it known as text processing. It is the textual data analysis using different tools and techniques. In order to pass the text to a machine learning model, we need to process it to find out certain important information and the numerical features about the text.', 'Textblob is an open-source python library for processing textual data. It performs different operations on textual data such as noun phrase extraction, sentiment analysis, classification, translation, etc.\xa0', 'Textblob is built on top of NLTK and Pattern also it is very easy to use and can process the text in a few lines of code. Textblob can help you start with the NLP tasks.', 'In this article, we will explore textblob and learn about all of its major features with this Hands-on tutorials.\xa0', 'Textblob requires certain features from\xa0 NLTK, so we will start by installing both NLTK and Textblob using pip install nltk & pip install textblob.', 'We will import both NLTK and textblob, and we will download certain dependencies using NLTK.\xa0', 'from textblob import TextBlob', 'import nltk', ""nltk.download('punkt')"", ""nltk.download('averaged_perceptron_tagger')"", ""nltk.download('brown')"", 'We can use any text for this text processing tutorial. I have taken an article from today’s newspaper.\xa0', ""art = '''Among the 10 countries that have reported the highest number of case in the world, daily cases are still continuously rising in only two – India and Colombia.\xa0 Other than the US and Brazil, daily cases also appear hitting a plateau in Mexico (7th spot, 480,278 cases). Russia (4th, 892,654 cases), South Africa (5th, 563,598 cases), and Chile (9th, 375,044 cases). The remaining two – Spain (10th, 370,060 cases) and Peru (7th, 483,133 cases) – managed to control outbreaks once, but are now seeing a resurgence of cases. All caseloads are from the worldometers.info dashboard. To be sure, the global Covid-19 curve has flattened twice before — first, when the Chinese outbreak peaked and the contagion was yet to reach the West; the second, when cases dropped in Europe — however, it has risen again with more ferocity both times as the virus has spread to new regions.'''"", 'We will start with different techniques of text processing but before that, we need to pass the text to the TextBlob function.\xa0', 'blob = TextBlob(art)', 'Starting with some of the basic text processing functions like finding the tags and noun phrases.', 'Tags function is used to find the respective tags of the particular word which describes whether the word is a noun, adjective, etc.\xa0\xa0', 'blob.tags', 'Noun phrases function helps us find out the noun phrases in the text given.', 'blob.noun_phrases\xa0', 'Sentiment function is used to find out the polarity and subjectivity of the text. The polarity is used to check whether the text is positive or negative and subjectivity\xa0is used to check whether the text is objective or subjective.', 'blob.sentiment', 'We can use the function polarity and subjectivity to find their values individually\xa0also.', 'Words function split the text into words that are used in the text.', 'blob.words', 'Sentences function split the text into the sentences which are used to form the\xa0text.', 'blob.sentences', 'We can also find the polarity of all individual sentences using the polarity function mentioned above.', 'for sentence in blob.sentences:', '\xa0\xa0\xa0 print(sentence.sentiment.polarity)', 'We can select different words from our text and can singularize and pluralize them. Similarly, we can pass any word and convert it into a singular or plural form.\xa0', 'word_text = blob.words', 'word_text[3]', 'word_text[3].singularize()', 'word_text[4].pluralize()', 'Lemmatize function is used to find out the lemma for the word.', 'word_text[3].lemmatize()', 'Spell check function and correct function helps in checking and correcting the spelling mistakes in our sentence or word or article. ', 'sent = TextBlob(""Among the 10 countries that have reported the highest number\xa0 of case in the world"")', 'print(sent.correct())', 'from textblob import Word', ""w = Word('amog')"", 'w.spellcheck()', 'By default, Textblob uses Pattern’s parser. We will parse our text using the parser function.', 'blob.parse()', 'N-grams function returns a tuple of n successive words from a given text. You\xa0just need to pass the value of n in the n-gram function to decide the number of\xa0words in the n-gram.', 'blob.ngrams(n=5)', 'These are some of the text processing functions that are provided by textblob. We can use textblob for text processing as it is easy to use and has a lot of predefined functions.']","'from textblob import TextBlob'
 'import nltk', ""nltk.download('punkt')"", ""nltk.download('averaged_perceptron_tagger')"", ""nltk.download('brown')"", ""art = '''Among the 10 countries that have reported the highest number of case in the world, daily cases are still continuously rising in only two – India and Colombia.\xa0 Other than the US and Brazil, daily cases also appear hitting a plateau in Mexico (7th spot, 480,278 cases). Russia (4th, 892,654 cases), South Africa (5th, 563,598 cases), and Chile (9th, 375,044 cases). The remaining two – Spain (10th, 370,060 cases) and Peru (7th, 483,133 cases) – managed to control outbreaks once, but are now seeing a resurgence of cases. All caseloads are from the worldometers.info dashboard. To be sure, the global Covid-19 curve has flattened twice before — first, when the Chinese outbreak peaked and the contagion was yet to reach the West; the second, when cases dropped in Europe — however, it has risen again with more ferocity both times as the virus has spread to new regions.'''"", 'blob = TextBlob(art)', 'blob.tags', 'blob.noun_phrases\xa0', 'blob.sentiment', 'blob.words', 'blob.sentences', 'for sentence in blob.sentences:', 'print(sentence.sentiment.polarity)', 'word_text = blob.words', 'word_text[3]', 'word_text[3].singularize()', 'word_text[4].pluralize()', 'word_text[3].lemmatize()', 'sent = TextBlob(""Among the 10 countries that have reported the highest number\xa0 of case in the world"")', 'print(sent.correct())', 'from textblob import Word', ""w = Word('amog')"", 'w.spellcheck()', 'blob.parse()', 'blob.ngrams(n=5)'"
121,guide_to_opennre_toolkit/,https://analyticsindiamag.com/beginners-guide-to-opennre-toolkit/,"['Have you ever noticed that when we search for any sentence in a search engine and start filling up the widget, it shows the rest of the thing automatically or gives us the recommendation for the rest of the queries? How do they do these things? Here in the picture, one thing comes into the part relation extraction. Relation extraction models are specially made for completing the task of predicting attributes for entities and relationships between the words and sentences. For example, extracting entities from the sentence “Elon Musk founded Tesla” – “elon musk” “founded” “Tesla” to understand the sentence “Elon Musk is founder of Tesla” is similar to the first sentence. Thus, relation extraction is one of the key components of some of the natural language processing tasks.', 'Knowledge graph construction plays a huge part; we can find many relation facts by using relation extraction. Using those facts, we can expand the knowledge graph, a path for machine learning models to understand the human world. Also, instead of NLP, it can be used for question answering, recommendation systems, search engines, and summarization applications.\xa0', 'There are various types of data sets available for this type of modeling like DocRED, TACRED, ACE 2005. And also, various models and frameworks are already trained on them like COKE, SSAN etc. A basic structure of any simple architecture, which we call HRERE (Heterogeneous Representation for Neural Relation Extraction):', 'Image source', 'Roughly there can be four kinds of relation extraction tasks we can perform using different algorithms.', 'Image source.', 'Image source.', 'Image source', 'Image source', 'OpenNRE is an open-source toolkit to perform neural relation extraction. The following image can understand the basic algorithm of NRE.', 'Image source', 'OpenNRE provides a framework for the implementation of relation extraction models as an open-source and extensible toolkit. This package provides facilities to\xa0', 'In the package, there are various models which are trained on wiki80, wiki180, TACRED datasets. To know more about the datasets, readers can go through this link.\xa0 This package provides high performing models based on convolutional neural networks and BIRT algorithms.', 'Let’s get a quick start of the OpenNRE using google colab.', 'Before installing the toolkit, we will need to start a GPU for the notebook; this will help us provide speed to our programs. To start the GPU, we can directly go to the runtime button; after this, click on the change runtime type. Next, you will get the widget hardware accelerator, select the GPU\xa0 and click on the save.', 'Here in the colab we are going to clone the package; that’s why we need to mount our drive in the notebook. The below example will show you how to mount the drive on your runtime using an authorization code.', 'Now we can start our trial of the package:', 'We can clone the OpenNRE repository using the following command.', 'output:', 'To use the models we have cloned, we will need to direct the notebook to the folder where our package is available.', 'Installing the package using its requirement.txt file.', '!pip install -r requirements.txt', 'Now we are prepared to use OpenNRE.', 'Importing the OpenNRE.', 'import opennre', 'Loading a pre-trained model from the package mode with CNN encoders on Wiki180 datasets\xa0', ""model = opennre.get_model('wiki80_cnn_softmax')"", 'This will take a few minutes to load the model. After loading it, we will use the model using the inferred command for relation extraction.', ""model.infer({'text': 'He was the son of ashok, and grandson of rahul.', 'h': {'pos': (18, 46)}, 't': {'pos': (78, 91)}})"", 'Here we can see the model’s prediction is a child and the confidence score of about 32%; the score is low, but the prediction is almost satisfying because the model has told us the son is related to the child.', 'Let’s check for another model of the package.', 'Loading the model.', ""model = opennre.get_model('wiki80_bert_softmax')"", 'Here I have loaded a model from the package, which is made with the birt encoder trains on the wiki180 data.', 'Checking the performance of the model.', ""model.infer({'text': 'He was the son of ashok, and grandson of rahul.', 'h': {'pos': (18, 46)}, 't': {'pos': (78, 91)}})"", 'Here we can see that it has predicted the right again, and with high confidence it has told us that Rahul is a father of Ashok and Ashok is a father of him. And the confidence of the model is around 99% which is also very satisfying.', 'One more model is inbuilt with the package named wiki180_bertentity_softmax, which is also made with a BIRT encoder.', 'Loading the model:', ""model = opennre.get_model('wiki80_bertentity_softmax')"", 'Checking the performance of the model.', ""model.infer({'text': 'He was the son of ashok, and grandson of rahul.', 'h': {'pos': (18, 46)}, 't': {'pos': (78, 91)}})"", 'This model also says that the relationship of the son word is with the child word and the confidence of the prediction is also good.']","""from google.colab import drive\ndrive.mount('/content/drive')""
 '!git clone https://github.com/thunlp/OpenNRE.git', 'cd OpenNRE', '!pip install -r requirements.txt', 'import opennre', ""model = opennre.get_model('wiki80_cnn_softmax')"", ""model.infer({'text': 'He was the son of ashok, and grandson of rahul.', 'h': {'pos': (18, 46)}, 't': {'pos': (78, 91)}})"", ""model = opennre.get_model('wiki80_bert_softmax')"", ""model.infer({'text': 'He was the son of ashok, and grandson of rahul.', 'h': {'pos': (18, 46)}, 't': {'pos': (78, 91)}})"", ""model = opennre.get_model('wiki80_bertentity_softmax')"", ""model.infer({'text': 'He was the son of ashok, and grandson of rahul.', 'h': {'pos': (18, 46)}, 't': {'pos': (78, 91)}})"""
122,tool_for_active_learning/,https://analyticsindiamag.com/hands-on-guide-to-small-text-a-python-tool-for-active-learning/,"['Today’s scenario where the data is more accessible and increasing in amount has increased the efforts to analyse and preprocess the data for a data scientist. There are many kinds of processes that need to be followed by a data scientist in preprocessing the data, cleaning the data, validating the data, and labelling the data. Labelling the data can be very much effort taking and time taking. In supervised learning, we can understand how much labelled data is important for a model to be trained to perform well. For example, in the NLP process, we have sentiments of the people, but the sentiments are not categorized; we can’t perform our learning in this case. Before the development of the model, we need to provide tags or labels to the sentiments, and here we can understand how difficult it can be to go through all the data for providing labels to the sentiment. To reduce the number of efforts taken by labelling raw data, active learning comes into the picture.', 'In an active learning environment, learners try to become experienced in making decisions in the field for which learners are assigned meaning-making. For example, in checking multiple-choice questions, the paper checker gives marks for the right answers and negative or zero marks for wrong answers. So the learner will be the paper checker, and he will try to be an expert in giving marks from the category of marks.\xa0', 'In machine learning processes, it is a subset of the whole learning process, as we know that this happens many times when we have to analyze data that is unlabeled, and in the whole process of machine learning, we need to provide our data points labels so that we can easily complete our machine learning process. And also, it is very helpful for models to provide well-labelled data for completing the learning process, but in cases where we need to provide levels to the data, active learning takes part.\xa0', 'The concept of active learning is to make a machine-learning algorithm to reach a higher level of accuracy by providing them with small numbers of training datasets. At the same time, models are allowed to choose the dataset from which they want to learn.\xa0', 'To provide labels to the unlabeled data, we usually require a human data annotator, which is a powerful way to annotate the data but is it efficient. For a data scientist, there are several tasks like analyzing the data, visualizing it, understanding it and finding patterns in the data. Aren’t these tasks pretty time-consuming? And if with all these we add one more task for them to provide labels to data accurately, the whole procedure will become too harsh, hectic and time-consuming. As A Result, data scientists will need to focus more on the procedure of labelling data. To reduce those efforts, active learning helps.', 'Active learning works differently in different situations. Roughly we can categorize active learning into three categories.', 'In this category, we try to make an algorithm that can determine the benefits to query for the labels for a specific unlabeled dataset while the model is training. The algorithm works with the models and provides labels to the dataset if labelling sequenced data sufficiently benefits training.', 'In this type of active learning algorithm, algorithms try to evaluate data samples for modeling and label the best fit data samples from a data source before performing the modeling part of the machine learning. Often algorithms learn from the already labelled datasets to provide labels to the unlabeled data and these are the most commonly used algorithms for active learning.', 'These algorithms are not useful for all the cases because we try to generate the synthesis data. The algorithm is allowed to generate the data for labelling. This is useful where we can easily generate the data instances.', 'Small text is an easy way to apply active learning in different kinds of machine learning procedures. Next, in the article, we are going to discuss the Small text for active learning.', 'As the article’s name suggests, the article is more focused on the small text library for active learning, which provides active learning algorithms for text classification and allows mixing and matching many classifiers and query strategies to build active learning applications. Using its features, we can easily develop classifiers using sklearn libraries, and also, in addition, we can use PyTorch classifiers with transformer models.\xa0\xa0', 'This article starts with small-text by implementing a binary classification model on sklearn provided 20 newsgroup data. Before starting the model, we will know how we can install this package in python.', 'If anyone wants to go ahead with the PyTorch classifiers and transformers, this is an extra requirement we need to download. We can download using the following command.', 'pip install small-text[transformers]', 'Since I am using google Colab, I will need to clone the whole package presented in the link. So next, I will clone the package.', 'After cloning it, we can install the package by giving the exact address where it got installed. Also, before giving the address, you will need to mount your drive for the Colab notebook.', 'Using the following command, we can mount our drive.', 'After the mounting, we can install the package which we have cloned using the following command.', 'After restarting the runtime, we can use the whole package.', 'Now we can start our procedure of developing a binary classifier using small-Text library packages.', 'Importing the required library:', 'Defining active learning parameters:', 'Defining a function to load the data using sklearn:', 'Defining a function to preprocess the data:', 'Defining a function to evaluate the results using sklearn’s\xa0 metrics:', 'Merging all these functions into the main function:', 'Defining a function to perform active learning.', 'This loop will perform 15 iterations of active learning; in each iteration, 20 samples of the news are queried and then updated. The update step will reveal the true label to the active learner.\xa0', 'Defining a function to initialize the active learner; this function is required for model-based query strategies.', 'Running the main function:', 'Here we can see that the accuracy of the model has increased. The size of the output was big, that is why in the image, the whole output is not put in the article, but it started with the training accuracy of 1.00 and the test accuracy of 0.76 with the sample size of 40.']","'pip install small-text'
 '!git clone https://github.com/webis-de/small-text.git', ""from google.colab import drive\n\ndrive.mount('/content/drive')"", 'pip install /content/small-text', 'import numpy as np\nfrom small_text.active_learner import PoolBasedActiveLearner\nfrom small_text.classifiers import ConfidenceEnhancedLinearSVC\nfrom small_text.classifiers.factories import SklearnClassifierFactory\nfrom small_text.query_strategies import PoolExhaustedException, EmptyPoolException\nfrom small_text.query_strategies import RandomSampling\nfrom small_text.data import SklearnDataSet', 'clf_template = ConfidenceEnhancedLinearSVC()\nclf_factory = SklearnClassifierFactory(clf_template)\nquery_strategy = RandomSampling()', ""from sklearn.datasets import fetch_20newsgroups\n\ndef get_twenty_newsgroups_corpus(categories=['rec.sport.baseball', 'rec.sport.hockey']):\n\n\xa0\xa0\xa0train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'),categories=categories)\n\n\xa0\xa0\xa0\xa0test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'),categories=categories)\n\n\xa0\xa0\xa0\xa0return train, test\n\ndef get_train_test():\n\n\xa0train, test = get_twenty_newsgroups_corpus()\n\xa0\xa0\xa0return train, test"", ""from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import normalize\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import normalize\n\ndef preprocess_data(train, test):\n\n\xa0vectorizer = TfidfVectorizer(stop_words='english')\n\xa0x_train = normalize(vectorizer.fit_transform(train.data))\n\n\xa0x_test = normalize(vectorizer.transform(test.data))\n\n return SklearnDataSet(x_train, train.target), SklearnDataSet(x_test, test.target)"", ""from sklearn.metrics import f1_score\n\ndef evaluate(active_learner, train, test):\n\n\n\xa0\xa0\xa0\xa0y_pred = active_learner.classifier.predict(train)\n\n\xa0\xa0\xa0\xa0y_pred_test = active_learner.classifier.predict(test)\n\n\xa0\xa0\xa0\xa0print('Train accuracy: {:.2f}'.format(\n\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0f1_score(y_pred, train.y, average='micro')))\n\n\xa0\xa0\xa0\xa0print('Test accuracy: {:.2f}'.format(f1_score(y_pred_test, test.y, average='micro')))\n\n\xa0\xa0\xa0\xa0print('---')"", ""def main():\n\n\xa0\xa0\xa0# Active learning parameters\n\n\xa0\xa0\xa0\xa0clf_template = ConfidenceEnhancedLinearSVC()\n\n\xa0\xa0\xa0\xa0clf_factory = SklearnClassifierFactory(clf_template)\n\n\xa0\xa0\xa0\xa0query_strategy = RandomSampling()\n\n\xa0\xa0\xa0\xa0text_train, text_test = get_train_test()\n\n\xa0\xa0\xa0\xa0train, test = preprocess_data(text_train, text_test)\n\n\xa0\xa0\xa0\xa0active_learner = PoolBasedActiveLearner(clf_factory, query_strategy, train)\n\n \xa0\xa0\xa0labeled_indices = initialize_active_learner(active_learner, train.y)\n\n\xa0\xa0\xa0\xa0try:\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0perform_active_learning(active_learner, train, labeled_indices, test)\n\xa0\xa0\xa0\xa0except PoolExhaustedException:\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0print('Error! Not enough samples left to handle the query.')\n\xa0\xa0\xa0\xa0except EmptyPoolException:\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0print('Error! No more samples left. (Unlabeled pool is empty)')"", ""def perform_active_learning(active_learner, train, labeled_indices, test):\n\n\xa0\xa0\xa0for i in range(15):\n\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0# ...where each iteration consists of labelling 20 samples\n\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0q_indices = active_learner.query(num_samples=20)\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0y = train.y[q_indices]\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0active_learner.update(y)\n\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0labeled_indices = np.concatenate([q_indices, labeled_indices])\n\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0print('Iteration #{:d} ({} samples)'.format(i, len(labeled_indices)))\n\n\xa0\xa0\xa0\xa0\xa0\xa0evaluate(active_learner, train[labeled_indices], test)"", '\n\ndef initialize_active_learner(active_learner, y_train):\n\n\xa0\xa0\xa0\xa0indices_pos_label = np.where(y_train == 1)[0]\n\xa0\xa0\xa0\xa0indices_neg_label = np.where(y_train == 0)[0]\n\n([np.random.choice(indices_pos_label, 10, replace=False),\n            \xa0np.random.choice(indices_neg_label, 10, replace=False)])\n\n    x_indices_initial = x_indices_initial.astype(int)\n    y_initial = [y_train[i] for i in x_indices_initial]\n\n\xa0\xa0\xa0\xa0active_learner.initialize_data(x_indices_initial, y_initial)\n\xa0\xa0\xa0\xa0return x_indices_initial', ""if __name__ == '__main__':\n\xa0\xa0\xa0\xa0main()"""
123,text_classification_using_transformers/,https://analyticsindiamag.com/guide-to-pysentimiento-toolkit-text-classification-using-transformers/,"['As the word sentimiento means feeling in English, pysentimiento is a python toolkit for sentiment analysis and text classification. To make a model for sentiment analysis, we need to take care of model type, seek the best hyper-parameter tuning, fit the data into the model, train and test the model. Pysentimiento comes to save us from all these hard-working processes. Pysentimiento is the best way to perform text classification and sentiment analysis. The best thing is that it has two features that we can use, we can analyze the text in two languages(English and Spanish) with a single module, and also we have the option to perform preprocessing of the text. Before going to the code work, let’s just have some overview about the hugging face and transformers because it is a Transformer-based library.', 'Hugging Face is one of the biggest startups for generating different packages and modules in the NLP section of data science. Today, many big companies like Apple, Manzo, Bing etc. are producing such amazing features for users, using hugging face libraries. Hugging Face is an open-source community with transform libraries which all are backed up by PyTorch and TensorFlow. These libraries provide thousands of pretrained models to tune them according to our requirements.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'Transformers are part of the hugging face repositories. They are one of the most used repositories from hugging, which provides us thousands of pretrained models and APIs to \xa0 \xa0 quickly download and use those models to get better results using our datasets. Pysentimiento is a kind of model for text classification provided by transformers. Transformers are mainly focused on natural language processing. Some of the models provided by transformers are very easy and reliable for performing NLP tasks like classification, information extraction, question answering, translation in more than 100 languages.\xa0', 'For more information on hugging face and transformers, you can refer to these articles-', 'Let’s just move towards the pysentimiento toolkit. We will be going to see how we can perform some basic operations of text classification and sentiment analysis using pysentimiento toolkit, a transformer-based library using google colab.', 'Installation of the package can be done by.', 'Let’s try to do some sentiment analysis in English.', 'Importing sentiment analyzer of pysetimiento.', 'input:', 'Predicting a positive nature sentence:', 'Defining a positive sentence\xa0', 'Input:', 'Input:', 'Here we can see that the sentiment analyzer has analyzed that the sentence is positive with a probability of 99.4 %, which is quite satisfying. Let’s move into some more results.', 'Input :', 'Here in the above inputs, we can see that we have replaced love with like. In the predictions, we can see the significant changes; for the word love in a sentence, the toolkit has predicted 99.4% positivity. For like word, it has predicted the positivity is 98.7%. On the other hand, there are changes in the neutral nature but not in the negative nature.\xa0', 'Input:', 'Here we can see the sequencing of the training data. We have all the words in a single line, but the model understands the weightage of love is more than the weightage of hate in the sentence and predicts accordingly.', 'Let’s try the SentimentAnalyzer in the Spanish language.', 'Making an object of sentiment analyzer of pysentimiento.', 'Input:', 'Predicting the nature of the sentence ‘i love you’ in Spanish.', 'Input:', 'Here we can see that the prediction for the Spanish language is also good. Let’s have some more tests.', 'Input:', 'Here in the above input, we have given “I like you” as a sentence.', 'Input:', 'Here in the above input, I have provided “i hate you” as a sentence, and we can see in the results the sentiment analyzer is working fine. Instead of a sentiment analyzer this toolkit provides us with an emotion analyzer as well. We can use it in any project where we need to do some emotional analysis of the text. Let’s look at how we can make it work and how the results it can provide.', '\xa0Importing the package.', 'Input:', 'Predicting results for an emotion', 'Input:', 'Here we can see the results telling us the emotion of yessssss word can be joy and tell us that it can also be something else. In the emotion analysis, the best-suggested factor is that we should perform it in audio data because we don’t know how a person is saying it. Still, the performance of the toolkit is also very satisfying. Let’s have some more tests also.', 'Input:', 'Here in the output, we can see that for a whole sentence where we defined the nature of the sentence joyfull, the analyzer predicted the emotion output as a joy.', 'By those results, we can say it is a well-trained model and can be useful for us.', 'This feature can also be used for Spanish language texts. So let’s perform the emotion analysis for the Spanish language too.', 'Importing the package.\xa0', 'Input:', 'Defining and predicting the emotions of sentences.', 'input:', 'Here in the output, we can see that the results are satisfying for the word. Now in the next step, we will use a sentence for emotion analysis using the pysentimento toolkit.', 'Input:', 'Here in the input, we have provided a sentence that means “we are amazing” in English and can see the predictions that the emotion of the sentence is joy.', 'With the package pysentimiento we get one more feature that can be used for text preprocessing, especially for the tweets where we can classify username, URL, repeated characters and hashtags or special characters from the text from a tweet. This feature is called preprocess_tweet. So let’s try this for some tweets.', 'Input:', 'Replacing username and url from a tweet.', 'Input:', 'Here we can see how easily it was processed. First, let’s check the processor for repeated characters and hashtags in a tweet.', 'Input:', 'Here we can see how it extracted the hashtag value and repeated character from the tweet.', 'This feature can also handle the emojis in tweets or text. So let’s check how it can do that.', 'Input:', 'Here we can see that he classified in the text where the emojis are and what the emojis are named. So let’s put all the things together in one tweet and process it.', '\xa0Input:', 'Here in the output, we can see that we have good enough results for text processing also.']","'!pip install pysentimiento'
 ' \xa0from pysentimiento import SentimentAnalyzer\n sentiment_analyzer_en = SentimentAnalyzer(lang=""en"") ', ""sentence ='i love analytics india magazine'"", 'sentiment_analyzer_en.predict(sentence)', "" sentence ='i like analytics india magazine'\n sentiment_analyzer_en.predict(sentence) "", "" sentence = 'i love pizza but i hate broccoli in pizza'\n sentiment_analyzer_en.predict(sentence) "", 'sentiment_analyzer_es = SentimentAnalyzer(lang=""es"")', "" sentence = 'te amo oraciones'\n sentiment_analyzer_es.predict(sentence) "", "" sentence = 'te odio oraciones'\n sentiment_analyzer_es.predict(sentence) "", "" sentence = 'te odio oraciones'\n sentiment_analyzer_es.predict(sentence) "", ' from pysentimiento import EmotionAnalyzer\n emotion_analyzer = EmotionAnalyzer(lang=""en"") ', "" emotion = 'yessssss'\n emotion_analyzer_en.predict(emotion) "", 'EmotionOutput(output=joy, probas={joy: 0.515, surprise: 0.211, others: 0.210, anger: 0.019, fear: 0.018, sadness: 0.014, disgust: 0.013})', "" emotion = 'yeah, we won the match have celebration'\n emotion_analyzer_en. predict(emotion) "", 'EmotionOutput(output=joy, probas={joy: 0.898, others: 0.066, surprise: 0.012, disgust: 0.006, sadness: 0.006, fear: 0.006, anger: 0.005})', 'emotion_analyzer_es = EmotionAnalyzer(lang=""es"")', ""emotion_analyzer_es.predict('te amo')"", 'EmotionOutput(output=joy, probas={joy: 0.982, others: 0.013, surprise: 0.002, sadness: 0.002, fear: 0.001, disgust: 0.001, anger: 0.000})', ""emotion_analyzer_es.predict('Nosotros somos asombrosos')"", 'EmotionOutput(output=joy, probas={joy: 0.896, surprise: 0.061, others: 0.039, sadness: 0.001, fear: 0.001, disgust: 0.001, anger: 0.001})', 'Importing the preprocess_tweet package', 'from pysentimiento.preprocessing import preprocess_tweet', ' tweet = ""@yugeshverma i need to give a tweet https://www.google.com""\n preprocess_tweet(tweet) ', 'preprocess_tweet(""yessssssss this is a tweet #AIM"")', 'preprocess_tweet(""happy birthday ????????"", lang = \'en\')', ' tweet = ""@Analyticsindiamag we are having goooood #content in https://analyticsindiamag.com/ ????????""\n preprocess_tweet(tweet, lang = \'en\') '"
124,text_representations_and_classifiers/,https://analyticsindiamag.com/guide-to-facebooks-fasttext-for-text-representations-and-classifiers/,"['In the present scenario of Artificial Intelligence, Facebook AI Research (FAIR) is one of the leading contributors of open-source tools, libraries and architectures. There are many examples of contributions by Facebook like Flashlight, Opacus, Detectron2, Fasttext etc., to learn more about the contribution by FAIR, refer to this link. Fasttext is one of the open-source libraries for text classification and word representation, Contributed by FAIR.\xa0', 'FastText is an open-source library for efficient text classification and word representation. Therefore, we can consider it an extension of normal text classification methods. In conventional methods, we convert the words or texts into vectors that contain numeric values to make a machine learning algorithm understand the text. Wherein fastText breaks words into several sub-word.\xa0', 'Text Classification is a process to assign information (such as emails, posts, messages, tweets etc.) to one or multiple labels. Labels can be the score of review, non-spam or spam messages or categories in which information about the document is available.', 'Word Representation is used to improve the performance of text classifiers. An idea admired by many people in the current machine learning scenario is to represent words by a vector; vectors contain analogies or sentiment about a language or document.\xa0', 'This post is a tutorial to introduce the fastText library using google colab and tell how the whole pipeline looks like. First, we will install the fastText library and afterwards import the data,\xa0 suggested by the fasttext to practice, preprocess it to predict the model’s response, and improve the model results.', 'This article will make a text classification model using fastText is an open-source library and google colab as our IDE.', 'Also Read:', 'The following code implementation is in reference to the official implementation.', 'Installing the FastText is easy on UNIX(Linux) operating system. To do this in google colab we will build the binary of fastText, which is required to act.', 'To build it, we require the following codes in a cell and run it.', 'Input:', 'FastText has its bindings for python, which allows us to install it using pip directly.', 'Input :', '!pip install fasttext', 'After successfully installing fastText, we will gather the data. But, before it, let’s just take a look at the dataset.', 'The data set consists of thousands of questions asked on Cooking StackExchange, which have various assigned levels and are already present in FastText format. Here we require data where each line contains text information that is classified or levels.', '\xa0For example:', '__level__apple A apple in a day keep the doctor away.', 'In the next following steps, we are downloading the dataset and describing information about the data.', '\xa0Input :', '!wget https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz && tar xvzf cooking.stackexchange.tar.gz', 'Input:', '!head cooking.stackexchange.txt', 'Input:', '!wc cooking.stackexchange.txt\xa0', 'Here in the above outputs, we can see the format of the dataset and the number of rows in the data set, which is 15404.', 'In the following step, we are splitting the dataset into training and validation data.', 'Input:', 'Here we have split 12404 rows of the cooking.stackexchange.txt set into cooking.train data and 3000 rows of cooking.stackexchange.text into cooking.valid data.', 'Next,\xa0 we are ready to train our model with cooking.train data using the following command.', 'Input :', '!./fasttext supervised -input ./cooking.train -output ./cooking_model1', 'Or\xa0', 'It takes very little time in training. In the output, we can see the details about the models; in the above code, the input argument takes the data for training. So here cooking.train is our training data.', 'Now we can check the training of our classifier by:', 'Input\xa0 :', 'model.predict(""Which baking pan is best for bake amulet ?"")', 'In the output, we can see the label predicted by the model is baking; here it is working fine; let’s check for another label :', 'Input :', 'model.predict(""Why not put glasses in the dishwasher?"")', 'Here we can see that the label predicted by the model is wrong, and the model failed in a simple example. We will test it with our validation data (cooking.valid).', 'Input:', 'model.test(""cooking.valid"",k=1)', 'Looking at the result, they are not that good, as output shows that there are 3000 rows in the validation data set and the second value is precision, mathematically representation of precision is :', 'For text search in a document, precision can be the number or correct result divided by all the results.', 'The third value is recall; the mathematical representation of recall is:', 'In a similar example, recall is the number of correct answers divided by the number of required answers. Both precision and recall vary between 0 to 1.', 'In the test result, we can see that results are not that satisfactory. Therefore, to improve the results, we will predict with the top 3 labels of the dataset.', 'Input:', 'model.test(""cooking.valid"",k=3)', 'Input:', 'model.predict(""Why not put knives in the dishwasher?"", k=5)', 'Here we can see that the model predicted one label correctly out of 5 labels, and only one label is giving 0.035 recall value.', 'Next, we will discuss improving the above model.\xa0', 'Before changing any model parameters, let’s just see which parameter we can set according to the requirement.', 'Input :', '!./fasttext supervised', '\xa0To improve our model, we will preprocess the data as we can see some words and levels containing uppercase letters or some of them contain punctuations, so we try to remove or change those things.\xa0', 'Input:', 'After cleaning and splitting the data, we will fit the model again for training.', 'Input :', '!./fasttext supervised -input ./cooking.train -output ./model', 'After cleaning the data, we can observe the words count goes down to 8952, and the number of the label is increased by 1.', 'Let’s check the accuracy of the model.\xa0', 'Input:', 'model.test(""cooking.valid"")', '(3000, 0.16433333333333333, 0.07106818509442121)', 'We can compare the results; here, the precision is also starting to go up.', 'Now we will increase the number of epochs.', 'Input :', 'model = fasttext.train_supervised(input=""cooking.train"", epoch=25)', 'Let’s check the new model.', 'Input:', 'model.test(""cooking.valid"")', 'After testing the model, we can see that the model is much better. However, there are some more ways to improve the model. First, we can increase the learning rate. For this, we need to change the lr argument in the model. The suggested good value of the learning rate is between 0.1-1.0.\xa0', 'Input:', '\xa0model = fasttext.train_supervised(input=""cooking.train"", lr=1.0)', 'Let’s check for the results or the new model.\xa0\xa0\xa0\xa0', 'Input:', 'model.test(""cooking.valid"")', 'This time the model is quite improved by precision and recall value, now we will try to put both epoch and learning rate together in the training of the model, and then we will check for the results.', 'Input :', 'model = fasttext.train_supervised(input=""cooking.train"", lr=1.0, epoch=25)', 'Let’s check test the model.', 'Input :', 'model.test(""cooking.valid"")', '(3000, 0.5843333333333334, 0.25270289750612657)', 'We have improved our model precision from 16% to 58.4% in the above steps, changing the epochs and learning rate. Next, we will take one more step to improve the model’s performance where we will use the bigram instead of just using unigram. Before doing this, let’s go through the word n-gram.', 'Word n-gram –\xa0 the basic idea of word n-gram is the sequence of n words. Like\xa0 ‘apple’ is a unigram, ‘eating apple’ is a bigram and ‘eating two apples’ is trigram or 3-gram. The fasText is capable of making word n-gram when preparing for word vectors. For example, there is a word banana; we will use bigram in our next model to train it. Atthe time of training model, we provide it with an argument n-gram = 2; For example, the n-gram=2 for the word banana is\xa0', 'Here < and > shows the start and end of the word; this is a quite useful technique. Now it is very important to know why it is useful; what if we had the word ban as a vocabulary word? It will be represented as\xa0 <ba>\xa0 and\xa0 <na>, which make this word (ban) to be identified as we can extract word from banana.', 'Let’s try to improve the model performance using word bigrams.', 'Input :', 'model = fasttext.train_supervised(input=""cooking.train"", lr=1.0, epoch=25, wordNgrams=2)', 'Here we can see it has again improved by 1%. Now the model’s precision is almost 60%.', 'Till now, in this article, we were using regular softmax because the dataset was small, and we have seen that it took only a few seconds to fit the dataset. But if the size of the dataset is enormous, or we can say having several values in millions or billions, then the suggested advice is to use hierarchical softmax.', 'Hierarchical softmax is a loss function that computes an approximation of softmax faster. For a detailed explanation, you can go through this video.', 'Let’s put the hierarchical softmax as a loss function to train the model and test the model.\xa0\xa0', 'Input:', 'model = fasttext.train_supervised(input=""cooking.train"", lr=0.5, epoch=25, wordNgrams=2,\xa0 loss=\'hs\')', 'Testing the model :', 'Input:', 'model.test(""cooking.valid"")', 'As we can see with the result, our precision has decreased by almost 3%. This might be happening because of data size, which is very little for training a model using a hierarchical softmax loss function.', 'Now we will be providing a bucket list and dimension of words to model. Finally, we are changing the loss function to ova, whose primary meaning is one vs all and decreasing the learning rate to 0.5.', 'Input :', 'model = fasttext.train_supervised(input=""cooking.train"", lr=0.5, epoch=25, wordNgrams=2, bucket=200000, dim=50, loss=\'ova\')', 'Let’s test the model on cooking.valid data.', 'Input:', 'model.test(""cooking.valid"")', 'Here again, we can reach 60% of the precision for our model. It keeps increasing as we are going into depth and changing the relevant parameters.', 'Now let’s see the predictions made by the model. In this prediction, we ask the model to make predictions as many as it can over the 0.5 level of probability.', 'Input :', 'model.predict(""Which baking dish is best to bake a banana bread ?"", k=-1, threshold=0.5)', 'As we can see in our final results model has predicted all the values correctly, where when we started building, it was not even producing a single successful result. Making this model is very easy and less time-consuming. We discussed how to use fastText and why and where to use its different models using different parameters like the epochs, learning rate, dimensions of words, bucket value, and changing loss function.\xa0']","'!pip install fasttext'
 '!wget https://dl.fbaipublicfiles.com/fasttext/data/cooking.stackexchange.tar.gz && tar xvzf cooking.stackexchange.tar.gz', '!head cooking.stackexchange.txt', '!wc cooking.stackexchange.txt\xa0', '!head -n 12404 cooking.stackexchange.txt > cooking.train', '!./fasttext supervised -input ./cooking.train -output ./cooking_model1', 'model.predict(""Which baking pan is best for bake amulet ?"")', 'model.predict(""Why not put glasses in the dishwasher?"")', 'model.test(""cooking.valid"",k=1)', 'model.test(""cooking.valid"",k=3)', 'model.predict(""Why not put knives in the dishwasher?"", k=5)', '!./fasttext supervised', '!cat cooking.stackexchange.txt | sed -e ""s/\\([.\\!?,\'/()]\\)/ \\1 /g"" | tr ""[:upper:]"" ""[:lower:]"" > cooking.preprocessed.txt', '!head -n 12404 cooking.preprocessed.txt > cooking.train', '!./fasttext supervised -input ./cooking.train -output ./model', 'model.test(""cooking.valid"")', '(3000, 0.16433333333333333, 0.07106818509442121)', 'model = fasttext.train_supervised(input=""cooking.train"", epoch=25)', 'model.test(""cooking.valid"")', '\xa0model = fasttext.train_supervised(input=""cooking.train"", lr=1.0)', 'model.test(""cooking.valid"")', 'model = fasttext.train_supervised(input=""cooking.train"", lr=1.0, epoch=25)', 'model.test(""cooking.valid"")', '(3000, 0.5843333333333334, 0.25270289750612657)', 'model = fasttext.train_supervised(input=""cooking.train"", lr=1.0, epoch=25, wordNgrams=2)', 'model = fasttext.train_supervised(input=""cooking.train"", lr=0.5, epoch=25, wordNgrams=2,\xa0 loss=\'hs\')', 'model.test(""cooking.valid"")', 'model = fasttext.train_supervised(input=""cooking.train"", lr=0.5, epoch=25, wordNgrams=2, bucket=200000, dim=50, loss=\'ova\')', 'model.test(""cooking.valid"")', 'model.predict(""Which baking dish is best to bake a banana bread ?"", k=-1, threshold=0.5)'"
125,clean_raw_text_data/,https://analyticsindiamag.com/guide-to-cleantext-a-python-package-to-clean-raw-text-data/,"['The real world is a messy, messy place. Everyone has different opinions, but they can’t help but agree on this fact! What else is messy? Data !! Lots and lots of data which we collect, scrape or extract from numerous sources. So messy that in a survey, it was mentioned that data scientists spend around 60% of their time cleaning data. Unfortunately, approximately 50-55% find it quite enjoyable. Yeah, it’s enjoyable.', 'But we know that data cleaning is time-consuming, right? Also, lots of tools have popped up from time to time. The task is to make this crucial and vital task more bearable (at least a little more bearable) . The Python Community hosts a ton of libraries to make data orderly and umm…legible?\xa0 This can vary from never-ending data frames to stylizing them or whether it be analyzing datasets.', 'Using NLTK and Regex is known all over the community so much that we often undermine what else is really there that we can use for this hefty task. This blog is about such a new library (released only last year, January 2020) called CleanText. CleanText is an open-source python package (common for almost every package we see) specifically for cleaning raw data (as the name suggests and I believe you might have guessed).', 'Simple, easy to use package with minimalistic code to write with a ton of features to leverage (we all want that, right?). So there are two methods (yeah, mainly there are only two in this case), namely:', 'The beautiful thing about the CleanText package is not the amount of operations it supports but how easily you can use them. A list of those are mentioned below, and we’ll later write some code showcasing all of that for better understanding.', 'Enough introduction; let’s see how to install and use clean text.\xa0', 'CleanText package requires Python3 and NLTK for execution.\xa0', 'For installing using pip, use the following command.', '!pip install cleantext', 'After this, import the library.', 'import cleantext', 'We’ll need to leverage stopwords from the NLTK library to use in our implementation.', 'As mentioned earlier, there are two methods which we can use; these are as below.', 'This will return the text in string format.', 'cleantext.clean(""your_raw_text_here"", all= True)\xa0', 'For returning a list of clean words', 'cleantext.clean_words(""your_raw_text_here"", all= True)\xa0', 'Two main methods, as discussed, are shown below, firstly.', 'cleantext.clean(""the_text_input_by_you"", all= True)\xa0', 'Secondly,\xa0', ""cleantext.clean_words('Your s$ample !!!! tExt3% to \xa0 cleaN566556+2+59*/133 wiLL GO he123re', all=True)"", 'Notice that every operation has been carried out, and then we have been provided with the output.', 'Text having letters encoded with Unicode characters, different Unicode for different letters. There are different encodings such as UTF-8, UTF-32 and so on.\xa0', 'Notice the ‘u’ has been encoded and we have to convert it into a normal character described by ASCII as the former will not be recognised as an English Language letter and will be discarded.', 'This may be the case with many such words, which are included from different languages in English.', 'Abbreviated from American Standard Code for Information Interchange, this is a character encoding just like Unicode. They are used for representing text in computers and telecommunications equipment. This is to create a standard for character sets so that different devices can communicate with each other.', 'This will output – ‘kozuscek’', 'As you can see, the present text is untouched, and the encoding in our text has been converted successfully to text. This happens with data when doing NLP tasks; hence this is a useful operation that can be easily performed.', 'Uppercase and Lowercase letters are considered different; hence, we must change them to lowercase (preferably). While understanding the text to make meaning out if it, this hardly matters hence should be performed.', 'As I said, minimalistic code is required to handle these tasks using this library.', 'Many times we encounter situations where we have to replace URLs with some other particular string. Usually, this requires complex Regex expressions (I hate them), the solution to this is shown below.', 'Using this package makes us believe that using python is really like writing code in English.', 'Straight forward methods, arguments. Simple in and simple out.', 'We also encounter cases when there are currency symbols in our text; we can either remove them completely(nope, won’t help) or replace them with text (which is so better). Below is an example, using Rupee, which is the standard currency in India.', 'Not only have we removed the currency symbol, which won’t mean anything to the model, but we also replaced it with our text seamlessly.', 'This is undoubtedly the most useful operation we require while handling language-related tasks.', 'These don’t add any value to any tasks we perform on the text dump we have.', 'You can also change what punctuations to keep and to remove. Super-friendly right?!', 'Another important operation or manipulation on the text data which is vital as this will not add any semantic or syntactic value.', 'I recommend using this package which takes very little time to implement and try different combinations of the methods mentioned above. The notebook is present here for reference. All code is written Google Colab.\xa0']","'!pip install cleantext'
 'import cleantext', 'cleantext.clean(""your_raw_text_here"", all= True)\xa0', 'cleantext.clean_words(""your_raw_text_here"", all= True)\xa0', 'cleantext.clean(""the_text_input_by_you"", all= True)\xa0', 'Output - ‘thetextinputbyy’', ""cleantext.clean_words('Your s$ample !!!! tExt3% to \xa0 cleaN566556+2+59*/133 wiLL GO he123re', all=True)"", 'Output - [‘sampl’ , ‘text’ , ‘clean’]'"
126,supervision_to_nlp_tasks/,https://analyticsindiamag.com/meet-skweak-a-python-toolkit-for-applying-weak-supervision-to-nlp-tasks/,"['skweak is a software toolkit based on Python, developed for applying weak supervision to various NLP tasks. It has been recently introduced by Pierre Lison, Jeremy Barnes and Aliaksandr Hubin from Norway in April 2021 (research paper).', 'Are you familiar with the term ‘weak supervision’? Have a look at its brief meaning before proceeding.', 'Weak supervision refers to a novel ML technique that uses noisy, unstructured or limited data sources to label training data in a supervised learning approach. Instead of annotating the data manually, labelling functions created using existing knowledge of the domain annotate the data independently and hence eliminate the efforts and cost required for manual annotations.', 'skweak applies weak supervision to various NLP tasks such as sequence labelling and text classification. What it does can be summarized by the following steps:', 'Any machine learning model can then be trained on the labelled corpus.', 'Image source: Research paper', 'Here’s a demonstration of using skweak for annotating a corpus having 200 news articles. The code has been implemented using Google colab with Python 3.7.10, skweak 0.2.9 and spacy 2.2.4 versions. Step-wise explanation of the code is as follows:', '!pip install skweak', '!pip install spacy', 'Where, ‘company_detector’ is the name given to labelling function and ‘find_company’ is the function to be used for annotation', 'docs = list(detect_company.pipe(docs))', 'Condensed output:', 'Where, ‘other_org_detector’ is the name given to the labelling function and ‘find_other_arg’ is the function to be used for annotation.', 'docs = list(detect_other_org.pipe(docs))', 'First, we extract companies’ data from a JSON file available here.\xa0', 'docs = list(gzt.pipe(docs))', 'Apply the labelling function on a spacy document from the corpus.', 'skweak.utils.display_entities(docs[28], ""gzt"")', 'Condensed output:', 'Apply the NER model on a document.', 'skweak.utils.display_entities(docs[17], ""spacy"")', 'Condensed output:', 'We now aggregate the labels of different labelling functions using a generative model.\xa0This will create a unique annotation for each of the documents in the corpus.', 'Specify that “ORG” term can represent both a company or a non-commercial organization.agg_model.add_underspecified_label(“ORG”, [“COMPANY”, “OTHER_ORG”])', 'Fit the aggregated model on the corpus.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'docs = agg_model.fit_and_aggregate(docs)', 'Run the aggregated model on a document.', 'skweak.utils.display_entities(docs[17], ""hmm"")', 'Condensed output:']","'!pip install skweak'
 '!pip install spacy', 'docs = list(detect_company.pipe(docs))', 'docs = list(detect_other_org.pipe(docs))', 'docs = list(gzt.pipe(docs))', 'skweak.utils.display_entities(docs[28], ""gzt"")', 'skweak.utils.display_entities(docs[17], ""spacy"")', 'docs = agg_model.fit_and_aggregate(docs)', 'skweak.utils.display_entities(docs[17], ""hmm"")'"
127,popular_nlp_library_corenlp/,https://analyticsindiamag.com/hands-on-guide-to-stanfordnlp-a-python-wrapper-for-popular-nlp-library-corenlp/,"['Natural Language Processing is a part of Artificial Intelligence which aims to the manipulation of the human/natural language. It is used for extracting meaningful insights from textual datasets. NLP is mainly used for Text Analysis, Text Mining, Sentiment Analysis, Speech Recognition, Machine Translation, etc. Python provides different modules/packages for working on NLP Operations.', 'CoreNLP is a one-stop solution for all NLP operations like stemming, lementing, tokenization, finding parts of speech, sentiment analysis, etc. It is written in Java programming language but is used for different languages. CoreNLP is a framework that makes it easy to apply different language processing tools to a particular text.\xa0', 'StanfordNLP is a python wrapper for CoreNLP, it provides all the functionalities of CoreNLP to the python users. StanfordNLP group consists of faculty, postdocs, programmers, and students who work together on algorithms that allow computers to process and understand human languages.\xa0', 'In this article, we will explore StanfordNLP and see what types of natural language processing functionalities it provides.\xa0', 'We will start exploring StanfordNLP but before that, we need to install it using pip install stanfordnlp.', 'We will be exploring stanfordnlp, so we need to import it. Also, we need to download the English models of StanfordNLP as we will be working with the English language.', 'import stanfordnlp as st', 'st.download(‘en’)\xa0 \xa0 \xa0 #Downloading the English Models', 'This command will download the English models for stanfordnlp.', 'For exploring different NLP operations we first need to create a default pipeline for the English language. Also, let us define a text/ sentence which we will be working on.', 'pipe = stanfordnlp.Pipeline()', 'text = pipe(""This artcile will tell you How to use StanfordNLP. Let us start."")', 'Dependency function displays the word in the sentence along with the indices for the word in the Universal Dependencies and the dependency relation of the words.', 'text.sentences[0].print_dependencies()', '', 'Tokenization is separating the text into smaller units which can be words, characters, or subwords.\xa0 Tokenization function also provides the Lemmatization of all the words in the sentence along with verb form, dependency relation, etc.\xa0', 'text.sentences[1].print_tokens()', '', 'It is the process of grouping together the different inflected forms of a word so they can be analyzed as a single itemLemmatization is an effortless task when we use StanforNLP, we just need to split the sentence into words and apply lemma function to each word.', 'for i in text.sentences:', '\xa0\xa0\xa0 \xa0 \xa0 \xa0 \xa0 for j in i.words:', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0 \xa0        print(j.lemma)', '', 'Part of speech tagging assigns each word with the parts of speech such as nouns, verbs, adjectives, etc.', 'for i in text.sentences:', '\xa0\xa0\xa0 \xa0 \xa0 \xa0 \xa0 for j in i.words:', '\xa0\xa0\xa0\xa0\xa0\xa0 \xa0            print(j.pos)', '', 'Similarly, we can also find treebank-specific POS (XPOS) tags, and universal morphological features (UFeats).', 'It determines the syntactic head of each word in the sentence and the dependency relation between two words. It has two functions ‘governor’ and ‘dependency_relation’', 'for i in text.sentences:', '\xa0\xa0\xa0\xa0 \xa0 \xa0 \xa0 for j in i.words:', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0          print(j.governor, j.dependency_relation)', '', 'These are some of the basic NLP techniques which we can apply to our textual data. StanfordNLP is fast and effective. Other than the basic functions, the StanfordNLP module contains different packages for different uses.', 'Models package contains different language packs, currently, StanfordNLP supports around 53 languages and we can download the models for different languages as we have seen in the starting of the article where we downloaded English language.', 'Pipeline package is used to process textual data by building desired pipelines using different languages, processors, models of our choice, also we can attempt to use gpu by ‘use_gpu’ if gpu is available.', 'The server is defined as the internal server which contains a simple web API server for surviving the NLP operations. This server can be used to directly operate the functionalities provided by CoreNLP, for connecting to the CoreNLP client API java should be installed on your systems.']","'import stanfordnlp as st'
 'st.download(‘en’)\xa0 \xa0 \xa0 #Downloading the English Models', 'pipe = stanfordnlp.Pipeline()', 'text = pipe(""This artcile will tell you How to use StanfordNLP. Let us start."")', 'text.sentences[0].print_dependencies()', 'text.sentences[1].print_tokens()', 'for i in text.sentences:', 'for j in i.words:', 'print(j.lemma)', 'for i in text.sentences:', 'for j in i.words:', 'print(j.pos)', 'for i in text.sentences:', 'for j in i.words:', 'print(j.governor, j.dependency_relation)'"
128,for_multilingual_nlp_applications/,https://analyticsindiamag.com/hands-on-tutorial-on-polyglot-python-toolkit-for-multilingual-nlp-applications/,"['Natural Language Processing is a process of making the human language understandable to machines and then performing different operations on it to extract useful information. NLP is a part of Artificial Intelligence which makes the interaction between computer and human language.', 'There is a large variety of python libraries that can help us in performing NLP tasks. All libraries have certain unique features and which make them different from each other. Generally, NLP libraries have functions like Tokenize, Stemming, Lamenting, Spell CHeck, etc.\xa0', 'Polyglot is an open-source python library which is used to perform different NLP operations. It is based on NumPy which is why it is fast. It has a large variety of dedicated commands which makes it stand out of the crowd. It is similar to spacy and can be used for languages that do not support spacy.', 'In this article, we will explore different NLP operations and functions which can be performed using polyglot.', 'Like any other python library, we will install polyglot using pip install polyglot.', 'We will import polyglot and explore its different functionalities. All functionalities will be imported as and when required.', 'Before performing different operations on our data, let us first initialize some text which we will use for performing different functions on.', ""init = '''Analytics India Magazine chronicles technological progress in the space of\xa0\xa0analytics, artificial intelligence, data science & big data by highlighting the innovations, players, and challenges shaping the future of India through promotion and discussion of ideas and thoughts by smart, ardent,\xa0action-oriented individuals who want to change the world.'''"", 'Polyglot can identify the language of the text passed to it using the language function. Let us see how to use it.', 'detect = Detector(init)', 'print(detect.language)', 'In tokenize, we can print the wordlist which is the words that are there in the text used as well as the sentences which are there in the text.\xa0', 'from polyglot.text import Text', 'text = Text(init)', 'text.words', 'text.sentences', 'Parts of speech tagging is used to identify the syntactic functionality of word occurrence.', 'from polyglot.mapping import Embedding', 'text.pos_tags', 'It extracts phrases from the plain text that are entities like location, person, and organizations.', 'text.entities', 'Let us try this with some more texts.\xa0', ""init1 = '''Hello my name is Himanshu Sharma and I am from India'''"", 'text = Text(init1)', 'text.entities', 'It defines the regularities behind word formation in human language. Let us see how to use it.', 'from polyglot.text import Word', 'words = [""programming"", ""parallel"", ""inevitable"", ""beautiful""]', 'for w in words:', '\xa0\xa0\xa0\xa0\xa0w = Word(w, language=""en"")', '\xa0\xa0\xa0\xa0\xa0print(w, w.morphemes)', 'It is used to find out the polarity of the text.', 'text = Text(""The new economic policies are quite good."")', 'for w in text.words:', '\xa0\xa0\xa0\xa0print(w, w.polarity)', 'These are some of the NLP operations which we can perform using polyglot.']","""init = '''Analytics India Magazine chronicles technological progress in the space of\xa0\xa0analytics
 artificial intelligence, data science & big data by highlighting the innovations, players, and challenges shaping the future of India through promotion and discussion of ideas and thoughts by smart, ardent,\xa0action-oriented individuals who want to change the world.'''"", 'detect = Detector(init)', 'print(detect.language)', 'from polyglot.text import Text', 'text = Text(init)', 'text.words', 'text.sentences', 'from polyglot.mapping import Embedding', 'text.pos_tags', 'text.entities', ""init1 = '''Hello my name is Himanshu Sharma and I am from India'''"", 'text = Text(init1)', 'text.entities', 'from polyglot.text import Word', 'words = [""programming"", ""parallel"", ""inevitable"", ""beautiful""]', 'for w in words:', 'w = Word(w, language=""en"")', '\xa0print(w, w.morphemes)', 'text = Text(""The new economic policies are quite good."")', 'for w in text.words:', 'print(w, w.polarity)'"
129,using_text2emotion_in_python/,https://analyticsindiamag.com/social-media-monitoring-emotional-analysis-using-text2emotion-in-python/,"['In today’s digital world every brand knows how important social media has become for them to drive their businesses. Every brand tries to get sales or conversions of its products or services by driving its potential customers emotionally on social media through Ads, Posts, videos, memes, etc. Have you ever thought how useful it would be for your business to know the emotions of your customers about your product by analyzing the feedback or comments from your social media posts? This article gives you an idea of the same using text2emotion, a python package developed by me along with three of my colleagues.', 'It helps you in classifying the tone of the text by categorizing it into five different emotions as Happy, Angry, Surprise, Sad, and Fear.', 'Key Features:', 'Let us now look at an Industrial use case where analyzing emotions from text plays a vital role to give us more clarity on this topic.', '', 'In today’s digital world Brand Monitoring and reputation management has become one of the most important aspects of every business unit. This is where emotion analysis plays a vital role. Understanding how the end-users or customers recognize your brand or product is very useful for every company and organization.\xa0', 'We can implement the text2emotion package to create a software that brings flexibility into the business by giving information about the perception of a brand by the end users and gives more insight into the reputation of the company and its products. It will help companies by allowing them:', 'All this helps us in modifying our product and services according to the need of the customers and generate more revenue.', 'Let us now look at the working of this package.', 'A) TEXT PRE-PROCESSING', 'In the first step, our aim is to remove all the impurities or unwanted things from our data by data cleaning so that it can become suitable for emotion analysis.', 'B) EMOTION IDENTIFICATION', 'In the second step, we will identify the different emotions from the words obtained from pre-processed text and will keep a count of each and every emotion.', 'C) EMOTION ANALYSIS', 'After the completion of Emotion Identification, we need to analyze the emotions in order to get proper output for the input message.', 'Google Collab : text2emotion', 'Below given is the demo of code implementation with Streamlit App for the users.', 'Enter the text message in the box and click on the submit button.', '2. Click the submit button.', '3. Bingo! Get the output of your message in visual form.', 'It identifies the emotions in the text and gives you output in visual form accordingly.', 'Let’s get hands-on experience in the library.', 'text2emotion Documentation']",
130,profiling_of_textual_dataset/,https://analyticsindiamag.com/complete-guide-on-nlp-profiler-python-tool-for-profiling-of-textual-dataset/,"['Natural Language Processing is a subfield of Artificial Intelligence that works on making the human language understandable to the machine/computer. NLP has different functionalities that work on the textual data and find out useful insights and information. NLP can practically be used for Speech Recognition, creating voice search engines, etc. NLP can be used to perform a large variety of operations on text data like tokenizing, lamenting, stemming POS tagging, etc.', 'NLP Profiler is a simple NLP library which works on profiling of textual datasets with one one more text columns. Basically NLP profilers provide us with high-level insights about the data along with the statistical properties of the data. It works the same way as pandas.describe() works for pandas dataframe for statistical properties.', 'It takes the textual data as input with at least one column with text data and returns a dataframe which contains useful insights about the data like sentiment analysis, the subjectivity of data, etc. NLP profiler is in its early stage and is continuously improving.\xa0', 'In this article, we will explore what are the different functionalities that are there in NLP profiler and implement them in order to gain useful insights from the data.', 'NLP Profiler can be installed using the git repository where it is hosted. Before Installing it you need to download and install the git version according to your operating system. After git is installed we can install NLP Profiler by running the below-given command in the command prompt.', 'pip install git+https://github.com/neomatrix369/nlp_profiler.git@master', 'We will load the data using pandas so we will import pandas and for creating the data profile we will import the NLP profiler.', 'import pandas as pd', 'from nlp_profiler.core import apply_text_profiling', 'We need a textual dataset in order to explore NLP profiler, here I have used a dataset containing tweets which can be downloaded from Kaggle. This dataset contains different attributes like tweets, usernames, etc. But are only concerned with the text i.e. the tweets, so we will load this dataset and slice this dataset to make a new dataframe that contains only text column.', 'This dataset is pretty large so I have taken only the first 100 rows otherwise it will take a lot of time in computation.', ""df = pd.read_csv('Tweets.csv')"", ""text_nlp = pd.DataFrame(df, columns=['text'])"", 'text_nlp.head()', 'Next, we will pass this data to the text profiling function where we need to mention the name of the dataframe and the column which contains text so that it can create a new dataframe that contains the Text Profile with different attributes.', ""profile_data = apply_text_profiling(text_nlp, 'text')"", 'profile_data.head()', 'Here you can see how NLP Profiler has created a new dataframe that contains 22 attributes about the text like polarity, sentiment, subjectivity, etc. This is a great way of analyzing different text data and gain useful insights.', 'We can also use the describe function to analyze the statistical properties of these attributes.', 'profile_data.describe()', 'Here we can different statistical properties of the textual dataset according to different attributes/properties of the data.\xa0', 'Next, let us visualize some of the attributes which are created by NLP Profiler.', 'Let us visualize some of the attributes created by NLP Profiler in order to get some meaningful insights and patterns.', 'Sentiment polarity and polarity score will tell us how text data is divided for different sentiments like positive, negative, etc.', ""profile_data[‘'sentiment_polarity_score'].hist()"", ""profile_data['sentiment_polarity'].hist()"", 'Subjectivity is used to analyze whether the text is subjective or objective.', ""profiled_text_dataframe['sentiment_subjectivity_summarised'].hist()"", 'Spelling quality checks the spelling of the words that are used in the text and differentiate them accordingly.', ""profiled_text_dataframe['spelling_quality'].hist()"", 'It counts the emojis that are used while writing the text.', ""profiled_text_dataframe['emoji_count'].hist()"", 'Similarly, we can plot and analyze different attributes which are created by NLP Profiler.']","'pip install git+https://github.com/neomatrix369/nlp_profiler.git@master'
 'import pandas as pd', 'from nlp_profiler.core import apply_text_profiling', ""df = pd.read_csv('Tweets.csv')"", ""text_nlp = pd.DataFrame(df, columns=['text'])"", 'text_nlp.head()', ""profile_data = apply_text_profiling(text_nlp, 'text')"", 'profile_data.head()', 'profile_data.describe()', ""profile_data[‘'sentiment_polarity_score'].hist()"", ""profile_data['sentiment_polarity'].hist()"", ""profiled_text_dataframe['sentiment_subjectivity_summarised'].hist()"", ""profiled_text_dataframe['spelling_quality'].hist()"", ""profiled_text_dataframe['emoji_count'].hist()"""
131,toolkit_for_text_processing/,https://analyticsindiamag.com/texthero-guide-a-python-toolkit-for-text-processing/,"['Text processing is a method to extract and analyze information from textual datasets. Textual datasets contain data in text formats and are used to store some useful information. Processing the textual data is important in order to clean, analyze, and visualize the data and further use it for machine learning models.', 'Texthero is one such library that is used to analyze and process the textual datasets and make them zero to hero. It is a python package that is used to work with textual data efficiently and quickly.', 'In this article, we will try to explore texthero and its text processing capabilities. We will see how efficiently and easily we can process data using texthero.', 'Like any other library, we first need to install texthero using pip install texthero.', 'We will be importing texthero for text processing and pandas for loading the dataset and manipulating it.\xa0\xa0', 'import pandas as pd', 'import texthero as hero', 'The dataset we will be using here can be downloaded from Kaggle. This dataset contains certain attributes which we will analyze but we will mainly focus on the ‘content’ column.', 'df = pd.read_csv(‘text.csv’)', 'df', 'We can see that our dataset contains a sentiment analysis of tweets of different authors. We will focus on the tweets and will try and apply different functions used for text processing using Texthero.', 'We will start by cleaning the text in the ‘content’ column which is the tweets by the users. We will clean the text and store it in a new column.', ""df['clean_content'] = hero.clean(df['content'])\xa0"", 'df[‘clean_content’].head()', 'The clean function has certain defined properties which like, it removes all stopwords, punctuations, digits, whitespaces, etc. Also, it converts the text into all lowercase. We can use all these functions separately according to our wish.', 'Tokenize function returns a pandas series where each row contains a list of tokens', ""hero.tokenize(df['clean_content'])"", 'Stemming means removing the end of words with a heuristic process. Stem function makes use of two NLTK stemming algorithms known as Snowball Stemmer and Porter Stemmer.\xa0', ""hero.stem(df['clean_content'], stem=’snowball’)"", 'There are many ways of visualizing the textual data, here we will use ‘Wordcloud’ to visualize the cleaned data we created.', ""hero.visualization.wordcloud(df['clean_content'], width= 250, height = 150,\xa0max_words=200, background_color='WHITE')"", 'Similarly, we can visualize the most frequently used words or the top used words using the top_words visualization by TextHero.\xa0\xa0\xa0\xa0\xa0', ""hero.visualization.top_words(df['clean_content'])"", 'Now we will implement some of the NLP operations provided by TextHero on our data.', 'Named entities function returns a Pandas Series where each row contains a list of tuples containing information regarding the given named entities. We will be using the spacy as a package here.\xa0', ""hero.named_entities(df['clean_content'], package='spacy')"", 'It returns a group of consecutive word that belongs together. As our dataset is pretty large so we will analyze the noun chunks in only 100 rows.', ""hero.noun_chunks(df['clean_content'][:100])""]","'import pandas as pd'
 'import texthero as hero', 'df = pd.read_csv(‘text.csv’)', 'df', ""df['clean_content'] = hero.clean(df['content'])"", 'df[‘clean_content’].head()', ""hero.tokenize(df['clean_content'])"", ""hero.stem(df['clean_content'], stem=’snowball’)"", ""hero.visualization.wordcloud(df['clean_content'], width= 250, height = 150,\xa0max_words=200, background_color='WHITE')"", ""hero.visualization.top_words(df['clean_content'])"", ""hero.named_entities(df['clean_content'], package='spacy')"", ""hero.noun_chunks(df['clean_content'][:100])"""
132,nlp_library_by_stanford/,https://analyticsindiamag.com/stanza-a-new-nlp-library-by-stanford/,"['In most NLP libraries, data that is to be processed is in English. Although a few libraries do support other languages, they do not deliver the same results as they do with the data in English. This is because languages vary widely from one another, and the techniques that work for English may not fit well for other languages. To address these challenges, Stanford developed a new library Stanza — a Python-based library for many human languages.', 'Stanza is a Python-based NLP library which contains tools that can be used in a neural pipeline to convert a string containing human language text into lists of sentences and words. This can produce base forms of those words, parts of speech, and morphological features. The toolkit is designed to align with more than 70 languages, using the Universal Dependencies formalism.', 'Stanza is built with highly accurate neural network components that also enable efficient training and evaluation with your own annotated data. The modules are built on top of the PyTorch library. It also supports GPU to expedite the analysis of various languages, including English.', 'Also, Stanza includes a Python interface to the CoreNLP Java package and inherits additional functionality from there. This includes, constituency parsing, coreference resolution, and linguistic pattern matching.', 'The package is available with pip package manager.', 'pip install Stanza', 'The pipeline consists of models ranging from tokenizing raw text to performing syntactic analysis on the entire sentence. The design is devised keeping the diversity of human languages in mind by data-driven models that learn the differences between languages. Besides, the components of Stanza are highly modular and reuses basic model architectures, when possible, for compactness.', 'Tokenization and Sentence Split: On feeding raw text, Stanza tokenizes it and groups tokens into sentences as the first step of processing. Unlike other existing toolkits, Stanza combines tokenization and sentence segmentation from raw text into a single module. This is done to predict the position of words in a sentence, as use of words are context-sensitive in some languages.\xa0', 'Multi-Word Token Expansion: The above methods identify multi-word tokens, which are then further extended into the syntactic words as the foundation for downstream processing. This is accomplished by the use of sequence-to-sequence (seq2seq) model to ensure frequently observed expansions in the training set, as they are always robustly expanded while maintaining the flexibility to model unseen words statistically.', 'POS and Morphological Feature Tagging: For each word in a sentence, Stanza assigns it as a part-of-speech (POS), and evaluates its universal morphological features (UFeats, e.g., singular/plural, 1st/2nd/3rd person, among others). To predict POS and UFeats, researchers adopted a bidirectional long short-term memory network (Bi-LSTM) as the basic architecture.', 'Lemmatization: Stanza also lemmatizes each word in a sentence to regain its canonical form (e.g., did→do). Similar to the multi-word token expander, Stanza’s lemmatizer is deployed as an ensemble of a dictionary-based lemmatizer and a neural seq2seq lemmatizer. Besides, an additional classifier is built on the encoder output of the seq2seq model, to predict shortcuts like lowercasing and identity copy for robustness on long input sequences such as URLs.', 'Researchers evaluated Stanza on a total of 112 datasets and figured out that its neural pipeline adapts to the text of different genres, resulting in obtaining state-of-the-art performance at each step of the pipeline. Besides, Stanza features a Python interface to the widely used Java CoreNLP software, thereby allowing access to richer functionalities like relation extraction and coreference resolution.', 'Stanza is open-source and has pre-trained models for all supported languages and datasets available for public download. Researchers hope Stanza can enable multilingual NLP research and applications, and drive new research that can produce insights from a wide range of human languages.', 'While Stanza supports a wide range of languages, it also extends its functionality to other NLP Python tools with its CoreNLP. However, there are a few things that researchers still have to improve to make it a go-to NLP library for processing different languages effectively.\xa0', 'Firstly, the downloadable Stanza models are only trained on a single dataset. Thus, to check its robustness, they need to train the models with data that are pooled from different sources. Secondly, the library is optimized for accuracy, which at times, comes at the cost of computational efficiency, limiting the toolkit’s use. Finally, the researcher will also have to make it compatible with different techniques of NLP, such as neural coreference resolution or relation extraction for richer text analytics.']",
133,nlp_framework_nlp_model/,https://analyticsindiamag.com/nlp-deep-learning-nlp-framework-nlp-model/,"['Spacy is an open-source software python library used in advanced natural language processing and machine learning. It will be used to build information extraction, natural language understanding systems, and to pre-process text for deep learning. It supports deep learning workflow in convolutional neural networks in parts-of-speech tagging, dependency parsing, and named entity recognition.', 'Spacy is mainly developed by Matthew Honnibal and maintained by Ines Montani. Scipy is written in Python and Cython (C binding of python). We can use more than 60 languages available for text processing such as English, Hindi, Spanish, German, French, Dutch. Mainly focus on industrial purpose. In contrast, NLTK is mainly used for research purposes and to learn natural language processing.', 'Introduction to Natural Language Processing', 'It is a technique using python and open source library for Extract information from unstructured text, to identify “named entities”, Analyze word structure in text, including parsing and semantic analysis access popular text databases, including WordNet and treebanks, Integrate techniques drawn from fields deep learning and artificial intelligence. And predictive text and email filtering to automatic summarization and translation.', 'Spacy v1:', 'It is the first version of Spacy released in February 2015. It includes nominal features of natural language processing, such as stemming, tokenization, and lemmatization, and some other features.', 'Spacy v2:', 'Spacy is the stable version released on 11 December 2020 just 5 days ago. It is built for the software industry purpose. It supports much entity recognition and deep learning integration for the development of a deep learning model and many other features include below.', 'Features:', 'Installation:', 'Scipy can be installed using setuptools and wheel.', 'Using pip:', 'pip install -U pip setuptools wheel', 'pip install spacy', 'Source: link ', 'Using conda:', 'conda install -c conda-forge spacy', 'Source: link ', 'To use spacy you are required to install the model using the pip command:', '$ python -m spacy download en_core_web_sm', 'Source code: https://spacy.io/models', 'Spacy Pipeline:', 'Word tokens are the basic units of text involved in any NLPlabeling task. The first step, when processing text, is to split it into tokens.', 'Import the Spacy language class to create an NLP object of that class using the code shown in the following code. Then processing your doc using the NLP object and giving some text data or your text file in it to process it. Select the token you want to print and then print the output using the token and text function to get the value in text form.', '# Import the English language class and create the NLP object\xa0', 'Source code: https://course.spacy.io/en/chapter1', '2. Parts of speech tagging:', 'When we learn basic grammar, we understand the difference between nouns, verbs, adjectives, and adverbs, and although it may seem pointless at the time, it turns out to be a critical element of Natural Language Processing.', 'Spacy provides convenient tools for breaking down sentences into lists of words and then categorizing each word with a specific part of speech based on the context.', 'Here is the below code to get the P.O.S:', 'Import the Spacy, and load model then process the text using nlp object now iterate over the loop to get the text->POS->dependency label as shown in the code.', '3. Name Entity Detection:', 'one of the most common labeling problems is finding entities in the text. Typically Name Entity detection constitutes the name of politicians, actors, and famous locations, and organizations, and products available in the market of that organization.', 'Just import the spacy and load model and process the text using the nlp then iterate over every entity and print their label.', '4. Dependency parsing:', 'The main concept of dependency parsing is that each linguistic unit (words) is connected by a directed link. These links are called dependencies in linguistics.', '#import the spacy and displacy to visualize the dependencies in each word.', 'Code:', '5. Matcher:', 'The Matcher is very powerful and allows you to bootstrap a lot of NLP based tasks, such as entity extraction, finding the pattern matched in the text or document.', 'Same as the above code, import the spacy, Matcher and initialize the matcher with the doc and define a pattern which you want to search in the doc. Then add the pattern to the matcher. Then print matches in the matcher docs.', 'Look at the below code for clarity.', 'Code:', 'Github:https://github.com/explosion/spaCy', 'Summary:']","'pip install -U pip setuptools wheel'
 'pip install spacy', 'conda install -c conda-forge spacy', '$ python -m spacy download en_core_web_sm', 'Output: I'"
134,using_scattertext_nlp_tool/,https://analyticsindiamag.com/visualizing-sentiment-analysis-reports-using-scattertext-nlp-tool/,"['Natural Language Processing allows the computer to understand the human language with the help of different modules/packages that python provides. NLP can practically be used for Speech Recognition, creating voice search engines, etc. NLP can be used to perform a large variety of operations on text data like tokenizing, lamenting, stemming POS tagging, etc.', 'Spacy is an NLP based python library that performs different NLP operations. Some of its main features are NER, POS tagging, dependency parsing, word vectors. Also, it contains models of different languages that can be used accordingly.', 'Scattertext is an open-source python library that is used with the help of spacy to create beautiful visualizations of what words and phrases are more characteristics of a given category. It is a tool for finding distinguishing terms in corpora and presenting them in an interactive, HTML scatter plot. Scattertext visualizations are highly informative because in the visualization the points corresponding to terms are selectively labeled so that they don’t overlap with other labels or points.', 'In this article, we will draw a sentiment analysis visualization using spacy and scatter text and see how beautifully scatter text allows you to visualize and find text in the data.', 'We will start by installing spacy and scattertext using pip install spacy and pip install scattertext respectively.', 'We will be importing spacy and scattertext for visualization and pandas for loading our dataset.', 'import spacy', 'import pandas as pd', 'import scattertext as st', 'For creating a sentiment analysis visualization we will import ‘Twitter Airline Sentiment Dataset’ from Kaggle. The dataset contains different attributes like Username, tweet, id, text, etc. We will use the data to visualize the different terms used for different sentiments.', ""twitter_df = pd.read_csv('Tweets.csv')"", 'twitter_df.dtypes', 'As we have already discussed, spacy contains models for different languages. We will use spacy and download the English model as we are working in the English Language.', ""nlp = spacy.load('en')"", 'Next, we will create a scattertext corpus of the dataset we are working on As we are working on the sentiment analysis we will set the category_col to ‘airline_sentiment’, and the text column which contains tweets will be used as text_col.', ""corpus = st.CorpusFromPandas(twitter_df,\xa0category_col='airline_sentiment',                       \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0        text_col='text',\xa0\xa0nlp=nlp).build()"", 'For creating this corpus we have used the NLP as the English model which we downloaded in the previous step, and create it using the build() function.', 'This is the main and the final step. Here we will create a visualization with the following parameters:', 'Now let us define all these and create the visualization using produce_scattertext_explorer.', 'sent = st.produce_scattertext_explorer(corpus,', ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0category='negative',"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0category_name='Negative',"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0not_category_name='Positive',"", '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0width_in_pixels=1000,', ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0metadata=netflix_df['name'])"", 'This command will create the desired visualization and we will write this into an Html file that can be run standalone.', 'open(“Twitter_Sentiment.html"", \'wb\').write(html.encode(\'utf-8\'))', 'This is the final visualization we created using scattertext.', 'In the visualization, we can clearly see that X-Axis displays the positive frequency and the y-axis displays the negative frequency. The axis is divided into three sections namely:', 'We can also see that the visualization contains the ‘Top Negative Words’, ‘Top Positive Words’, and the ‘Characteristics’ also. Other than this we can see that there is a search bar that is used to search a word in the corpus and display its frequency along with the text where it is used.', 'Let us search the word ‘hour’ and see the results.', 'Here we can see it clearly that the search results display the frequency of the word in the negative and the positive texts along with some of the tweets where this word is used.\xa0', 'The visualization created is highly interactive i.e. when you hover over any word in the visualization it displays its frequency along with score as a tooltip, and no word overlaps any other word.\xa0']","'import spacy'
 'import pandas as pd', 'import scattertext as st', ""twitter_df = pd.read_csv('Tweets.csv')"", 'twitter_df.dtypes', ""nlp = spacy.load('en')"", ""corpus = st.CorpusFromPandas(twitter_df,\xa0category_col='airline_sentiment',                       \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0        text_col='text',\xa0\xa0nlp=nlp).build()"", 'sent = st.produce_scattertext_explorer(corpus,', ""category='negative',"", ""category_name='Negative',"", ""not_category_name='Positive',"", 'width_in_pixels=1000,', ""metadata=netflix_df['name'])"", 'open(“Twitter_Sentiment.html"", \'wb\').write(html.encode(\'utf-8\'))'"
135,ml_models_with_pytorch/,https://analyticsindiamag.com/how-to-use-torchbearer-for-fitting-ml-models-with-pytorch/,"['For TensorFlow based models, we have Keras which provides access to all the complex functionality of the TensorFlow preprocessing, modelling, and managing callbacks in the form of simple and High-level API. In this similar space for PyTorch, there is a library called Torchbearer which is basically a model fitting library for PyTorch models and offers a high-level metric and callback API that can be used in a variety of applications. In this article, we are going to discuss this library in detail with its hands-on implementations. We will also see how this framework helps in interpreting the fitted machine learning models. Below are the major points that we are going to cover in this article.', 'Let’s first understand the need for this library.\xa0', 'Deep learning’s meteoric rise has spawned a slew of frameworks that enable hardware-accelerated tensor processing and automatic differentiation. Differentiable programming, a more broad characterization, has gradually taken its place. Fitting is a method that involves maximizing the parameters of a differentiable algorithm using gradient descent. Pytorch is one library that has grown in popularity in recent years, thanks to its ease of use in creating models that execute non-standard tensor operations.', 'This makes PyTorch particularly suitable for research projects in which any aspect of a model definition may need to be changed or modified. However, because PyTorch focuses solely on tensor processing and automated differentiation, it lacks the high-level model-fitting API found in other frameworks like Keras. Furthermore, such libraries rarely allow differentiable programming in its broadest sense.', 'As a result, the torchbearer, a Python library that supports research by speeding the model fitting process while maintaining transparency and generality. A model fitting API is at the heart of the torchbearer, allowing for easy customization of all aspects of the fitting process. We also offer a robust metric API that allows you to collect rolling information and averages. Finally, torchbearer has a number of advanced callbacks.', 'The torchbearer library is written in Python and uses PyTorch, torchvision, and tqdm, with some functionality provided by NumPy, sci-kit-learn, and tensor board. Torchbearer’s core abstractions are trials, callbacks, and metrics.\xa0', 'Torchbearer is different from other similar libraries like ignite or tnt because of these design concepts. Neither library, for example, has a large number of built-in callbacks for sophisticated functions. Furthermore, both ignite and tnt use an event-driven API for model fitting, which makes the code less clear and legible for humans.', 'Let’s briefly discuss its major APIs.', 'The Trial class implements a PyTorch model fitting interface based on the Trial.run(…) method. There are also predict(…) and evaluate(…) methods for inferring models and assessing saved models. The Trial class also has a state dict method that returns a dictionary comprising the model parameters, optimizer state, and callback states, which can be stored and then reloaded using load_state_dict.', 'During the fitting procedure, the callback API defines classes that can be used to perform a variety of tasks. A torchbearer component provided to each callback is the mutable state dictionary, which holds intermediate variables required by the Trial. Callbacks can change the nature of the fitting process in real-time as a result of this. Callbacks can be implemented as decorated functions using the decorator API.', 'The metric API makes use of a tree to allow data to flow from one metric to a set of children. This enables the computation of aggregates such as the running mean or standard deviation. Assembling these data structures can be difficult, so torchbearer includes a decorator API to make it easier. The default_for_key(…) decorator allows the metric to be referenced in the Trial definition with a string.', 'In this section, we will try to implement the SVM model torch and will train, evaluate and visualize the hyperplane using the Torchbearer library. This example is taken from the official repository of the Torchbearer.', 'SVM seeks the hyperplane with the greatest margin of separation between the data classes. We minimize the following for a soft margin SVM where x is our data:', 'This can be expressed as an optimization over our weights w and bias b, where we minimize the hinge loss while accounting for a level 2 weight decay term.', 'Now before modelling this in PyTorch let’s first install and import the Torchbearer library.', 'After this let’s define the SVM and hinge loss function.', 'Now we’ll create and normalize the synthetic data that will be separated by the hyperplanes.', 'Now we’ll define the callbacks and visualization function.\xa0', 'We’d like to use a soft-margin SVM because we don’t know if our data is linearly separable. To accomplish this, we can use the L2WeightDecay callback in torchbearer. Because we only use a mini-batch at each step to approximate the gradient over all of the data, this entire process is known as subgradient descent. Now let’s train the model and see the result.\xa0', 'The plot result can be saved in the working directory as below.', 'And here are SVM’s hyperplanes that separate the above synthetic data seamlessly.']","' # install and import torchbearer\n!pip install -q torchbearer\nimport torchbearer\n'
 '# define SVM\nimport torch.nn as nn\n \nclass LinearSVM(nn.Module):\n    """"""Support Vector Machine""""""\n \n    def __init__(self):\n        super(LinearSVM, self).__init__()\n        self.w = nn.Parameter(torch.randn(1, 2), requires_grad=True)\n        self.b = nn.Parameter(torch.randn(1), requires_grad=True)\n \n    def forward(self, x):\n        h = x.matmul(self.w.t()) + self.b\n        return h\n\n# define the loss function\ndef hinge_loss(y_pred, y_true):\n    return torch.mean(torch.clamp(1 - y_pred.t() * y_true, min=0))\n', '# load data\nimport numpy as np\nfrom sklearn.datasets import make_blobs\n \nX, Y = make_blobs(n_samples=1024, centers=2, cluster_std=1.2, random_state=1)\nX = (X - X.mean()) / X.std()\nY[np.where(Y == 0)] = -1\nX, Y = torch.FloatTensor(X), torch.FloatTensor(Y)\n# normalize the data\ndelta = 0.01\nx = np.arange(X[:, 0].min(), X[:, 0].max(), delta)\ny = np.arange(X[:, 1].min(), X[:, 1].max(), delta)\nx, y = np.meshgrid(x, y)\nxy = list(map(np.ravel, [x, y]))\n', '# visualization function\n \nfrom torchbearer import callbacks\n \n%matplotlib notebook\nimport matplotlib\nimport matplotlib.pyplot as plt\n \n@callbacks.on_step_training\n@callbacks.only_if(lambda state: state[torchbearer.BATCH] % 10 == 0)\ndef draw_margin(state):\n    w = state[torchbearer.MODEL].w[0].detach().to(\'cpu\').numpy()\n    b = state[torchbearer.MODEL].b[0].detach().to(\'cpu\').numpy()\n \n    z = (w.dot(xy) + b).reshape(x.shape)\n    z[np.where(z > 1.)] = 4\n    z[np.where((z > 0.) & (z <= 1.))] = 3\n    z[np.where((z > -1.) & (z <= 0.))] = 2\n    z[np.where(z <= -1.)] = 1\n \n    plt.clf()\n    plt.scatter(x=X[:, 0], y=X[:, 1], c=""black"", s=10)\n    plt.contourf(x, y, z, cmap=plt.cm.jet, alpha=0.5)\n    fig.canvas.draw()\n', ""# train the model\nfrom torchbearer import Trial\nfrom torchbearer.callbacks import L2WeightDecay, ExponentialLR\n \nimport torch.optim as optim\n \ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n \nfig = plt.figure(figsize=(5, 5))\n \nsvm = LinearSVM()\nmodel = Trial(svm, optim.SGD(svm.parameters(), 0.1), hinge_loss, ['loss'],\n              callbacks=[draw_margin, ExponentialLR(0.999, step_on_batch=True), L2WeightDecay(0.01, params=[svm.w])]).to(device)\nmodel.with_train_data(X, Y, batch_size=32)\nmodel.run(epochs=50, verbose=1)\n"", ""#fig will be saved in the working directory\nfig.savefig('svm.png', bbox_inches='tight')\n"""
136,efficient_deep_learning_development/,https://analyticsindiamag.com/a-guide-to-tensorlayer-for-efficient-deep-learning-development/,"['Creating a functional deep learning system is a time-consuming and hard task. It entails time-consuming tasks like building sophisticated neural networks, coordinating many network models, data processing, creating a succinct workflow, and handling a significant volume of training-related data. There are currently tools available to aid in this development process, such as Keras and TFLearn, which provide flexibility and abstraction for multiple connected modalities. In this post, we’ll look at TensorLayer, a Python-based machine learning tool. Below are the major points listed that are to be discussed in this post.', 'Let’s first understand the need for this tool.', 'The increasing interaction challenges the development of deep learning. Many cycles must be spent by developers integrating components for experimenting with neural networks, handling intermediate training stages, organizing training-related data, and enabling hyperparameter adjustment in reaction to various events.\xa0', 'To reduce the number of cycles necessary, an integrative development method is used, in which complex operations on neural networks, states, data, and hyper-parameters are abstracted and given in complementing modules. This results in a single environment in which developers may efficiently explore concepts via high-level module operations and apply changes to modules only when necessary.', 'This strategy is not intended to create module lock-in. Modules are instead modelled as simple single-function blocks that share an interaction interface, allowing for easy plug-ins of user-defined modules.', 'TensorLayer is a collaborative effort to realize this objective. It is a modular Python toolkit that provides simple modules to help academics and engineers build complicated deep learning systems. The TensorLayer implementation is designed to be fast and scalable. TensorFlow is used as the distributed training and inference engine.\xa0', 'The overhead associated with delegation into TensorFlow is small. TensorLayer also makes use of MongoDB as a storage backend. For managing unbounded training data, this backend is supplemented with an efficient stream controller. This controller can batch results from a dataset query and generates batch training tasks as needed to support automation.', 'TensorLayer employs GridFS as a blob backend and MongoDB as a sample indexer to efficiently handle huge data items such as videos. Finally, TensorLayer employs an agent pub-sub architecture in order to achieve an asynchronous training workflow. Agents can be installed on several types of devices and subscribe to separate task queues. These queues are kept in dependable storage so that failed tasks can be automatically replayed.', 'TensorLayer, unlike other TensorFlow-based tools like Keras and TFLearn, allows for simple low-level control over the execution of layers and neural networks. It also includes additional dataset and workflow modules, which relieve users of time-consuming data pre-processing, post-processing, module serving, and data administration duties. Its non-invasive unified module interaction interface accepts layers and networks imported from Keras and TFLearn.', 'Helper functions include providing and importing layer implementations, establishing neural networks, handling states involved in model life-cycles, producing online or offline datasets, and developing a parallel training plan. Layer, model, dataset, and workflow are the four modules that contain these functions. These modules are described in turn in the sections that follow. We’ll go over them one by one.', 'TensorLayer features a layer module with reference implementations of a variety of layers, including CNN, RNN, dropout, batch normalization, and many more. Similar to the widely used Lasagne, layers are built to form a neural network in a declarative manner. To aid developers with parameter sharing, each layer is given its own key. TensorFlow is in charge of the networks. TensorLayer is a hybrid and distributed platform that inherits from TensorFlow.', 'Models are logical representations of self-contained functional units that can be trained, assessed, and deployed in the field. Every model has its own network structure. Various versions or states of the model can exist throughout training (i.e., weights). Persisted, cached, and reloaded states are all possible.\xa0', 'User-defined model events can be recorded with TensorLayer. Training steps, learning speed, and accuracy are all reflected in traditional competitions. They are frequently used to diagnose a training process in order to enable model versioning and interactive learning, for example.', 'The dataset module is where you keep track of your training samples and predictions. They’re saved in MongoDB as documents. A unique key, sample, label, and user-defined tags are all included in each document.', 'Declarative queries that carry requirements to tag fields are used to define datasets. Queries create views of the underlying data and do not require additional storage.', 'General streaming datasets are used to model the data. A stream controller is assigned to each dataset, which constantly monitors the availability of samples and predictions and subsequently triggers the appropriate training activities for that dataset.', 'The workflow module makes it easier to construct model group operations and learning systems that use asynchronous feedback loops. It is also effective for complicated cognitive systems with components that require training. For example, the creator of an image captioning system [28] trained a CNN to grasp the context of images before training an RNN decoder to generate descriptions based on the detected context. This example builds a two-stage asynchronous training plan that TensorLayer can support.', 'In this section, we’ll perform the image classification using transfer learning. The model used here is VGG16. To perform this classification we just need to install a package of tensor layers and the rest of the things are managed by the package.\xa0', 'Let’s now quickly install and import dependencies.', 'Now first thing first, the model can be imported from tensorlayer.model package. After loading the model the code outputs the model summary.', 'Here is a summary of the model.', 'Now next we have to load and pre-process the image as the model runs on predefined image settings.\xa0', 'Here is the image that we are feeding.', 'Now we’ll process the image for prediction.', 'The results are obtained in the form of probability of classes that are identified in the image and are arranged in decreasing order.']","'! pip install tensorlayer\n\n# interdependencies required for image pre-processing\n! pip install scipy==1.2.1\n\nimport numpy as np\nimport tensorflow as tf\n \nimport tensorlayer as tl\nfrom tensorlayer.models.imagenet_classes import class_names\n'
 '# get the whole model\nvgg = tl.models.vgg16(pretrained=True)\n', ""#image loading, pre-processing\nimg = tl.vis.read_image('/content/steam-train-rides-1570200690.jpg')\nimg = tl.prepro.imresize(img, (224, 224)).astype(np.float32) / 255\n"", '#process the image to the model and get probas\noutput = vgg(img, is_train=False)\nprobs = tf.nn.softmax(output)[0].numpy()', '# print result\npreds = (np.argsort(probs)[::-1])[0:5]\nfor q in preds:\n    print(class_names[q], probs[q])\n'"
137,other_machine_learning_libraries/,https://analyticsindiamag.com/what-is-tensorlayer-and-how-is-it-different-from-tensorflows-other-machine-learning-libraries/,"['', 'The development of projects in areas like machine learning, deep learning and reinforcement learning is experiencing a lot of growth with more scalability and better research insights. The credit goes to the software development community along with tech companies as well, which makes it possible to reach out to more aspiring programmers and engineers, by open-sourcing most of their work.', 'We explore one such open-source DL and RL software library called TensorLayer, which is a part of Google’s popular machine learning and numerical computational framework TensorFlow. The idea behind the new library was to facilitate a modular approach to DL as well as RL to tackle complexity and iterative tasks when it comes to large neural networks and their interactions. It was first released in 2016 and gradually adopted changes along the way to become the most sought after libraries for DL.The entire code for TensorLayer is written in Python – the most preferred programming language for ML.', 'The core of the TensorLayer working follows a modular approach (as shown in the image). The key functions covered with regard to DL are, building neural networks, layer implementations, gathering & creating datasets and, finally structuring a training plan for proper working of the library in light of any failures while performing learning tasks. The highlighting feature of TensorLayer lies in the integrated development environment(IDE)-like approach where the host of operations such as neural networks, their states, data and other parameters are assorted into modules with an abstraction level. This allows other developers to customise with respect to the specific inter-connected areas such as front-end applications or back-end servers by providing an interactive modular interface.', '\xa0', 'In the words of the developers of TensorLayer, the modules are again classified on four aspects which are described below.', 'TensorLayer relies on TensorFlow’s computational engine for training and uses MongoDB as the storage backend. MongoDB is chosen because of its advantage over other storage tools which use SQL methods such as simpler interface and better flexible features. ', 'One critical factor that needs much attention is performance. When compared with TensorFlow, TensorLayer closely fares the same when classic ML models of neural networks are performed. Although this might look like a setback, it will allow the layer module in TensorLayer to cater \xa0customised models. On the other hand, while handling datasets it uses indexing concepts for quicker row selection in datasets. Also, the cache is stored locally to handle workloads with larger data split into smaller bits for optimal performance. In addition, this also helps the workflow module to implement DL models using ‘asynchronous training. ', 'TensorLayer users have found ways to explore various machine learning areas which have seen greater impact when it comes to efficiency. Here are a few specific sub-machine learning areas.', 'In the paper published by the team who developed TensorLayer, the applications concentrate on adversarial learning such as Deep Convolutional Generative Adversarial Network (DCGAN) and Super-Resolution Generative Adversarial Network (SRGAN), which are gaining popularity in multimedia deep learning. In addition, TensorLayer’s application is also extended to address domain-specific problems by hyperparameter optimisation and cross-validation, in areas such as medical signal processing which require frequent maintenance and development in the ML models. ', 'The latest version of TensorLayer package requires TensorFlow to be installed on the computer prior to implementation. It is also suggested to download and install Python3 to incorporate any new advancements in Python language. The detailed instructions for installation can be found here. TensorLayer is available on Windows, MacOS and Linux platforms. It is an open-source library.']",
138,neural_networks_using_adanet/,https://analyticsindiamag.com/how-to-ensemble-neural-networks-using-adanet/,"['Ensembling is a mechanism in machine learning that serves as the foundation for a variety of powerful algorithms. In general, ensembling is a learning technique in which many models are combined to solve a problem. Similarly, in deep learning, the ensemble can be used where nonlinearity is high and single architectural-based models perform poorly most of the time. In the context of ensemble learning, we will discuss what ensemble learning is and how it can be used in deep learning using the AdaNet framework in this article. The main points to be covered in this article are listed below.', 'Let’s start the discussion by understanding what an ensemble is.\xa0', 'In machine learning, ensemble approaches combine many weak learners to achieve better prediction performance than each of the constituent learning algorithms alone. A machine learning ensemble, in contrast to a statistical ensemble in statistical mechanics, which is usually infinite, consists largely of a relatively small number of potential models but allows for a significantly more flexible structure to exist within those possibilities.', 'A hypothesis space is searched by supervised learning algorithms for a suitable hypothesis that will deliver accurate predictions for a particular circumstance. Even when the hypothesis space contains hypotheses that are ideally suited to a certain scenario, choosing the optimal one might be challenging. Ensembles integrate many hypotheses to create a new (hopefully superior) hypothesis.', 'The word “ensemble” refers to approaches that use the same underlying learner to create several hypotheses. Multiple classifier systems is a larger phrase that includes hybridization of hypotheses that are not driven by the same base learner.', 'While there are nearly infinite ways to accomplish this, perhaps three classes of ensemble learning techniques are most commonly discussed and implemented in practice. Their popularity is due to their ease of use and ability to solve a wide variety of predictive modelling problems. The three methods are bagging, stacking, and boosting. We’ll go over each one briefly now.', 'The generation of so-called bootstrapped data sets is the first step in the bagging process. The number of elements chosen in each bootstrapped set is the same as in the original training dataset, but elements are chosen at random with replacement.\xa0', 'As a result, in a given bootstrapped set, a given sample from the original training set may appear zero, one, or multiple times. Out-of-bag sets are another byproduct of the bootstrapping process. It is a statistical technique for calculating the statistical value of a data sample using small datasets. Producing several distinct bootstrap samples, estimating a statistical quantity, and determining the mean of the estimates can result in a better overall estimate of the desired quantity.', 'Similarly, a large number of training datasets can be prepared, estimated, and projected. Predictions from many models are typically superior to predictions from a single model fitted directly to the training dataset.', 'The technique of training a learning algorithm to incorporate the predictions of several learning algorithms is known as stacking (also known as a stacked generalization). All of the other algorithms are trained first using the available data, and then a combiner algorithm is taught to make a final forecast using the predictions of all of the other algorithms as supplementary inputs.\xa0', 'If an arbitrary combiner technique is used, stacking might reflect any of the ensemble approaches, however, in fact, the combiner is usually a logistic regression model. In the vast majority of circumstances, stacking outperforms employing a single trained model. It’s been proved to work in both supervised and unsupervised learning environments (regression, classification, and distance learning).', 'Boosting entails forming an ensemble one step at a time by training each new model instance to emphasize the training examples that prior models misclassified. Boosting has been demonstrated to be more accurate than bagging in some circumstances, but it also has a higher risk of overfitting the training data. Although some newer algorithms are said to generate greater results, Adaboost is by far the most prevalent implementation of boosting.', 'At the very first round of boosting, the sample training data is assigned an identical weight (uniform probability distribution). After that, the data is delivered to a base learner (say L1). The misclassified occurrences by L1 are given a larger weight than the correctly classified examples, but the total probability distribution remains the same. This boosted data is then sent on to the second base learner (let’s call it L2), and so on. Following that, the results are pooled in the form of voting.', 'AdaNet is a TensorFlow-based lightweight framework for learning high-quality models automatically with minimum expert interaction. AdaNet provides a comprehensive framework for learning not only neural network design but also how to ensemble models to get even better results.', 'AdaNet is simple to use and produces high-quality models, saving ML practitioners time by creating an adaptive method for learning a neural architecture as an ensemble of subnetworks and saving ML practitioners the time spent identifying ideal neural network topologies. AdaNet can generate a varied ensemble by adding subnetworks of various depths and widths, and it can trade off performance gain with the number of parameters.', 'AdaNet offers a one-of-a-kind adaptive computation graph that can be used to build models that add and remove operations and variables over time while maintaining the optimizations and scalability of TensorFlow’s graph model. Users can use this adaptive graph to create progressively growing models (such as boosting style), architecture search algorithms, and hyper-parameter tuning without having to manage an external for-loop.', 'Now further we will discuss two important mechanisms of AdaNet which are responsible for the ensembling.', 'Ensembles are the key first-class objects in AdaNet. Every model you train will be a part of some sort of ensemble. An ensemble is made up of one or more subnetworks, each of which has its outputs combined by an ensembler (Shown below Figure).\xa0', 'Because ensembles are model-independent, a subnetwork can be as complex as a deep neural network or as simple as an if-statement. All that matters is that the ensembler can combine the subnetworks’ outputs to form a single prediction for a given input tensor.', 'The AdaNet method iteratively executes the following architecture search to construct an ensemble of subnetworks in the animation shown below:']",
139,declarative_deep_learning_toolbox/,https://analyticsindiamag.com/ludwig-a-type-based-declarative-deep-learning-toolbox/,"['Declarative machine learning (ML) attempts to automate the generation of efficient execution plans from high-level ML problems or method specifications. The overriding goal is to make ML methods easy to use and/or construct, which is especially important in the context of complex applications. In this article, we will have a look into what declarative learning is and how a Toolbox called Ludwig built by UberAI can be used in the context of it. The major points to be discussed in this article are listed below.', 'Let’s start the discussion by understanding what declarative learning is in machine learning.', 'Declarative ML intends to simplify the usage and/or creation of ML algorithms by isolating application or algorithm semantics from the underlying data representations and execution plans, resulting in a high-level definition of ML tasks or algorithms. The following are the certain properties of declarative ML:', 'Ludwig is a deep learning toolbox built on the level of abstraction indicated above, with the purpose of encapsulating best practices and exploiting inheritance and code modularity. Ludwig makes it easier for practitioners to create deep learning models by simply stating their data and tasks, as well as to reuse, extend, and favour best practices.\xa0', 'The data types of the inputs encoded by the encoding functions (picture, text, series, category, etc.) and the data types of the predicted outputs by the decoding functions are used to name these equivalence classes.\xa0', 'This type-based abstraction allows for a more high-level interface than is currently provided in deep learning frameworks, which abstract at the tensor operation or layer level. This is achieved by providing abstract interfaces for each data type, allowing for an extension by allowing any new implementation of the interface to be produced.', 'Ludwig is based on the concept of a declarative model specification, which allows a much broader audience (including non-programmers) to use deep learning models, thereby democratizing them.', 'One of the fundamental elements that define Ludwig’s design is a type-based abstraction. Ludwig currently supports the following types: binary, numerical (floating-point values), category (unique strings), set of categorical elements, a bag of categorical elements, sequence of categorical elements, time series (sequence of numerical elements), text, image, audio (which doubles as speech when different preprocessing parameters are used), date, H3 (a geospatial indexing system), and vector (one dimensional tensor of numerical values). It’s simple to create more types thanks to the type-based abstraction.', 'Every model in Ludwig is made up of encoders that encode various aspects of an input data point, a combiner that combines the information from the various encoders, and decoders that decode the information from the combiner into one or more output features. Encoders-Combiner-Decoders is the generic name for this design (ECD). The figure below shows a representation of the situation.', '(Encoder-Combiner-Decoder Architecture)', 'Because it maps most deep learning model architectures and allows for modular building, this architecture is provided. Instead of constructing an entire model from scratch, data type abstraction allows you to define models by simply describing the data types of the input and output characteristics involved in the task and building standard sub-modules appropriately.', 'An ECD architecture instantiation can contain many input features of different or the same type, and the same is true for output features. Pre-processing and encoding functions are computed for each feature in the input portion based on the type of the feature, while decoding, metrics, and post-processing functions are computed for each feature in the output part based on the type of each output feature.', 'The ECD design allows for numerous instantiations, as demonstrated in Figure below, by combining distinct input qualities of different data types with diverse output features of other data types. An ECD with a text input feature and a categorical output feature can be trained to do text classification or sentiment analysis, while an ECD with a text input feature and a text output feature can be trained to do image captioning, and an ECD with categorical, binary, and numerical input features and a numerical output feature can be trained to do regression tasks like house pricing prediction.', '(ECD architecture for different machine learning tasks)', 'Training our desired deep learning models is now way too easy using Ludwig. For training and testing, we tend to write a lot of code for the steps like pre-processing, model building, etc. But here in Ludwig we just need to define a model configuration file which is basically a YAML configuration file that specifies which tabular file columns are input characteristics and which are output target variables Although YAML stands for “YAML ain’t a Markup Language,” it is a human-readable data serialization language. It’s frequently used in configuration files and data storage and transmission applications. It is a file containing input and output definitions, as well as other parameters.', 'Before defining configuration take a look at the dataset that I have used here which is taken from this Kaggle repository and is about Heart Failure Prediction.', 'Now, based on the dataset, our model config file looks like below. Here I have defined two output features.\xa0', 'input_features:', '\xa0\xa0\xa0\xa0–', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0name: Sex', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0type: category', '\xa0\xa0\xa0\xa0–', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0name: ChestPainType', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0type: category', '\xa0\xa0\xa0\xa0–', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0name: RestingBP', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0type: numerical', '\xa0\xa0\xa0\xa0–', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0name: Cholesterol', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0type: numerical', '\xa0\xa0\xa0\xa0–', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0name: RestingECG', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0type: category', '\xa0\xa0\xa0\xa0–', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0name: ST_Slope', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0type: category', '\xa0\xa0\xa0\xa0–', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0name: Age', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0type: numerical', 'output_features:', '\xa0\xa0\xa0\xa0–', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0name: HeartDisease', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0type: binary', '!ludwig train \\', '\xa0\xa0–dataset ‘/content/heart.csv’ \\', '\xa0\xa0–config_file /content/config_file.yaml', 'After the complete execution of the above code, you can see the result. Here is the accuracy at the 1st epoch and the 100th epoch depicted below.']","'! pip install ludwig'
 '! pip install petastorm'"
140,toolkit_for_neural_networks/,https://analyticsindiamag.com/a-guide-to-chainer-a-flexible-toolkit-for-neural-networks/,"['Implementing neural networks necessitates the use of a variety of specialized building elements, such as multidimensional arrays, activation functions, and automatic differentiation. We employ numerous frameworks such as Tensorflow, Pytorch, Theano, and others to avoid the complexity we confront if we choose to develop those things from scratch. In this post, we will look at a framework called Chainer and will understand how it is better than the traditional frameworks. We will also try to understand this superiority by implementing the Chainer framework in Python.', 'Here in this post, we will focus mostly on the following two points:-', 'Let us begin by defining the difference between the present framework’s approach and Chainer’s approach.', 'In typical neural network frameworks, models are frequently built in two phases, using a Define-and-Run technique as shown in Figure 1(a). During the Define phase, the model’s computational graph is constructed and developed. This stage involves generating a neural network object using a model definition that includes the inter-layer data flow graph, beginning weights, and activation functions.', 'The computations for both the forward and backward passes are commonly defined using automatic differentiation, with optional graph optimizations. The real forward and backward calculation of the graph is done in the Run phase. In this phase, the model is trained by minimizing the loss function using optimization algorithms such as stochastic gradient descent, given a set of training instances.', 'Fig 1 (Source)\xa0', 'For static models like CNNs, the Define-and-Run paradigm works well since having the whole computational graph available allows for potential graph improvements to enhance memory efficiency and/or runtime performance. However, there are two key issues with applying various types of NN models.', 'Chainer, on the other hand, uses a “Define-by-Run” approach as shown in Figure 1(b), in which the network is defined dynamically through the real forward computation. Chainer, rather than storing programming logic, stores the history of computation. This method allows us to fully utilize Python’s programming logic capabilities. Chainer, for example, does not require any magic to add conditionals and loops to network definitions. Chainer’s key notion is the Define-by-Run scheme.', 'Here in this section, we will implement the CNN-based image classifier on the CIFAR10 dataset. We will try to explore various functionality given by the Chainer.\xa0', 'Our model is defined as a Chain subclass. Three convolutional layers will be followed by two fully linked layers in our CNN model. Despite the fact that this is still a modest CNN. Each layer of a neural network is divided into one of two sorts of functions (really, function objects) in Chainer: ‘Link’ and ‘Function.’', 'Chainer’s Link can be thought of as a wrapper around a function that allows us to pass parameters to it. That is, when Link is called, it will also call the associated Function.', 'Then we implement code that conducts the “forward pass” computations to describe a model. Various connections and chains will be called by this code (recall that Link and Chain are callable objects). Chainer will automatically handle the “backward pass,” so we won’t have to worry about it unless we wish to create some special functions.', 'Let’s create a ‘train’ function that we can use to quickly train other models in the future. This function accepts a model object and trains it to categorize the 10 CIFAR10 classes before returning the trained model. This train function will be used to train the MyModel network described previously.', 'We need to construct batches of our dataset before we can train. Chainer already provides an Iterator class and various subclasses that can be used for this purpose, and users can easily create their own as well.', 'In this example, we’ll use the SerialIterator Iterator subclass. The SerialIterator can either return the examples in the same order as they occur in the dataset (that is, sequentially) or it can shuffle the instances and return them in random order.', 'The complete training loop can be coded as below', 'We have trained the network for 20 epochs and loss and accuracies is obtained as below.', 'As you can see there is much difference between training accuracies and validation accuracies it seems like the model has overfitted the dataset.', 'Let’s examine how well our CNN performs as we add more layers to it. We’ll also make our model modular by writing it as a three-chain combination. This will aid in the improvement of readability and the reduction of code duplication: ConvBlock – a single completely connected neural net, – a single convolutional neural net, – a single fully connected neural net, – a single fully connected neural net, – a single fully connected neural LinearBlock – Make a full model by chaining together a lot of these two blocks.', 'ConvBlock is specified as a descendant of Chain. It has a single convolution layer and a Batch Normalization layer, both of which were registered by the constructor. The __call__ method receives data and performs an activation function on it. The Max Pooling and Dropout methods are also used if the pool drop is set to True.', 'Let’s now stack the component blocks to define the deeper CNN network.', 'Now we all have settled Deeper CNN. Let’s train and observe the accuracy and loss.', 'In comparison to the previous smaller CNN, the accuracy on the test set has much improved. The accuracy used to be around 61 percent, but today it’s over 86 percent. To boost accuracy even further, we should not only improve the models layer but also to increase the training data (data augmentation) or to integrate different models to achieve the best results (Ensemble method)']","'import chainer\nimport chainer.functions as F\nimport chainer.links as L\n \nclass MyModel(chainer.Chain):\n \n    def __init__(self
 n_out):\n        super(MyModel, self).__init__()\n        with self.init_scope():\n            self.conv1=L.Convolution2D(None, 32, 3, 3, 1)\n            self.conv2=L.Convolution2D(32, 64, 3, 3, 1)\n            self.conv3=L.Convolution2D(64, 128, 3, 3, 1)\n            self.fc4=L.Linear(None, 1000)\n            self.fc5=L.Linear(1000, n_out)\n \n    def __call__(self, x):\n        h = F.relu(self.conv1(x))\n        h = F.relu(self.conv2(h))\n        h = F.relu(self.conv3(h))\n        h = F.relu(self.fc4(h))\n        h = self.fc5(h)\n        return h\n', ""from chainer.datasets import cifar\nfrom chainer import iterators\nfrom chainer import optimizers\nfrom chainer import training\nfrom chainer.training import extensions\n \ndef train(model_object_, batch_size=64, gpu_id=0, Max_epoch=20):\n \n    # 1. Get the Dataset\n    train_set, test_set = cifar.get_cifar10()\n \n    # 2. Create a Serial Iterator for Train and test data\n    train_iter_set = iterators.SerialIterator(train_set, batch_size)\n    test_iter_set = iterators.SerialIterator(test_set, batch_size, False, False)\n \n    # 3. Use classifier from chainer's Link\n    model_ = L.Classifier(model_object_)\n    if gpu_id >=0:\n        model_.to_gpu(gpu_id)\n \n    # 4. Set optimization\n    opti_ = optimizers.Adam()\n    opti_.setup(model_)\n \n    # 5. Update weights\n    updater_ = training.StandardUpdater(train_iter_set, opti_, device=gpu_id)\n \n    # 6. Train network\n    trainer = training.Trainer(updater_, (Max_epoch, 'epoch'), out='{}_cifar10_result'.format(model_object_.__class__.__name__))\n \n    # 7. Evaluate network\n    class Test_Mode_Evaluator(extensions.Evaluator):\n \n        def evaluate(self):\n            model_ = self.get_target('main')\n            ret = super(Test_Mode_Evaluator, self).evaluate()\n            return ret\n \n    trainer.extend(extensions.LogReport())\n    trainer.extend(Test_Mode_Evaluator(test_iter_set, model_, device=gpu_id))\n    trainer.extend(extensions.PrintReport(['epoch', 'main/loss', 'main/accuracy', 'validation/main/loss', 'validation/main/accuracy', 'elapsed_time']))\n    trainer.extend(extensions.PlotReport(['main/loss', 'validation/main/loss'], x_key='epoch', file_name='loss.png'))\n    trainer.extend(extensions.PlotReport(['main/accuracy', 'validation/main/accuracy'], x_key='epoch', file_name='accuracy.png'))\n    trainer.run()\n    del trainer\n \n    return model_\n \ngpu_id = 0  # Set -1 if don't have a GPU\n \nmodel = train(MyModel(10), gpu_id=gpu_id)\n"", 'class ConvBlock(chainer.Chain):\n \n    def __init__(self, n_ch, pool_drop=False):\n        w = chainer.initializers.HeNormal()\n        super(ConvBlock, self).__init__()\n        with self.init_scope():\n            self.conv = L.Convolution2D(None, n_ch, 3, 1, 1,\n                                 nobias=True, initialW=w)\n            self.bn = L.BatchNormalization(n_ch)\n        self.pool_drop = pool_drop\n \n    def __call__(self, x):\n        h = F.relu(self.bn(self.conv(x)))\n        if self.pool_drop:\n            h = F.max_pooling_2d(h, 2, 2)\n            h = F.dropout(h, ratio=0.25)\n        return h\n \nclass LinearBlock(chainer.Chain):\n \n    def __init__(self):\n        w = chainer.initializers.HeNormal()\n        super(LinearBlock, self).__init__()\n        with self.init_scope():\n            self.fc = L.Linear(None, 1024, initialW=w)\n \n    def __call__(self, x):\n        return F.dropout(F.relu(self.fc(x)), ratio=0.5)\n', 'class DeepCNN(chainer.ChainList):\n \n    def __init__(self, n_output):\n        super(DeepCNN, self).__init__(\n            ConvBlock(64),\n            ConvBlock(64, True),\n            ConvBlock(128),\n            ConvBlock(128, True),\n            ConvBlock(256),\n            ConvBlock(256, True),\n            LinearBlock(),\n            LinearBlock(),\n            L.Linear(None, n_output)\n        )\n \n    def __call__(self, x):\n        for f in self.children():\n            x = f(x)\n        return x\n', ""gpu_id = 0  # Set to -1 if you don't have a GPU\nmodel = train(DeepCNN(10), gpu_id=gpu_id)\n"""
141,python_for_big_data/,https://analyticsindiamag.com/a-guide-to-dask-parallel-computing-tool-in-python-for-big-data/,"['When you open a large Dataset with Python’s Pandas and try to get a few metrics, the entire thing just stops badly. If you work with Big Data on a regular basis, you’re probably aware that if you’re using Pandas, a simple loading of a series for a couple of million rows can take up to a minute! In the industry, the term/technique parallel computing is used for this. In relation to parallel computing, we will cover parallel computing and the Dask library, which is preferred for such tasks in this article. We will also go through different machine learning features as well available with Dask. The following are the main points to be discussed.', 'Let’s start by understanding parallel computing.', 'Parallel computing is a sort of computation that performs several calculations or processes at the same time. Large problems are frequently broken into smaller ones that can be tackled simultaneously. Parallel computing can be divided into four types: bit-level, instruction-level, data, and task parallelism. Parallelism has long been used in high-performance computing, but it has recently gained traction due to physical limitations that restrict frequency growth.', 'The software divides the problem into smaller problems or subtasks as soon as it starts running. Each subtask is completed independently, with no outside intervention, and the results are then combined to produce the final output.', 'Parallel computing and concurrent computing are commonly confused and used interchangeably, but the two are distinct: parallelism can exist without concurrency (such as bit-level parallelism), and concurrency can exist without parallelism (such as multitasking by time-sharing on a single-core CPU). Computer work is often broken down into many, often many, extremely similar sub-tasks that may be executed individually and whose results are then pooled after completion in parallel computing.', 'Multi-core and multi-processor computers have many processing parts within a single system, whereas clusters, MPPs, and grids operate on the same task using multiple computers. For speeding specific activities, specialized parallel computer architectures are sometimes employed alongside regular CPUs.', 'Numpy, pandas, sklearn, seaborn, and other Python libraries make data manipulation and machine learning jobs a breeze. The python [pandas] package is sufficient for most data analysis jobs. You can manipulate data in a variety of ways and use it to develop machine learning models.\xa0', 'However, as your data grows larger than the RAM available, pandas will become insufficient. This is a rather typical issue. To overcome this, you can utilize Spark or Hadoop. These aren’t, however, Python environments. This prevents you from utilizing NumPy, sklearn, pandas, TensorFlow, and other popular Python machine learning packages. Is there a way around this? Yes! This is where Dask enters the picture.', 'Dask is a Python-based open-source and extensible parallel computing library. It’s a platform for developing distributed apps. It does not immediately load the data; instead, it just points to the data, and only the relevant data is used or displayed to the user. Dask can use more than a single-core processor and employs parallel computation, making it incredibly quick and efficient with large datasets. It prevents mistakes caused by memory overflow.', 'Dask uses multi-core CPUs to efficiently perform parallel computations on a single system. If you have a quad-core CPU, for example, Dask can effectively process all four cores of your system at the same time. Dask keeps the entire data on the disk and processes chunks of data (smaller parts rather than the entire data) from the disk in order to consume less memory during computations. To save memory, the intermediate values generated are deleted as quickly as possible during the procedure.', 'In short, Dask can process data efficiently on a cluster of machines because it uses all of the cores of the connected workstations. The fact that all machines do not have to have the same number of cores is a fascinating aspect. If one system has two cores and the other has four, Dask can tolerate the disparity in core count.', 'Dask has two task scheduler families:', 'The following virtues are highlighted by Dask:', 'Before using the functionality of dask we need to install it. It can be simply installed using pip command as python –m pip install “dask[complete]”\xa0 that will install all functionality of dask not just core functionality.\xa0\xa0\xa0', 'Dask offers a variety of user interfaces, each with its own set of distributed computing parallel algorithms. Arrays built with parallel NumPy, Dataframes built with parallel pandas, and machine learning with parallel scikit-learn is used by data science practitioners looking to scale NumPy, pandas, and scikit-learn.', 'Dask DataFrames are made out of smaller pandas DataFrames. A huge pandas DataFrame divides into several smaller DataFrames row by row. These tiny DataFrames can be found on a single machine’s disk or numerous machines’ disks (thus allowing to store datasets of size larger than the memory). Each Dask DataFrame calculation parallelizes operations on existing pandas DataFrames.', 'The structure of a Dask DataFrame is depicted in the graphic below:', 'Now below we will compare the time taken by the pandas library and Dask to load a high sized CSV file will compare the result. CSV file contains the English to Hindi Truncated corpus which weighs around 35MB and has nearly 1,25,000 instances.\xa0', 'As we can see that the loading speed of dask is much faster than that of pandas.', 'Dask ML delivers scalable machine learning techniques in Python that are scikit-learn compatible. First, we’ll look at how scikit-learn handles computations, and then we’ll look at how Dask handles similar operations differently. Although scikit-learn can do parallel computations, it cannot be scaled to several machines. Dask, on the other hand, performs well on a single machine and can be scaled up to a cluster.', 'Using Joblib, sklearn supports parallel processing (on a single CPU). To parallelize several sklearn estimators, you can utilize Dask directly by adding a few lines of code (without modifying the existing code).', 'Dask ML implements simple machine learning techniques that make use of Numpy arrays. To provide scalable algorithms, Dask substitutes NumPy arrays with Dask arrays. This has been implemented for the following purposes:', 'And these can be implemented as below,']","""# loading file with pandas \nimport pandas as pd\n%time data_1 = pd.read_csv('/content/drive/MyDrive/data/Hindi_English_Truncated_Corpus.csv')""
 '# loading the file using dask\nimport dask.dataframe as dd\n%time data = dd.read_csv(""/content/drive/MyDrive/data/Hindi_English_Truncated_Corpus.csv"")\n', '! pip install dask-ml\n \n# ML model\nfrom dask_ml.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(data, labels)\n \n# Pre-processing\nfrom dask_ml.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(sparse=True)\nresult = encoder.fit(data)\n \n# Clustering\nfrom dask_ml.cluster import KMeans\nmodel = KMeans()\nmodel.fit(data)\n'"
142,science_and_machine_learning/,https://analyticsindiamag.com/why-you-should-use-dask-if-you-are-into-data-science-and-machine-learning/,"['What if there was a solution to speed up algorithms, parallelise computing, parallelise Pandas and \xa0NumPy and integrate with libraries like sklearn and XGBoost? Then it would be called Dask.', 'There are many solutions available in the market which are parallelisable, but they are not clearly transformable into a big DataFrame computation. Today these companies tend to solve their problems either by writing custom code with low-level systems like MPI, or complex queuing systems or by heavy lifting with MapReduce or Spark.', 'Dask exposes low-level APIs to its internal task scheduler to execute advanced computations. This enables the building of personalised parallel computing system which uses the same engine that powers Dask’s arrays, DataFrames, and machine learning algorithms. ', 'Dask emphasizes the following virtues:', 'Dask’s 3 parallel collections namely Dataframes, Bags and Arrays, enables it to store data that is larger than RAM. Each of these is able to use data partitioned between RAM and a hard disk as well distributed across multiple nodes in a cluster.', 'Dask can enable efficient parallel computations on single machines by leveraging their multi-core CPUs and streaming data efficiently from disk. It can run on a distributed cluster.', 'Dask also allows the user to replace clusters with a single-machine scheduler which would bring down the overhead. These schedulers require no setup and can run entirely within the same process as the user’s session.', '', 'Dask DataFrames coordinate many Pandas DataFrames/Series arranged along the index. A Dask DataFrame is partitioned row-wise, grouping rows by index value for efficiency. These Pandas objects may live on disk or on other machines.', 'Dask DataFrame has the following limitations:', '', 'Any Machine Learning project would suffer from either of the following two factors ', 'Dask can address the above problems in the following ways:', 'Dask also has methods from sklearn for hyperparameter search such as GridSearchCV, RandomizedSearchCV etc.', 'from dask_ml.datasets import make_regression', 'from dask_ml.model_selection import train_test_split, GridSearchCV', 'Here is an implementation of sklearn with Dask for prediction models:', 'from sklearn.linear_model import ElasticNet', 'from dask_ml.wrappers import ParallelPostFit', 'el = ParallelPostFit(estimator=ElasticNet())', 'el.fit(Xtrain, ytrain)', 'preds = el.predict(Xtest)', 'Implementing joblib to parallelise workload:', 'import dask_ml.joblib', 'from sklearn.externals import joblib', 'Dask lets analysts handle large datasets (100GB+) even on relatively low-power devices without the need for configuration or setup.', 'Pandas is still the go-to option as long as the dataset fits into the user’s RAM. For functions that don’t work with Dask DataFrame, dask.delayed offers more flexibility can be used.', 'Dask is very selective in the way it uses the disk. It evaluates computations in a low-memory footprint by pulling in chunks of data from disk, going ahead with the necessary processing shedding off the intermediate values.', 'Dask’s active participation at the community level has contributed a lot to the way it has evolved from within this ecosystem. This enables the rest of the ecosystem to benefit from parallel and distributed computing with minimal coordination.', 'As a result, Dask development is pushed forward by developer communities. This shall ensure that the Python ecosystem will continue to evolve with great consistency.', 'Installing Dask with pip:', 'pip install “dask[complete]”']",
143,tool_for_online_learning/,https://analyticsindiamag.com/a-guide-to-river-a-python-tool-for-online-learning/,"['What if you want to do machine learning with the data that is in motion? What if you wish to train machine learning models on real-time data? Online Machine Learning, also known as Incremental machine learning, is a method of executing machine learning with data that is in motion. In contrast to real-time online machine learning, in this post, we will look at how streaming data can be leveraged to do state-of-the-art machine learning tasks. We will also discuss a python library, River, with its implementation on online machine learning.\xa0 The following outlined points will be discussed.\xa0', 'Let’s start the discussion by knowing the difference between batch learning and online learning.\xa0\xa0', 'Batch learning approaches are incapable of gradual learning. They usually build models from the entire training set, which are then put into production. We must build a new model from scratch on the complete training set and the new data if we want batch learning algorithms to learn from new data as it arrives. Offline learning is another term for this. If the amount of data is large, training on the entire dataset could be expensive in terms of computer resources (CPU, RAM, storage, disk I/O, and so on).', 'If our system does not need to respond to quickly changing data, the batch learning method may suffice. If we don’t need to update our model too frequently, we can make use of the batch learning strategy. In other words, the entire process of training, evaluation, and testing are quite basic and uncomplicated, and it frequently yields better results than online techniques. For knowledge graph embedding projects, I’ve created batch learning algorithms. In the future, I hope to create an online approach for these projects to adapt to constantly changing knowledge graphs.', 'In online learning, the training is done in small groups or in an incremental fashion by continuously feeding data as it arrives. Each learning phase is quick and inexpensive, allowing the system to learn about new data as it comes in.', 'Machine learning systems that receive data in a continuous stream (e.g., stock prices) and must adapt to change quickly or autonomously benefit from online learning. It’s also a smart alternative if you only have a limited number of computational resources: after an online learning system has learnt about new data instances, it no longer needs them, so you may trash them (unless you wish to be able to “replay” the data). This can help you save a lot of space. Online learning is depicted in the figure above.', 'Online learning methods can also be used to train systems on massive datasets that are too large to fit in a single machine’s main memory (this is also called out-of-core learning). The algorithm loads a portion of the data, performs a training step on that data, and then continues the process until it has run on all of the data.', 'The learning rate is an important feature of online learning. The learning rate is the rate at which you want your machine learning to adapt to new data sets. A system with a rapid learning rate will swiftly forget what it has learned. A system with a low learning rate is more akin to batch learning.', 'One significant downside of an online learning system is that if it is fed incorrect data, the system will perform poorly, and the user will see the impact immediately. As a result, it is critical to implement proper filters to ensure that the data fed is of good quality. Furthermore, it is critical to closely monitor the functioning of the machine learning system.', 'To execute online machine learning, many frameworks are available. Several of them are,', 'Scikit-multi-flow (also known as skmultiflow) is a Python-based machine learning library that supports multi-output/multi-label and stream data. Scikit-multiflow makes it simple to create and perform experiments, as well as to enhance stream learning algorithms. It has a number of methods for classification, regression, concept drift detection, and anomaly detection. A suite of data stream generators and evaluators is also included. scikit-multiflow is compatible with Jupyter Notebooks and is meant to work with Python’s numerical and scientific libraries NumPy and SciPy.', 'Nippon Telegraph & Telephone and Preferred Infrastructure created Jubatus, an open-source online machine learning, and distributed computing system. It has classification, recommendation, regression, anomaly detection, and graph mining capabilities. Many client languages are supported, including C++, Java, Ruby, and Python. Iterative Parameter Mixture is used for distributed machine learning.', 'With Creme, we may use a different approach, which is to learn a stream of data continually. As a result, the model only processes one observation at a time and may be modified on the fly. This enables learning from large datasets that are too large to fit in main memory. In circumstances when new data is continually arriving, online machine learning works well. It excels in a variety of applications, including time series forecasting, spam filtering, recommender systems, CTR prediction, and IoT.', 'This article focuses on the River library, which combines the scikit-multiflow and creme libraries to provide functionality for executing online machine learning on streaming data. So, let’s take a closer look at River and its implementation.', 'The river is a machine learning library for continuous learning and dynamic data streams. For various stream learning challenges, it includes many state-of-the-art learning methods, data generators/transformers, performance indicators, and evaluators. It’s the outcome of combining two of Python’s most popular stream learning packages: Creme and scikit-multiflow.\xa0', 'In river, machine learning models are extended classes of specialized mixins that vary based on the learning job, such as classification, regression, clustering, and so on. This maintains library compatibility and makes it easier to extend/modify current models as well as create new models that are compatible with the river.', 'Learn and predict are the two main functions of all predictive models. The learn one method is used for learning (updates the internal state of the model). The predict one (classification, regression, and clustering), predict proba one (classification), and score one (anomaly detection) algorithms provide predictions depending on the learning goal. It’s worth noting that the river includes transformers, which are stateful objects that use the transform one method to convert an input.', 'Lets’ implement the river.\xa0', 'The river offers a Scikit-learn-like API and is also known as Scikit-learn for streaming or online machine learning. It is designed for streaming data and supports practically all ML estimators and transformers.', 'Let’s look at how to use the river to create a simple text classifier model that can categorize the sentiment of text as Positive (1) or Negative(-1). The dataset used in this post is taken from this Kaggle repository which contains strings of texts associated with respective sentiments. To convert our text into features, we’ll use BagOfWords() as our transformer or vectorizer, and Naive Bayes MultinomialNB as our Machine Learning Estimator.', 'While installing the river, make sure you are using the latest version of the NumPy package.\xa0\xa0', 'In terms of data, we’ll just utilize a list of tuples containing the text and the label in our scenario (Positive or Negative). However, data from a streaming engine or a CSV file can be ingested as well. If you’re dealing with the famed Pandas package, you’ll have to convert a CSV file to a dictionary or list of tuples.', 'Next, we’ll construct a pipeline that includes two stages: a transformer/vectorizer for converting text to features and an estimator.', 'Because the data is coming one at a time, we’ll have to fit our model to it one at a time during training using our pipeline’s.learn one(x,y) method. We can emulate this by using a for loop to iterate through our data. [Note that t is.fit one(x,y) in crème]', 'Now let’s check the prediction using predict_one and the probability of two classes.\xa0', 'We can employ functions from the river to determine the reliability and performance of our model. Accuracy metrics and classification reports can be used from sub-module metrics.', 'Accuracy: 90.07%']","'predict one'
 'predict proba one ', 'score one', 'BagOfWords() ', 'MultinomialNB'"
144,automating_machine_learning_tasks/,https://analyticsindiamag.com/a-guide-to-using-autogluon-for-automating-machine-learning-tasks/,"['AutoGluon, an open-source tool from AWS which is easily available to everyone, facilitates a variety of AutoML (Automated Machine Learning) tasks. It helps in automating different machine learning and deep learning tasks and figuring out the best suitable model for a particular task. In this post, we will discuss the AutoGluon and we will see its different features to support automating machine learning tasks. We will go through the implementation of tabular prediction using AutoGluon to understand how a particular machine learning task can be automated using it. In the end, we will also try to understand how one can find out the best suitable model for a particular machine learning task when using AutoGluon. The major points to be discussed in this article are listed below.\xa0\xa0', 'Let’s start the discussion by knowing what AutoML actually is.', 'Automated machine learning refers to the process of automating the tasks of applying machine learning to real-world problems (AutoML). AutoML covers the whole pipeline, from the raw dataset to the deployable machine learning model. AutoML was proposed as an AI-based solution to the ever-growing problem of machine learning applications. Because of AutoML’s high level of automation, non-experts can use machine learning models and procedures without becoming machine learning professionals.', 'Automating the entire machine learning process has the added benefit of providing simpler solutions, faster generation of those solutions, and models that frequently outperform hand-designed models. In a prediction model, AutoML was utilized to compare the relative relevance of each factor.', 'Automated Machine Learning research has produced a wide range of packages and approaches aimed at both researchers and end-users. Several off-the-shelf software that allows automated machine learning has been created in recent years. The packages that have been developed so far are listed below.', 'AutoGluon is an open-source AutoML tool that uses just one line of Python code to train extremely accurate machine learning models on unprocessed tabular datasets like CSV files. AutoGluon succeeds by assembling several models and stacking them in various layers, unlike other AutoML frameworks that largely focus on model/hyperparameter selection. Experiments show that our multi-layer combination of several models makes better use of training time than searching for the best.', 'The following are the design principles of the AutoGluon:', 'AutoGluon allows simple-to-use and extensible AutoML, with a focus on automated stack ensembling, deep learning, and real-world applications encompassing text, image, and tabular data. AutoGluon, designed for both novices and specialists in machine learning, and provides features like:', 'With AutoGluon, the Machine Learning developers can accomplish the following tasks:-', 'AutoGluon can generate models to predict the values in one column based on the values on the other columns using the common and standard datasets that are represented as tables (usually stored as CSV files). We can obtain excellent accuracy in standard supervised learning tasks such as classification and regression with just a single .fit() function. Additionally, there are tons of parameters that we can tune to even optimize the performance. Without having to deal with time-consuming procedures like data cleaning, feature engineering, rigorous hyperparameter tuning, algorithm selection and so on we can conclude our journey in a very effective way.\xa0', 'AutoGluon again provides a simple fit() function for classifying photos based on their content which generates high-quality image classification models automatically. A single call fit() will return an extremely accurate neural network on the image dataset we give, automatically employing accuracy-enhancing techniques like transfer learning and hyperparameter optimization on our behalf. Also here we can also prepare a dataset using the CSV files or we can organize the data into proper directories using its various functional APIs.\xa0\xa0\xa0', 'AutoGluon provides a simple fit() function for identifying the presence and placement of objects in photos, which creates high-quality object detection models automatically. A single call to fit() will train extremely accurate neural networks on the picture dataset you provide, automatically employing accuracy-boosting techniques like transfer learning and hyperparameter tuning.', 'To generate high-quality text prediction models automatically ( usually the transformer neural network)\xa0 fit() also can be used for this supervised kind of task. Each training sample could be the sentence, a brief paragraph, a combination of numerous text fields (e,g. Predicting how similar the two-sentence are), or it could even include other numeric/ categorical variables in addition to the text. The predicted values can be continuous values (regression) or discrete categories (classification).\xa0', 'A quick call to prediction is all it takes. The fit() method will automatically use accuracy boosting approaches including fine-tuning a pre-trained NLP model and hyperparameter optimization to train a highly accurate neural network on the input text dataset.\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0', 'Text data may be blended with numerical/categorical data in various applications. TextPredictor from AutoGluon can train a single neural network that works on many feature types at the same time, such as text, categorical, and numerical columns. The fundamental idea is to segregate the text, category, and numeric fields and combine them across modalities. To address such multimodal tasks AutoGluon can be used. We can train a multi-model ensemble using data such as images and associated features with it in tabular form.\xa0', 'Let us process further with understanding how AutoGluon can be used with tabular data for tabular predictions. In the further section, we will see how we can effectively perform the classification task using a top-notch performance of AutoGluon Tabular.', 'AutoGluon-Tabular is a simple and accurate method for working with tabular data. AutoGluon-Tabular is capable of complex data processing, deep learning, and multi-layer model assembly. It recognizes the data type in each column automatically for comprehensive data preprocessing, including particular handling of text fields. AutoGluon supports a wide range of models, from off-the-shelf boosted trees to bespoke neural network models.', 'These models are ensembled in an innovative way: models are stacked in many layers and trained layer by layer, ensuring raw data can be transformed into high-quality predictions within a specified time restriction. Over-fitting is reduced throughout the process by splitting the data in different ways and keeping careful track of out-of-fold cases.', '\xa0Below you can see the AutoGluon’s neural network architecture for tabular data is made up of numerical and categorical features. Layers with learnable parameters are denoted by the colour blue.', 'Source', 'Consider a structured dataset of raw values saved in a CSV file, such as Stars.csv, with the predicted label values stored in a column, labelled ‘Type.’ AutoGluon automatically preprocesses the raw data, determines the type of prediction problem (binary, multi-class classification, or regression), partitions the data into various folds for model-training vs. validation, fits various models individually, and finally creates an optimized model ensemble that outperforms any of the individual trained models.\xa0', 'Fit() includes extra hyperparameters that can be set for users that are prepared to suffer longer training times in order to maximize predicted accuracy. All intermediate outcomes are saved to disk. If a call was cancelled, we can resume training by using fit() with the option to continue training=True.', 'Let’s implement it. The dataset used for the experiment is taken from this Kaggle repository which is about Predicting the type of Stars based on 6 attributes.\xa0', 'To date, officially AutoGluon is not supported in the windows system. It is available for Linux and macOS as well. To get started, we need to install it as pip install mxnet autogluon.\xa0', 'Observe the data carefully we are going to feed the data as it is without any pre-processing steps like encoding the categorical variable as there are two. This is one of the beauties of the AutoGluon.\xa0', 'Now let us split the data into train and test after that we are ready to train all available models inside the AutoGluon.\xa0', 'Now we can check the probability assigned for each class by the Top classifier and also we look at the leaderboard of the models.\xa0']","'Fit()'
 'fit()', 'continue training=True', 'pip install mxnet autogluon'"
145,for_ml_model_building/,https://analyticsindiamag.com/hands-on-guide-to-padelpy-for-ml-model-building/,"['A machine learning model can be defined as an output of a rigorous training process and presented as the mathematical representation of a real-world process. A machine learning model is one that is trained to recognize and detect certain patterns present in a dataset. The created model is trained over a set of data, providing it with an algorithm that it can use to reason over and learn from the provided data. Once the model is properly trained, it can be used to reason over data that the model has not seen before and make predictions about that data. Suppose you want to build an application that can recognize a user’s emotions based on their facial expressions. In that case, a model can be easily trained by providing it with images of faces that are each tagged with a certain kind of emotion. Then that trained model can be included in an application that can recognize any user’s emotion. Every machine learning model is categorized in two ways, either supervised or unsupervised. A supervised model can then be further sub-categorized as either a regression or classification model.\xa0', 'The machine learning algorithms help in finding the pattern in a training dataset, which is then used to approximate the target function. In addition, algorithms are responsible for mapping the inputs to the outputs from the available dataset or the dataset being processed. In terms of machine learning, an algorithm can be defined as a run on data to create a machine learning model. Early stages of machine learning, also known as ML for short, saw experiments that only involved theories of computers recognizing patterns from data and learning from them. Today, after building and advancements from those foundational experiments, machine learning is becoming more complex. While machine learning algorithms have been around for a long time, the ability and possibilities to apply them to complex big data applications have been changing more rapidly and effectively with recent developments. Putting them in an application perspective while maintaining a degree of sophistication can set an organization way ahead of its competitors.\xa0', 'Machine learning algorithms also possess the ability to perform pattern recognition where the algorithms learn from data or are fit on a dataset. Many machine learning algorithms have been developed like algorithms for classification, such as k-nearest neighbors or algorithms for regression, such as linear regression or clustering just like k-means. Machine learning algorithms can be described and defined in a model using math and pseudocode. The efficiency and accuracy of both the algorithm and the model can also be analyzed and calculated. Machine learning algorithms can be implemented with any one of a range of modern programming languages such as Python or R. These languages provide a wide range of libraries that can be used to create complex and layered algorithms that the practitioners can use on their projects being developed.\xa0', 'PaDELPy is an open-source library that provides a Python wrapper for the PaDEL-Descriptor and a molecular descriptor calculation software. The descriptor can be defined as a mathematical logic that describes the properties of a molecule based on the correlation between the structure of the compound and its biological activity. The PaDEL-Descriptor can be used to work on scientific data to help calculate the molecular fingerprint of specific molecules used to build scientific machine learning models. The PaDEL-Descriptor is a java software that requires running a java file first to help execute and create scientific models. But using the PaDELPy library makes it easier to calculate molecular fingerprints using Python. There is no need to run the jar file, hence reducing the lengthy installation process and aiding quick implementation.\xa0', 'In this article, we will try to create and implement a scientific machine learning model using the PaDELPy library to calculate the molecular fingerprint and using\xa0 Random Forest we will be predicting the molecular activity of the drug from the HCV Drug dataset. The following code is inspired by the creators of the PaDELPy library, whose Github repository can be accessed from the link here. If you want to download the raw dataset used in this implementation, you can use the following link.', 'Let’s Get Started!', 'Our first step here will be to install the PaDELPy library. To do so, you can run the following line of code,', 'Now we will load and properly set up our calculator model first using the necessary PaDELPy files available only in the form of an XML format.\xa0', 'Now we will create a data dictionary with all the loaded and available data files so that we get a key-value pair,', 'With all the necessary PaDELPy files being set up for calculation, we will next load our dataset to be calculated.\xa0', 'To calculate the molecular descriptor using PaDEL, we will necessarily prepare the data by concatenating the two concerned columns from the dataset, which will act as an input to our model.', 'There are 12 fingerprint types present in PaDEL to be calculated from. To calculate all 12, make we will make adjustments to the descriptor types input argument to any of the ones in the fp dictionary variable,', 'We want to calculate the molecular fingerprint. Hence we will load the necessary file now; we are using the PubChem', 'Setting up the module to calculate the molecular fingerprint,', 'Display the calculated fingerprints,', 'Now we will try to create a classification model from the processed data and Random Forest,', 'As we can notice, the data looks pretty much sorted and tells us the most effective drugs. So with this, let’s make predictions from this processed data using our model.', 'Making predictions on the drug molecular activity from the created model,', 'Calculating performance metrics of train split\xa0 using matthews correlation coefficient,', 'Calculating performance metrics of test split using matthews correlation coefficient,', 'As observed from the performance metrics of Random Forest Model created for predicting the molecular drug activity, It seems to perform well on the preset dataset. You can use other algorithms as well to test the performance!']","'#installing the library\n!pip install padelpy'
 '#Downloading the XML data files\n!wget https://github.com/dataprofessor/padel/raw/main/fingerprints_xml.zip\n!unzip fingerprints_xml.zip\n#listing and sorting the downloaded files\nimport glob\nxml_files = glob.glob(""*.xml"")\nxml_files.sort()\nxml_files\n', ""['AtomPairs2DFingerprintCount.xml',\n 'AtomPairs2DFingerprinter.xml',\n 'EStateFingerprinter.xml',\n 'ExtendedFingerprinter.xml',\n 'Fingerprinter.xml',\n 'GraphOnlyFingerprinter.xml',\n 'KlekotaRothFingerprintCount.xml',\n 'KlekotaRothFingerprinter.xml',\n 'MACCSFingerprinter.xml',\n 'PubchemFingerprinter.xml',\n 'SubstructureFingerprintCount.xml',\n 'SubstructureFingerprinter.xml']\n"", ""#Creating a list of present files\nFP_list = ['AtomPairs2DCount',\n 'AtomPairs2D',\n 'EState',\n 'CDKextended',\n 'CDK',\n 'CDKgraphonly',\n 'KlekotaRothCount',\n 'KlekotaRoth',\n 'MACCS',\n 'PubChem',\n 'SubstructureCount',\n 'Substructure']"", '#Creating Data Dictionary\nfp = dict(zip(FP_list, xml_files))\nfp\n', ""{'AtomPairs2D': 'AtomPairs2DFingerprinter.xml',\n 'AtomPairs2DCount': 'AtomPairs2DFingerprintCount.xml',\n 'CDK': 'Fingerprinter.xml',\n 'CDKextended': 'ExtendedFingerprinter.xml',\n 'CDKgraphonly': 'GraphOnlyFingerprinter.xml',\n 'EState': 'EStateFingerprinter.xml',\n 'KlekotaRoth': 'KlekotaRothFingerprinter.xml',\n 'KlekotaRothCount': 'KlekotaRothFingerprintCount.xml',\n 'MACCS': 'MACCSFingerprinter.xml',\n 'PubChem': 'PubchemFingerprinter.xml',\n 'Substructure': 'SubstructureFingerprinter.xml',\n 'SubstructureCount': 'SubstructureFingerprintCount.xml'}\n"", ""#Loading the dataset\nimport pandas as pd\ndf = pd.read_csv('https://raw.githubusercontent.com/dataprofessor/data/master/HCV_NS5B_Curated.csv')\n \n#Loading data head\ndf.head()\n"", '#Loading data tail\ndf.tail(2)', ""#Concatenating necessary columns\ndf2 = pd.concat( [df['CANONICAL_SMILES'],df['CMPD_CHEMBLID']], axis=1 )\ndf2.to_csv('molecule.smi', sep='\\t', index=False, header=False)\ndf2\n"", ""#listing the dictionary pairs\nfp\n\n\n\n\n{'AtomPairs2D': 'AtomPairs2DFingerprinter.xml',\n 'AtomPairs2DCount': 'AtomPairs2DFingerprintCount.xml',\n 'CDK': 'Fingerprinter.xml',\n 'CDKextended': 'ExtendedFingerprinter.xml',\n 'CDKgraphonly': 'GraphOnlyFingerprinter.xml',\n 'EState': 'EStateFingerprinter.xml',\n 'KlekotaRoth': 'KlekotaRothFingerprinter.xml',\n 'KlekotaRothCount': 'KlekotaRothFingerprintCount.xml',\n 'MACCS': 'MACCSFingerprinter.xml',\n 'PubChem': 'PubchemFingerprinter.xml',\n 'Substructure': 'SubstructureFingerprinter.xml',\n 'SubstructureCount': 'SubstructureFingerprintCount.xml'}\n"", ""#Importing PubChem\nfp['PubChem']"", ""#Setting the fingerprint module\n \nfrom padelpy import padeldescriptor\n \nfingerprint = 'Substructure'\n \nfingerprint_output_file = ''.join([fingerprint,'.csv']) #Substructure.csv\nfingerprint_descriptortypes = fp[fingerprint]\n \npadeldescriptor(mol_dir='molecule.smi', \n                d_file=fingerprint_output_file, #'Substructure.csv'\n                #descriptortypes='SubstructureFingerprint.xml', \n                descriptortypes= fingerprint_descriptortypes,\n                detectaromaticity=True,\n                standardizenitro=True,\n                standardizetautomers=True,\n                threads=2,\n                removesalt=True,\n                log=True,\n                fingerprints=True)\n"", 'descriptors = pd.read_csv(fingerprint_output_file)\ndescriptors\n', ""X = descriptors.drop('Name', axis=1)\ny = df['Activity'] #feature being predicted\n\n#removing the low variance features\nfrom sklearn.feature_selection import VarianceThreshold\n \ndef remove_low_variance(input_data, threshold=0.1):\n    selection = VarianceThreshold(threshold)\n    selection.fit(input_data)\n    return input_data[input_data.columns[selection.get_support(indices=True)]]\n \nX = remove_low_variance(X, threshold=0.1)\nX\n"", '#Splitting into Train And Test\nfrom sklearn.model_selection import train_test_split\n \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n#Printing Shape\nX_train.shape, X_test.shape\n\n\n((462, 18), (116, 18))\n\n\n#Implementing Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import matthews_corrcoef\n \nmodel = RandomForestClassifier(n_estimators=500, random_state=42)\nmodel.fit(X_train, y_train)\n', ""RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=500,\n                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n                       warm_start=False)\n"", 'y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n', 'mcc_train = matthews_corrcoef(y_train, y_train_pred)\nmcc_train\n\n0.833162800019916\n', ""mcc_test = matthews_corrcoef(y_test, y_test_pred)\nmcc_test\n\n0.5580628933757674\n\n#performing cross validation\nfrom sklearn.model_selection import cross_val_score\n \nrf = RandomForestClassifier(n_estimators=500, random_state=42)\ncv_scores = cross_val_score(rf, X_train, y_train, cv=5)\ncv_scores\n\narray([0.83870968, 0.80645161, 0.86956522, 0.86956522, 0.81521739])\n\n#calcutating mean from the five fold \nmcc_cv = cv_scores.mean()\nmcc_cv\n\n0.8399018232819074\n\n#implementing metric test in a single dataframe\nmodel_name = pd.Series(['Random forest'], name='Name')\nmcc_train_series = pd.Series(mcc_train, name='MCC_train')\nmcc_cv_series = pd.Series(mcc_cv, name='MCC_cv')\nmcc_test_series = pd.Series(mcc_test, name='MCC_test')\n \nperformance_metrics = pd.concat([model_name, mcc_train_series, mcc_cv_series, mcc_test_series], axis=1)\nperformance_metrics"""
146,multimodal_and_transfer_learning/,https://analyticsindiamag.com/hands-on-guide-to-pykale-a-python-tool-for-multimodal-and-transfer-learning/,"['This article introduces pykale, a python library based on PyTorch that leverages knowledge from multiple sources for interpretable and accurate predictions in machine learning. This library consists of three objectives of green machine learning:', 'To complete these objectives, PyKale gives some basic features like:', '\xa0The library has a pipeline-based API that unifies the workflow in several steps that helps to increase the flexibility of the models. These APIs are designed to accomplish the following steps of any machine learning workflow:', 'The pykale supports graph, images, text and videos data that can be loaded by PyTorch Dataloaders and supports CNN, GCN, transformers modules for machine learning. It also supports the domain adaptation sector of transfer learning. More formally, we can say that it is a library specially focused on multimodal learning and transfer learning.\xa0', 'Next in the tutorial, we will see a demonstration of PyKale on domain adaptation on Digits with Lightning using google colab, an online hosting platform. In domain adaptation, we will take a model trained on one dataset and then we use a digits dataset as the target set and adapt the pre-trained model for the target dataset.', 'Installing the packages.', 'Input:', 'Cloning the pykale repository.', 'Input:', 'This Domain Adaptation is constructed based on the digits_dann_lightn example main.py', 'Directing to the digits_dann_lightn.', 'Input:', 'Importing the required modules:', 'Input:', 'PyKale provides a default configuration file for domain adaptation. We can use it for other similar problems. There is a .yaml for the tutorial which can be tailored with configuration files.', 'Input:', 'We can see in the dataset we have 1024 samples which are later splitted for training test and validation.\xa0', 'Selecting dataset using DigitDataset.get_source_target for source data and target data and providing a subset of classes 1, 3 and 8.', 'Input:', 'Setting seeds for the generation of pseudo-random numbers.', 'Input:', 'Setting up the model based on defined configuration and dataset earlier in the article.', 'Input:', 'Defining the object to process the training further. Under the object store, the parameters define how the model will get trained.', 'Input:', 'In this step, we can see the reports of the GPU and TPU availability. If you are running this on your local machine, you can set GPU and TPU settings to speed up the training. We can also save the logs of your outputs during or after training using setup_logger.', 'Input:', 'Optimizing model parameters using the adap_trainer.', 'Input:', 'Testing the model with the test data which is not used in the training of the model.', 'Input:', 'Printing the results of the model performance.', 'Input:']","'!pip install git+https://github.com/pykale/pykale.git#egg=pykale[extras]'
 '!git clone https://github.com/pykale/pykale.git', '%cd pykale/examples/digits_dann_lightn', 'import logging\nimport os\nimport numpy as np\nimport pytorch_lightning as pl\nfrom torch.utils.data import DataLoader\nrom torch.utils.data import SequentialSampler\nimport torchvision\nfrom model import get_model\nfrom kale.utils.csv_logger import setup_logger', 'from config import get_cfg_defaults\ngpus = None\xa0\nPyKale_cfg = get_cfg_defaults()\npath = ""./configs/tutorial.yaml""\xa0\nPyKale_cfg.merge_from_file(path)\nPyKale_cfg.freeze()\nprint(PyKale_cfg)', 'from kale.loaddata.digits_access import DigitDataset\nfrom kale.loaddata.multi_domain import MultiDomainDatasets\nsource, target, num_channels = DigitDataset.get_source_target(\n\xa0\xa0DigitDataset(PyKale_cfg.DATASET.SOURCE.upper()), \nDigitDataset(PyKale_cfg.DATASET.TARGET.upper()), PyKale_cfg.DATASET.ROOT\n)\ndataset = MultiDomainDatasets(\n\xa0\xa0\xa0\xa0source,\n\xa0\xa0target,\n\xa0\xa0\xa0\xa0config_weight_type=PyKale_cfg.DATASET.WEIGHT_TYPE,\n\xa0\xa0\xa0\xa0config_weight_type=PyKale_cfg.DATASET.WEIGHT_TYPE,\n\xa0\xa0\xa0\xa0val_split_ratio=PyKale_cfg.DATASET.VAL_SPLIT_RATIO,\n\xa0\xa0\xa0\xa0class_ids=[1, 3, 8],\n)', 'from kale.utils.seed import set_seed\nseed = PyKale_cfg.SOLVER.SEED\nset_seed(seed)', 'from model import get_model\n%time model, train_params = get_model(PyKale_cfg, dataset, num_channels)', 'import pytorch_lightning as pl\n\nadap_trainer = pl.Trainer(\n\xa0\xa0\xa0\xa0progress_bar_refresh_rate=PyKale_cfg.OUTPUT.PB_FRESH,\xa0 # in steps\n\xa0\xa0\xa0\xa0min_epochs=PyKale_cfg.SOLVER.MIN_EPOCHS,\n\xa0\xa0\xa0\xa0max_epochs=PyKale_cfg.SOLVER.MAX_EPOCHS,\n\xa0\xa0\xa0\xa0callbacks=[checkpoint_callback],\n\xa0\xa0\xa0\xa0logger=False,\n\xa0\xa0\xa0\xa0gpus=gpus)', 'from kale.utils.csv_logger import setup_logger\nlogger, results, checkpoint_callback, test_csv_file = setup_logger(\n\xa0\xa0\xa0\xa0train_params, PyKale_cfg.OUTPUT.DIR, PyKale_cfg.DAN.METHOD, seed\n)', '%time adap_trainer.fit(model)\nresults.update(\n\xa0\xa0\xa0\xa0is_validation=True, method_name=PyKale_cfg.DAN.METHOD, seed=seed, metric_values=trainer.callback_metrics,\n)', '%time apap_trainer.test()\nresults.update(\n\xa0\xa0\xa0\xa0is_validation=False, method_name=PyKale_cfg.DAN.METHOD, seed=seed, metric_values=trainer.callback_metrics,\n)', 'results.print_scores(pykale_cfg.DAN.METHOD)'"
147,qiskit_for_quantum_computing/,https://analyticsindiamag.com/beginners-guide-to-qiskit-for-quantum-computing/,"['Qiskit is an open-source quantum software development kit developed by IBM that provides help writing quantum computing programs and embeds them into different backends (statevector backend, unitary backend, openQASM backend) basic building blocks in a python programming language. Thus, Qiskit can build programs with basic quantum modules delivered by the package. One of the basics of Qiskit is quantum circuits. This article will discuss an overview of quantum computing, terminology, and working with Qiskit and visualizing the results.', 'Quantum computing is the field of computer science that mainly focuses on modern physics principles of quantum theory. Principles of quantum theories illustrate the behaviour of matters and energy at atomic and subatomic levels. Similarly, quantum computing explains quantum phenomena like quantum bits, superposition, etc. The quantum computing field comes in a field where the tasks are so difficult that normal computing phenomena cannot solve them. We use quantum bits or Qubits, which can be considered the memory unit. Whereas in conventional classical computing, bits represent the information by the computer. Qubits are one of the basic terminologies of quantum computing. However, there are many terminologies which we don’t know at a very basic level. We will learn the terminologies in the next steps, and it can be useful for us using python and the Qiskit package.', 'Very basic thing about Qiskit is that it works in two stages. One of the stages is the build stage, where we make different quantum circuits and, using those circuits, we reach the solution. Then, after completing the building part or reaching the solution, we go into the next stage, which is called execute stage, where we try to run our build or solution in the different backends (statevector backend, unitary backend, openQASM backend) after completing the run we process the data in the build for desired output.', 'Quantum circuit', 'Quantum circuits can be considered the flow of sequential computation on quantum data, or we can call a quantum circuit the sequence of quantum gates. The basis of quanSo the computing is quantum circuits. Let’s see how we can generate the circuit using Qiskit in google colab.', 'Installing the Qiskit package.', 'Input:', 'Importing the libraries.', 'Input:', 'Building the circuit', 'As we discussed first fundamental of Qiskit is the quantum circuit; we can make a circuit using QuantumCircuit()', 'Input:', 'Here we have created a circuit with the quantum register of 4 qubits.', 'After creating the circuit, we can add operations to manipulate the qubits. In the next step, we would be adding four operations on it.', 'Input:', 'Here in the above input on qubit 0, we have added h gate for superposition. And in entanglement, we have added operation for three qubits. Cx gate will control the qubit 0 and qubit one and will put the qubit in the bell state. Similarly, the cx gate in second place will control the qubit 0 and 2, and the final gate will control the qubit 0 and qubit three and put the qubit in ghz state.', 'We can also visualize the circuit.', 'Input:', 'In this circuit visualization we can see that the start qubit is on the upside, and the following qubits are on the downside side of the H gate. Therefore, the flow of the circuit will be left to the right.', 'As discussed earlier in the article, we have built the circuit and can now execute the circuit using qiskit modules. Qiskit’s Aer package provides the facility for simulation of circuit using different backends\xa0', 'Qiskit provides different backends for the simulation part in quantum computing.', 'The first backend we will implement for simulation is statevector_simulater, which returns a quantum state as a complex vector of 2n . \xa0\xa0\xa0dimension, where n is the number of qubits.', 'Importing the library.', 'Input:', 'Defining the job and executing it with statevector simulator\xa0', 'Input:', 'After compilation of the backend simulator, we can check for the status and results.', 'Input:', 'Input:', 'We can also look for the state vector of the circuit.', 'Input:', 'We can also visualize the state density of the circuits component.\xa0', 'Input:', 'Qiskit also provides the unitary backend for simulation. This simulation results the 2n ⨯\xa0 2n dimensions vector.', 'Input:', 'Let’s check for the final solution.', 'Input:', 'Input:', 'We can also make OpenQASM backend with qiksit. Till now, we were simulating on the ideal circuit. In real life experiments, circuits are terminated by measuring each qubit. And without measurement, we can not gain information about the state.\xa0\xa0\xa0\xa0\xa0', '\xa0Including measurement, simulation requires adding measurement on the circuit to use OpenQASM.', 'Creating the circuit:', 'Input:\xa0', 'Visualizing the circuit.', 'Input:', 'This circuit consists of 4 qubits and one classical register, and four measurements to map the outcome of the qubits.', 'Let’s simulate this circuit using OpenQASM backend.', 'Defining the object', 'Input:\xa0', 'Executing the object on the circuit.', 'Input:', 'Let’s check the results of the job.', 'Input:', 'Input:', 'Here in the output, we have asked for counts of zeros and ones, and we can see that we had some good accuracy. Where 50 per cent of the time. The output is zero. We can also visualize this count in a histogram using qiskit.', 'Input:', 'Here is the article we have seen about quantum computing and how we can perform at a basic level using the Qiskit package. There are various applications of quantum computing like Cybersecurity, artificial intelligence, financial modelling, computational biology etc. since quantum computing provides results that normal or classical computers cannot gain, it can make machine learning models work faster than the classical way.']","'!pip install qiskit'
 'import numpy as np\nfrom qiskit import *', 'ircuits = QuantumCircuit(4)', 'circuits.h(0)\ncircuits.cx(0, 1)\ncircuits.cx(0, 2)\ncircuits.cx(0, 3)', 'circuits.draw()', 'from qiskit import Aer', ""job = execute(circuits, Aer.get_backend('statevector_simulator'))"", 'job.status()', 'result = job.result()\nprint(result)', 'outputstate = result.get_statevector(circuits, decimals=3)\nprint(outputstate)', 'from qiskit.visualization import plot_state_city\nplot_state_city(outputstate)', ""backend = Aer.get_backend('unitary_simulator')\njob = execute(circuits, backend)\nresult = job.result()"", 'print(result.get_unitary(circuits, decimals=3))', 'outputstate = result.get_unitary(circuits, decimals=3)\nplot_state_city(outputstate)', 'meas = QuantumCircuit(4, 4)\nmeas.barrier(range(4))\nmeas.measure(range(4), range(4))\nqc = circuits + meas', 'qc.draw()', ""backend_sim = Aer.get_backend('qasm_simulator')"", 'job_sim = execute(qc, backend_sim, shots=1024)', 'result_sim.status', 'counts = result_sim.get_counts(qc)\nprint(counts)', 'from qiskit.visualization import plot_histogram\nplot_histogram(counts)'"
148,learning_with_apache_spark/,https://analyticsindiamag.com/beginners-guide-to-machine-learning-with-apache-spark/,"['Spark is known as a fast, easy to use and general engine for big data processing. A distributed computing engine is used to process and analyse large amounts of data, just like Hadoop MapReduce. It is quite faster than the other processing engines when it comes to data handling from various platforms. In the industry, there is a big demand for engines that can process tasks like the above. Today or later, your company or client will be asked to develop sophisticated models that would enable you to discover a new opportunity or risk associated with it, and this all can be done with Pyspark. It is not hard to learn Python and SQL; it is easy to start with it.', 'Pyspark is a data analysis tool created by the Apache Spark community for using Python and Spark. It allows you to work with Resilient Distributed Dataset(RDD) and DataFrames in python. Pyspark has numerous features that make it easy, and an amazing framework for machine learning MLlib is there. When it comes to huge amounts of data, pyspark provides you with fast and real-time processing, flexibility, in-memory computation and various other features. In simple words, it is a Python-based library that gives a channel to use spark, which combines the simplicity of Python and the efficiency of spark.', 'Let’s take the brief information about the architecture of PySpark from the official\xa0 documentation;\xa0\xa0\xa0', 'As it not allow you to write applications using python API’s but also provides a PySpark shell for interactively analyzing your data in a distributed environment. PySpark supports most Spark features such as Spark SQL, Data Frame, Streaming, MLlib for machine learning, and spark core.\xa0\xa0', 'Let’s take a look at that one by one.', 'Spark SQL and DataFrame:', 'It is a\xa0 module for structured data processing. It gives an abstraction called DataFrame, and it can also be used as a SQL query engine.\xa0', 'MLlib:', 'MLlib is a high-level machine learning library that provides a set of API’s that helps users to create and tune practical machine learning models; it has supported nearly all the algorithms, including classification, regression, collaborative filtering and so on.', 'Streaming:', 'With the help of the streaming feature, we can process the real-time data from various sources and this processed data can be pushed into system files, databases or even to the live dashboard.', 'Spark Core:', 'Spark Core is the base of the whole project. It works on specialized data structured called Resilient Distributed Dataset RDD in short and in-memory computing capabilities.', 'Today we will be focusing only on the MLlib and common data handling techniques using spark, and lastly, we will build a Logistic Regression model using spark, and also I will demonstrate how to do hypothesis testing.', 'The following code implementation is in reference to the official implementation.', 'The dataset is taken from the kaggle repository, and it is related to Advertisement, i.e. we need to find which kind of user is more likely to click on the ad.', 'The input features are:- Daily Time Spent on Site, Age, Area Income, Daily Internet Usage, Ad Topic Line, City, Male, Country.', 'The output variable:- Clicked on Ad.\xa0', 'Timestamps are not a relevant feature for our analysis; that’s why we are not considering them.', 'Let’s take a summary and correlation plot of our dataset;', 'From the above correlation graph, we can see no multicollinearity associated with any features, so we take all the features for further modeling. The preparation includes Categorical indexing, One hot encoding for Categorical features and Vector Assembler, which merges multiple columns into vector columns.\xa0', 'The pipeline is used to chain the multiple transformers we used above and to avoid data leakage.\xa0', 'Test ROC:- 0.93']",'Test ROC:- 0.93'
149,apache_spark_on_aws/,https://analyticsindiamag.com/beginners-guide-to-pyspark-how-to-set-up-apache-spark-on-aws/,"['A computer is a powerful machine when it comes to processing large amounts of data faster and efficiently. But considering the no limit nature of data, the power of a computer is limited. In the machine learning context, a machine or computer can efficiently handle only as much data as its RAM is capable of holding, which is very limited. There is a limit to which a machine can be upgraded.', 'But having multiple machines that work together is a whole different story. Cluster computing combines the computing power of multiple machines, sharing its resources for handling tasks that are too much for a single machine.', 'Apache Spark is a framework that is built around the idea of cluster computing. It allows data-parallelism with great fault-tolerance to prevent data loss. It has high-level APIs for programming languages like Python, R, Java and Scala. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.', 'In this article, we will learn to set up an Apache Spark environment on Amazon Web Services.', 'The first thing we need is an AWS EC2 instance. We have already covered this part in detail in another article. Follow the link below to set up a full-fledged Data Science machine with AWS.', 'Make sure to perform all the steps in the article including the setting up of Jupyter Notebook as we will need it to use Spark. Once you are done through the article follow along here.', 'To install spark we have two dependencies to take care of. One is java and the other is scala. Let’s install both onto our AWS instance.', 'Connect to the AWS with SSH and follow the below steps to install Java and Scala.', 'To connect to the EC2 instance type in and enter :', 'ssh -i ""security_key.pem"" ubuntu@ec2-public_ip.us-east-3.compute.amazonaws.com', 'Make sure to put your security key and your public IP correctly.', '', 'On EC2 instance, update the packages by executing the following command on the terminal:', 'sudo apt-get update', 'Install Java with the following command', 'sudo apt install default-jre', 'Verify the installation by typing java --version.', 'You will be able to see a similar output as follows:', '', 'Install Scala by typing and entering the following command :', 'sudo apt install scala', 'Verify by typing scala -version.', '', 'We also need to install py4j library which enables Python programs running in a Python interpreter to dynamically access Java objects in a Java Virtual Machine.', 'To install py4j make sure you are in the anaconda environment. You will see ‘(base)’ before your instance name if you in the anaconda environment. If not type and enter conda activate.To exit from the anaconda environment type\xa0conda deactivate', 'Once you are in conda, type\xa0pip install py4j to install py4j.', 'Head to the downloads page of Apache Spark at https://spark.apache.org/downloads.html and choose a specific version and hit download, which will then take you to a page with the mirror links. Copy one of the mirror links and use it on the following command to download the spark.tgz file on to your EC2 instance.', 'wget http://mirrors.estointernet.in/apache/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz', '', 'Extract the downloaded tgz file using the following command and move the decompressed folder to the home directory.', 'sudo tar -zxvf spark-2.4.3-bin-hadoop2.7.tgz\nmv spark-2.4.3-bin-hadoop2.7 /home/ubuntu/', 'Set the SPARK_HOME environment variable to the Spark installation directory and update the PATH environment variable by executing the following commands', 'export SPARK_HOME=/home/ubuntu/spark-2.4.3-bin-hadoop2.7\nexport PATH=$SPARK_HOME/bin:$PATH\nexport PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH', 'The Spark Environment is ready and you can now use spark in Jupyter notebook.', 'Make sure the PATH variable is set correctly according to where you installed your applications. If your overall PATH environment looks like what is shown below then we are good to go,', 'PATH:', '/home/ubuntu/spark-2.4.3-bin-hadoop2.7/bin:/home/ubuntu/anaconda3/condabin:/bin:/usr/bin:/home/ubuntu/anaconda3/bin/', 'PYTHONPATH:', '/home/ubuntu/spark-2.4.3-bin-hadoop2.7/python:', 'Type and enter pyspark on the terminal to open up PySpark interactive shell:', '', 'Head to your Workspace directory and spin Up the Jupyter notebook by executing the following command.', 'jupyter Notebook', 'Open the Jupyter on a browser using the public DNS of the ec2 instance.', 'https://ec2-19-265-132-102.us-east-2.compute.amazonaws.com:8888', '', 'Import the PySpark module to verify that everything is working properly.\xa0']","'ssh -i ""security_key.pem"" ubuntu@ec2-public_ip.us-east-3.compute.amazonaws.com'
 'sudo apt-get update', 'sudo apt install default-jre', 'java --version', 'sudo apt install scala', 'scala -version', 'conda activate', 'conda deactivate', 'pip install py4j', 'wget http://mirrors.estointernet.in/apache/spark/spark-2.4.3/spark-2.4.3-bin-hadoop2.7.tgz', 'sudo tar -zxvf spark-2.4.3-bin-hadoop2.7.tgz', 'mv spark-2.4.3-bin-hadoop2.7 /home/ubuntu/', 'export SPARK_HOME=/home/ubuntu/spark-2.4.3-bin-hadoop2.7', 'export PATH=$SPARK_HOME/bin:$PATH', 'export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH', '/home/ubuntu/spark-2.4.3-bin-hadoop2.7/bin:/home/ubuntu/anaconda3/condabin:/bin:/usr/bin:/home/ubuntu/anaconda3/bin/', '/home/ubuntu/spark-2.4.3-bin-hadoop2.7/python:', 'jupyter Notebook', 'https://ec2-19-265-132-102.us-east-2.compute.amazonaws.com:8888'"
150,made_easy_with_easyga/,https://analyticsindiamag.com/genetic-algorithms-made-easy-with-easyga/,"['Genetic algorithms(GA) are a rapidly growing area of artificial intelligence and machine learning. They are based on natural selection and genetics. Genetic algorithms are adaptive heuristic algorithms; as such, they represent an intelligent utilization of random search to solve optimization problems. The idea of GA is given by John Holland which follows the principle “Survival of the fittest ” by Charles Darwin about evolution. In computer science, we can say that algorithms performing well or adapting in nature will stay in evolution. Let’s consider we are having a problem, and there are many solutions for this problem, and from this solution space, we need to find an optimized solution. The algorithm will look into all the solutions. Suppose a combined solution can be the best solution. In that case, the algorithm will extract those solutions. By combining them, it will make a child solution. Next time, the generated child solution will be combined with another solution to give their child a solution. This will always help us to improve the results. This is why we say it is focused on optimization. More formally, we can say here we are not finding the solution; we already have the solution; we are finding the best-optimized solution by reproducing from a given solution space.', 'GA is commonly used to generate sustainable solutions of optimization or search problems; It is just like finding the best solution from a huge set of available solutions. To find the best solution, some steps are followed by the algorithms. The flowchart below can better explain the steps.', 'Here in the above flowchart, the lines showing the solutions; let’s start a discussion on blocks of the above flowchart.', 'Initial population –\xa0 this is just a space of solutions for a given problem. The larger the population, the result will be much better. For example, in travelling between two points, there can be many ways available, and some have huge traffic but the shortest distance, and some have low traffic and huge distance. So all ways can be considered as the initial population.', 'Converge (if yes)\xa0 – we have the best solution in this case, and we can proceed with that solution. The genetic algorithm will automatically choose the solution and stop working.', 'Converge (if not) – if the solution is not the best-optimized solution, the algorithm will send the solution to reproduce a new solution from it.', '\xa0Evaluate the fitness – whatever inputs we are providing, the best output should come. So in the evaluation, it checks for the best-fit inputs. For example, we have a search engine, and we are searching for how to make a genetic algorithm, and by mistake, we typed “how to take a genetic algorithm”. So it is the job of our search engine to consider “take” as “make”. To do this, search engines will seek the best fit from the available solution.', 'In the set of solutions, it can have many words like take, cake. So to create “make” algorithm will give some fitness value to “take” and “cake”, and that can be measured.\xa0', 'Select mate – It is the process of selecting a solution with a high fitness value. There will be many words in the above case, selecting the more relevant word for making the best-optimized solution.', 'Crossover – there are various methods to do a crossover. It is a method to generate a new population. As we have discussed, the higher the population, the better the result. So here, the crossover is a technique to increase the population size. For example, we have the words abcde and efghi, in one method of crossover, what it suggests is to swap the keyword by randomly selected place in the word to make a new population. In the picture below we can see the swap between de and hi from the words.', 'Mutation – Generating a new population mutation is nothing but ensuring that the new population is better than the old available population.\xa0', 'These all processes get repeated until the solution does not come under the selection.', 'For more information about genetic algorithms, we can refer to this article.', 'What if we don’t have much knowledge to build a genetic algorithm and are required to use it. A python package called EasyGA came to save us from this difficult situation. Next, in this article, we are going to discuss the basics of EasyGA.', 'As the name suggests, EasyGA is a package to implement genetic algorithms easily. It is a well-designed package to work with GA. It also allows users to customize its feature according to their requirements.\xa0', 'The following code implementation is in reference to the official implementation. Let’s start with the installation of the EasyGA using google colab.', 'Input:', 'We can use the above pip3 install command to install the EasyGA python package.\xa0', 'Let’s look at some basic examples of evolving a generic algorithm to get more knowledge about the package.', 'Importing the library', 'Input:', 'Creating an object of a genetic algorithm.', 'Evolving the genetic algorithm until the termination of it to make the population.', 'Extracting information about the algorithm.', 'Input:', 'Here we can see how our algorithm evolved in terms of population and best and worst chromosomes and how many generations are there in the algorithm.', 'Let’s look at an example to get a clear picture of the package. In this example, we will generate passwords, see the best and worst chromosomes, and visualise the algorithm’s evolution.', 'Evolving an algorithm for password matching.', 'Input:', 'Let’s check for the attributes of the algorithm.', 'Input :', 'Here in the above, we can see some of the population with their fitness value and the best fit and the worst fit chromosome and a historical graph between generation and fitness.', 'Here we have seen that in comparison to the old genetic algorithms, this package has made many things easier to perform. It is efficient and easy to use. Also, in some cases, we don’t need to be worried about the deep knowledge of genetic algorithms With a basic knowledge of them we can use this easily in less lines of code. This package has provided most of the things related to genetic algorithms under one roof and made it easy to use all of them.']","'!pip3 install EasyGA'
 'import EasyGA as EGA', 'ega = EGA.GA()', 'ega.evolve()', ' ega.print_generation()\n ega.print_population()\n ega.print_best_chromosome()\n ega.print_worst_chromosome() ', ' Current Generation : 100\n Chromosome - 0 [5][5][5][5][5][5][5][5][5][5] / Fitness = 10\n Chromosome - 1 [5][5][5][5][5][5][5][5][5][5] / Fitness = 10\n Chromosome - 2 [5][5][5][5][5][5][5][5][5][5] / Fitness = 10\n Chromosome - 3 [5][5][5][5][5][5][5][5][5][5] / Fitness = 10\n Chromosome - 4 [5][5][5][5][5][5][5][5][5][5] / Fitness = 10\n Chromosome - 5 [5][5][5][5][5][5][5][5][5][5] / Fitness = 10\n Chromosome - 6 [5][5][5][5][5][5][5][5][5][5] / Fitness = 10\n Chromosome - 7 [5][5][5][5][5][5][5][5][5][5] / Fitness = 10\n Chromosome - 8 [5][5][5][5][5][2][5][5][5][5] / Fitness = 9\n Chromosome - 9 [5][5][5][2][5][5][5][5][5][5] / Fitness = 9\n Best Chromosome : [5][5][5][5][5][5][5][5][5][5]\n Best Fitness\xa0 \xa0 : 10\n Worst Chromosome : [5][5][5][2][5][5][5][5][5][5]\n Worst Fitness\xa0 \xa0 : 9 ', ' import EasyGA\n import random\n ega = EasyGA.GA()\n password = input(""Please enter a word: \\n"")\n # Basic Attributes\n ega.chromosome_length = len(password)\n ega.fitness_goal = len(password)\n # Size Attributes\n ega.population_size = 50\n ega.generation_goal = 10000\n # User defined fitness\n def password_fitness(chromosome):\n \xa0\xa0\xa0\xa0return sum(1 for gene, letter\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0in zip(chromosome, password)\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if gene.value == letter\n \xa0\xa0\xa0\xa0)\n ega.fitness_function_impl = password_fitness\n # What the genes will look like.\n ega.gene_impl = lambda: random.choice([""A"",""a"",""B"",""b"",""C"",""c"",""D"",""d"",""E"",""e"",\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0""F"",""f"",""G"",""g"",""H"",""h"",""I"",""i"",""J"",""j"",\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0""K"",""k"",""L"",""l"",""M"",""m"",""N"",""n"",""O"",""o"",\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0""P"",""p"",""Q"",""q"",""R"",""r"",""S"",""s"",""T"",""t"",\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0""U"",""u"",""V"",""v"",""W"",""w"",""X"",""x"",""Y"",""y"",\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0""Z"",""z"","" ""])\n # Evolve the genetic algorithm\n ega.evolve() ', ' Please enter a word:\xa0\n analytics india\xa0 magazine\n ', ' ega.print_generation()\n ega.print_population()\n ega.print_best_chromosome()\n ega.print_worst_chromosome()\n # Show graph of progress\n ega.graph.highest_value_chromosome()\n ega.graph.show() ', ' Current Generation : 1467\n Chromosome - 0 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][a][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 25\n Chromosome - 1 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 2 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 3 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 4 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 5 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 6 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 7 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][t][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 8 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 9 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 10 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 11 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 12 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 13 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 14 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 15 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 16 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 24\n Chromosome - 17 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][a][ ][ ][m][a][g][e][z][P][n][e] / Fitness = 23\n Chromosome - 18 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][M][ ][ ][m][a][g][l][z][i][n][e] / Fitness = 23\n Chromosome - 19 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][e][g][a][z][i][n][e] / Fitness = 23\n Chromosome - 20 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][a][ ][ ][m][a][g][e][z][P][n][e] / Fitness = 23\n Chromosome - 21 [C][n][a][l][y][t][i][f][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 22\n Chromosome - 22 [a][n][a][l][y][t][i][c][s][i][i][n][d][i][d][ ][ ][m][a][g][ ][z][i][n][e] / Fitness = 22\n Chromosome - 23 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][K][z][z][i][n][e] / Fitness = 22\n Chromosome - 24 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][a][M][c][n][e] / Fitness = 22\n Chromosome - 25 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][g][e][z][P][n][e] / Fitness = 22\n Chromosome - 26 [a][n][a][l][y][t][i][c][s][ ][K][n][d][i][g][ ][ ][k][a][g][H][z][i][n][e] / Fitness = 21\n Chromosome - 27 [a][n][a][l][y][t][r][c][s][ ][i][n][d][I][g][ ][z][m][a][g][a][z][i][n][e] / Fitness = 21\n Chromosome - 28 [a][n][a][l][C][t][g][c][s][ ][O][n][d][i][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 21\n Chromosome - 29 [a][n][a][l][y][t][i][c][s][r][i][n][d][i][g][ ][ ][m][a][U][c][z][i][n][e] / Fitness = 21\n Chromosome - 30 [a][n][a][l][s][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][u][a][z][i][U][e] / Fitness = 21\n Chromosome - 31 [a][n][a][l][y][P][i][c][s][Z][i][n][d][T][g][ ][ ][m][a][g][a][z][i][n][e] / Fitness = 21\n Chromosome - 32 [a][n][a][l][y][t][i][K][k][ ][i][n][d][i][g][ ][ ][m][Q][g][a][z][i][n][e] / Fitness = 21\n Chromosome - 33 [a][n][a][p][y][t][i][c][s][ ][i][n][d][O][g][ ][ ][m][a][g][a][z][T][n][e] / Fitness = 21\n Chromosome - 34 [a][ ][a][l][y][t][i][c][s][ ][i][n][F][i][g][b][ ][m][a][g][a][z][i][n][e] / Fitness = 21\n Chromosome - 35 [a][n][a][l][y][t][i][K][k][ ][i][n][d][i][g][ ][ ][m][Q][g][a][z][i][n][e] / Fitness = 21\n Chromosome - 36 [a][n][a][l][y][Q][i][c][s][ ][i][D][d][i][M][ ][ ][m][a][g][l][z][i][n][e] / Fitness = 21\n Chromosome - 37 [a][n][a][l][y][t][i][K][k][ ][i][n][d][i][g][ ][ ][m][Q][g][a][z][i][n][e] / Fitness = 21\n Chromosome - 38 [a][n][a][l][y][t][o][c][s][ ][i][n][d][i][t][ ][J][m][R][g][B][z][i][n][e] / Fitness = 20\n Chromosome - 39 [a][n][a][l][y][t][z][c][s][ ][t][n][d][i][M][ ][ ][m][a][g][l][L][i][n][e] / Fitness = 20\n Chromosome - 40 [a][n][a][l][y][ ][i][c][s][ ][i][n][k][J][g][ ][ ][m][a][g][a][z][i][n][C] / Fitness = 20\n Chromosome - 41 [a][n][a][l][f][t][i][c][s][ ][i][n][d][i][J][ ][ ][ ][a][g][a][z][x][h][e] / Fitness = 20\n Chromosome - 42 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][K][z][M][c][n][e] / Fitness = 20\n Chromosome - 43 [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][g][ ][ ][m][a][K][z][M][c][n][e] / Fitness = 20\n Chromosome - 44 [R][n][a][l][y][t][h][c][F][ ][i][n][d][i][t][f][ ][m][a][g][a][z][L][n][e] / Fitness = 19\n Chromosome - 45 [a][n][a][l][y][t][d][c][s][I][y][ ][g][i][g][ ][ ][ ][a][g][a][z][i][n][e] / Fitness = 18\n Chromosome - 46 [s][n][a][l][y][t][i][c][y][ ][j][d][d][i][g][ ][P][h][a][g][a][z][i][n][e] / Fitness = 18\n Chromosome - 47 [a][n][a][l][y][Q][i][c][U][ ][i][D][d][i][M][b][b][m][a][g][l][z][i][n][e] / Fitness = 18\n Chromosome - 48 [a][n][a][R][J][t][i][c][b][ ][i][n][d][i][g][ ][ ][m][a][g][e][z][y][Z][e] / Fitness = 18\n Chromosome - 49 [a][n][a][l][y][t][i][K][k][ ][Y][n][d][i][g][ ][ ][m][Q][g][a][q][y][n][e] / Fitness = 18\n Best Chromosome : [a][n][a][l][y][t][i][c][s][ ][i][n][d][i][a][ ][ ][m][a][g][a][z][i][n][e]\n Best Fitness\xa0 \xa0 : 25\n Worst Chromosome : [a][n][a][l][y][t][i][K][k][ ][Y][n][d][i][g][ ][ ][m][Q][g][a][q][y][n][e]\n Worst Fitness\xa0 \xa0 : 18 '"
151,extremely_boosted_neural_network/,https://analyticsindiamag.com/guide-to-xbnet-an-extremely-boosted-neural-network/,"['So far, we have seen various applications of neural networks; most of the tasks performed by neural networks are based on unstructured data like Images, videos, text, audio files, and neural networks did a pretty good job on them. However, most of the business decisions are made with the help of tabular data. It has been observed that the performance of neural networks on tabular data is not up to the mark. Popular models from ensemble techniques like Random Forest, GradientBoosted, AdaBoost, XG-Boost outperforms the neural network for tabular data because it provides good interpretability.\xa0\xa0', 'A few days back, a naval architecture was launched, ‘XBNet’, which stands for ‘Extremely Boosted Neural Network’, which combines gradient boosted tree with a feed-forward neural network, making the model robust for all performance metrics. In this approach, trees are being trained in every layer of the architecture, and feature importance given by the trees and weight determined by gradient descent is used to adjust the weights of layers where trees are trained.\xa0\xa0\xa0\xa0\xa0\xa0', 'XBNet takes raw tabular data as input, and the model is being trained using an optimization technique called Boosted Gradient Descent which is initialized with the help of feature importance of a gradient boosted trees further it updates the weights of each layer in the neural network in two steps as below:', 'Before moving further, let’s briefly summarize ‘Boosted gradient descent’ and ‘Feature importance in trees’.', 'Gradient boosting is an ML technique for regression, classification and other tasks which produces prediction models in the form of an ensemble of weak prediction models like decision trees. When a decision tree is a weak learner, the resulting algorithm is called gradient boosted trees which usually outperforms Random forest. It builds the model stage-wise as other boosting methods do, and it generalizes them by optimising an arbitrary differentiable loss function.\xa0\xa0', 'Generally, feature importance provides a score indicating how useful or valuable each feature was in constructing the boosted decision tree within the model. The importance is calculated explicitly for each attribute in the dataset allowing attributes to be ranked and compared with each other. To know more details about feature importance, click here.', 'The feature importance of gradient boosted trees is determined by information gain of the tree features, which gives the idea to determine which attribute in a given set of features is most useful. That is used to distinguish the classes that are being learned. The information gain is calculated with the help of entropy. Entropy is used to calculate the homogeneity of a sample. The entropy and information gain are calculated as below; (all the formulas are taken from the official research paper)', 'Let P be a probability distribution such that-', 'P = (p1,p2,p3….pn)', 'where pn is the probability of a data point that belongs to a subset di of the dataset', 'D', 'Later on, calculated information gain is used to determine the feature importance of boosted trees.\xa0\xa0', 'The XBNet architecture creates a sequential structure of layers with input and output layers. The feature importance of the gradient boosted tree are trained at the time initialization of a model. As shown in the above picture, the gradient boosted tree is connected to each layer.', 'While training, the data that is fed at input completes forward and backward propagation and weights of the layer get updated according to gradient descent once. Then, before moving towards the next epoch of training, it goes through all the layers and updates its weight again according to the feature importance of the gradient boosted tree.\xa0', 'To ensure proper balance between weights given by the gradient descent and feature importance, weights given by the feature importance are scaled down to the same power as that of the gradient descent algorithm. This is because, after some epochs, the feature importance remains in the same order due to its nature, but several orders decrease the weights provided by gradient descent.\xa0', 'This architecture’s major and unique highlight is that the layers’ weights depend on the gradient descent algorithm and the feature importance of gradient boosted trees. This, in turn, boosts the performance of architecture.', 'Here we will compare the performance of XBNet and custom neural networks maintaining the same training parameters.', 'Install the architecture using pip as below', '! pip install --upgrade git+https://github.com/tusharsarkar3/XBNet.git', 'Import all the dependencies:', 'Set the input output features and train test split', 'Initialize the architecture with training data; while initializing you need to set input-output dimensions of each layer here. I have set the number of layers as two, so I need to set the dimension manually. Don’t worry; it is pretty straightforward; you will be prompted to do so, as shown below.\xa0\xa0', 'Set the loss function and optimizer.', 'Run the architecture using run_XBNET', 'm,acc, lo, val_ac, val_lo = run_XBNET(x_train,x_test,y_train,y_test,model,criterion,optimizer,epochs=100,batch_size=32)', 'Classification report for training and validation, respectively.\xa0', 'The plot of accuracy and loss-', 'The plot of accuracy and loss –', 'Some test results from the official research paper', 'As I mentioned before, the main highlight of this architecture is that weight distribution through the layer and the way it is being distributed, i.e., maintaining the balance between feature importance and gradient descent, has shown the result extremely well on training and validation data when compared to the custom neural network. The performance, interpretability and scalability of this architecture have set up a new benchmark.\xa0']","'! pip install --upgrade git+https://github.com/tusharsarkar3/XBNet.git'
 'm,acc, lo, val_ac, val_lo = run_XBNET(x_train,x_test,y_train,y_test,model,criterion,optimizer,epochs=100,batch_size=32)'"
152,pyramidal_mesh_alignment_feedback/,https://analyticsindiamag.com/guide-to-pymaf-pyramidal-mesh-alignment-feedback/,"['Generating 3D pose meshes from monocular images is a computer vision problem, aiming to automate a tedious and time-consuming aspect of Visual Effects. Modelling objects with long and complex kinematic chains, such as the human body, is labour intensive as the VFX artist has to go frame by frame to rotoscope different sections of the kinematic chain.\xa0', 'Existing approaches for automating these tasks fall under two broad paradigms: optimization-based and regression-based. Optimization-based approaches directly fit the models to 2D data and produce accurate mesh-image alignments but are slow and sensitive to the initialization. Regression-based approaches directly map raw pixels to model parameters to create parametric models in a feed-forward manner via neural networks.\xa0', 'These models are sensitive to minor deviations in parameters which often leads to misalignment between the generated meshes and the image evidence. In their paper, “3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop”, Hongwen Zhang, Yating Tian, et al. proposed a new feedback loop that utilizes a feature pyramid to rectify the parameters explicitly based on mesh-image alignment.', 'The PyMAF image encoder produces a pyramid of spatial features that provide information of the human pose in the image at different scale levels. This allows the subsequent deep regressor to leverage multi-scale alignment contexts. The point-wise features extracted by the encoder then go through a multi-layer perceptron for dimensionality reduction and are concatenated together to form a feature vector.\xa0 The pose parameters are represented as relative rotations along kinetic chains and are thus sensitive to minor parameter errors. To deal with such misalignments, the parameter regressor uses 2D supervisions on the 2D key-points projected from the estimated mesh and additional 3D supervisions on 3D joints and model parameters when ground truth 3D labels are available.\xa0', 'Regressing mesh parameters in a single pass is challenging; to overcome this limitation existing approaches have employed an Iterative Error Feedback (IEF) loop to update parameters iteratively. Although this approach reduces parameter errors, it uses the same global features each time for parameter update. These global features lack fine-grained information and are not responsive to new, more current predictions. PyMAF introduces a new Mesh Alignment Feedback (MAF) loop that leverages mesh-aligned features. In contrast to using uniformly sampled grid features or global features, the mesh-aligned features provide alignment details of the current estimation, which is more useful for parameter optimization.\xa0', 'Spatial features can easily be affected by noise in images, as can be seen in the image above. To tackle noise caused by occlusion and illumination difference, PyMAF utilizes an auxiliary pixel-wise loss on the spatial features at the last level. This auxiliary supervision provides mesh-image association cues for the image encoder to preserve the most relevant information in the spatial feature maps.\xa0', 'The following code has been taken from the official demo Colab notebook available here.', 'This article went through PyMAF, a regression-based approach for human pose 3D mesh recovery. It introduced a new mesh alignment feedback loop that leverages different scales of spatial information obtained from a feature pyramid. Model parameters are optimized by the feedback loop based on the alignment status of the currently estimated meshes. In addition to that, an auxiliary supervision task is imposed on the spatial feature maps during the training of the regressor. This pixel-wise supervision makes the regressors less susceptible to noise in the images and improves the reliability of the mesh-aligned features. PyMAF was evaluated on both indoor and in-the-wild datasets, and it consistently improved the mesh image alignment performance over previous regression-based methods.\xa0']","'demo.py'
 './sample_video.mp4'"
153,library_for_continual_learning/,https://analyticsindiamag.com/avalanche-a-python-library-for-continual-learning/,"['When does a deployed Machine Learning model fail? There could be some good answers. One among them is when a model is deployed to make predictions on unforeseen data whose pattern is entirely different from training data. This issue is common in dynamic scenarios where non-stationary data streams in continuously. Continual Learning (CL) is a real-time machine learning approach that tries to solve dynamically varying data patterns. While making predictions on incoming unforeseen data, a CL model uses the same data and the feedback on its predictions (whether correct or incorrect) to train itself continuously in real-time.\xa0', 'Continual Learning is also termed Incremental Learning or Life-long Learning. Though some attempts have been made in the field of Continual Learning, the developed methods lack in providing end-to-end training, reproducibility, portability and scalability. When a CL model attempts to learn the new pattern among data, it may forget the previous learning knowledge.\xa0\xa0', 'To this end, ContinualAI, a non-profit research organization and open community, including researchers from more than 15 organizations across the world, has introduced Avalanche, an end-to-end Python library for all Continuous Learning tasks. Avalanche is an open-source library based on PyTorch. This enables quick prototyping, training, evaluation, benchmarking and deployment of Continual Learning models and algorithms with minimal code. In contrast, Avalanche is an all-in-one Continual Learning solution driven by an open development community aiming for sharing, collaboration and improvement.', 'Avalanche has been designed keeping in mind the challenges of real-world applications and research projects. Avalanche’s major design principles are comprehensiveness, consistency, ease of use, modular design, reusability, reproducibility, portability, efficiency, and scalability. This makes Avalanche robust, suitable for any Continuous Learning environment and extensible.\xa0', 'Avalanche introduces its data structure, called learning experience (e). A learning experience is a task or batch of one or more samples. A learning experience is used to update the model in real-time. It is usually represented by a triplet of (x,y,t), where x refers to the input streaming data, y refers to its target and t refers to the task label. Task labels may be available in supervised learning but not in unsupervised learning. The actual data streams in as a sequence of experiences.', 'Avalanche is composed of five modules:', 'The Benchmarks module provides most of the benchmark datasets, contents of the stream, task labels, etc. in an off-the-shelf strategy. By performing different class assignments, different instantiations of the same dataset with different streams can be created in Avalanche. The supported benchmarks in Avalanche are Split/Permuted/Rotated MNIST, Split Fashion MNIST, Split CIFAR 10/100/110, Split CUB200, Split ImageNet, Split TinyImageNet, Split/Permuted/Rotated Omniglot, CORe50, OpenLORIS, and Stream5. Further, Avalanche employs different benchmark-creation scenarios using which new benchmarks, new datasets can be created and/or added to the library. Benchmark instances are generally modeled as the composition of experience streams.\xa0', 'The Training module of Avalanche supports both popular continual learning algorithms and custom-developed algorithms. The strategies that Avalanche supports at present are Naive (Fine-tuning), CWR, Replay, GDumb, Cumulative, LwF, GEM, A-GEM, EWC, Synaptic Intelligence, AR1, and Joint Training. Avalanche expects more training algorithms and strategies to be included in the near future. Training can be performed with a few lines of code common to any algorithm or strategy in the library.', 'The dynamic system of continual learning is tough to monitor. Avalanche provides the Evaluation module to monitor training progress effectively in real-time. Evaluation metrics of Avalanche consists of Accuracy, Loss (user-specified), Confusion Matrix, Forgetting, CPU Usage, GPU usage, RAM usage, Disk Usage, Timing, Multiply, and Accumulate. With few pieces of code, Avalanche is able to monitor and control its model’s training process.', 'Avalanche contains popular and ready-to-deploy architectures and models in the Models module. At present, Avalanche contains a collection of feedforward neural networks, convolutional neural networks, and a pre-trained MobileNet v1 architecture. In the near future, more pre-trained models will be included in the library.', 'Logging is more important in continual learning because of the data stream’s dynamic nature. The loggers that are available in the module are Text Logger, Interactive Logger, Tensorboard Logger, Weights and Biases (W&B) Logger. Logging helps decide automatically whether to stop or start training, or to alter parameters, and so on.', 'Avalanche and its dependencies can be installed using the following command. Avalanche runs on either CPU or CUDA GPU based on availability. Since Avalanche is built on top of PyTorch, a PyTorch environment has to be created.', 'Create the environment by importing necessary libraries and modules.', 'Configure the device settings by checking for availability of CUDA GPU. Instantiate a simple multi-layer perceptron model.', 'Prepare the benchmark dataset. Split the dataset into train and test sets.', 'Instantiate optimizer and loss functions.', 'Devise a training strategy. Here, the Naive strategy is implemented.', 'Perform training, evaluate the model and log results.', 'A portion of the output:', 'Training may take some time based on the device configuration. Have a look at the log results.', 'A portion of the output:', 'Find here the Colab Notebook with the above code implementation.', 'Avalanche provides the strategy of clean and simple code that makes it easy to use.', 'Defining a strategy and performing training in a few lines of codes (source).']","' # Install Avalanche and its dependencies\n !pip install git+https://github.com/ContinualAI/avalanche.git '
 ' import torch\n # use CrossEntropyLoss\n from torch.nn import CrossEntropyLoss\n # use stochastic GD optimizer\n from torch.optim import SGD\n # import the PermutedMNIST dataset\n from avalanche.benchmarks.classic import PermutedMNIST\n # import the SimpleMLP dataset\n from avalanche.models import SimpleMLP\n # we will use Naive training strategy\n from avalanche.training.strategies import Naive ', ' # Configure the device settings\n # check for CUDA GPU\n device = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n # model instantiation\n model = SimpleMLP(num_classes = 10) ', ' # Benchmark instantiation\n perm_mnist = PermutedMNIST(n_experiences=3)\n # split stream into train and test set\n train_stream = perm_mnist.train_stream\n test_stream = perm_mnist.test_stream ', ' # define optimizer and loss function for training\n optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n criterion = CrossEntropyLoss() ', ' # Instantiate Continual learning strategy\n strategy = Naive(\n \xa0\xa0\xa0\xa0model, optimizer, criterion, train_mb_size=32, train_epochs=2,\n \xa0\xa0\xa0\xa0eval_mb_size=32, device=device) ', ' results = []\n # train and test epochs\n for train_task in train_stream:\n \xa0\xa0\xa0\xa0# training\n \xa0\xa0\xa0\xa0strategy.train(train_task, num_workers=4)\n \xa0\xa0\xa0\xa0# evaluate the model and store the results\n \xa0\xa0\xa0\xa0results.append(strategy.eval(test_stream)) ', 'results'"
154,library_built_on_c/,https://analyticsindiamag.com/flashlight-facebook-launches-ml-library-built-on-c/,"['Earlier this month, Facebook AI launched an open-source machine learning library called Flashlight that lets developers and researchers execute AI/ML applications seamlessly via C++ API. The library is currently available on Github.\xa0', 'Facebook AI said its machine learning library is intuitive and simple to use as it contains only the most basic building blocks needed for research. Further, it claimed that it takes seconds to rebuild the entire library and training machine learning pipelines.', '“Deep and ML frameworks are good at what they do — but altering the internals of these frameworks has traditionally proved difficult. Finding the right code to change is time-consuming and error-prone, as low-level internals can be unintentionally obfuscated closed-source or hand-tuned for particular purposes. And once you have made changes, recompiling the framework afterwards is both time-and compute-intensive,” said Facebook Artificial Intelligence Research (FAIR) in its blog post.', 'Like dlib, mlpack and Shogun, Flashlight is also written in modern C++. It has an incredibly low framework overhead, as modern C++ enables parallelism and speed. In addition to this, it also provides simple bridges to integrate code from low-level domain-specific languages and libraries.', 'Other packages C++ supports includes Tensorflow for deep learning, Microsoft Cognitive Toolkit (CNTK) for deep learning; OpenCV for computer vision; and DyNet and FANN for neural networks.', 'Tech experts believe C++ has its limitations as it is very syntax oriented. In comparison, Python or R, C++ are beginner-friendly, and there is plenty of library support.', 'Source: Facebook', 'While modern C++ eliminates the need for tasks like memory management for providing powerful tools for functional programming, Flashlight claims to support research in C++ with no external figures or bindings to perform tasks such as threading, memory mapping, or interoperating with low-level hardware. Ergo, integrating fast, parallel code becomes direct and straightforward.', 'Source: Facebook\xa0', '“We are open-sourcing Flashlight to make for the AI community to tinker with the low-level code underpinning deep and ML frameworks, taking better advantage of the hardware at hand and pushing the limits of performance,” said Jacob Kahn, a research engineer at FAIR.', 'Before this, Facebook AI has introduced a new dataset called Casual Conversations to check the robustness of AI models across four primary dimensions — age, gender, skin type and lighting.\xa0', 'Facebook AI said Flashlight is developed on top of a shallow stack of basic abstractions that are modular and easy to use. For this, it has used an ArrayFire tensor library, which supports dynamic tensor shapes and types, thereby removing the need for rigid compile-time specifications and C++ templates. Besides this, ArrayFire helps in optimising operations with an efficient just-in-time compiler.', 'Flashlight also includes custom, tunable memory managers and APIs for distributed and mixed-precision training. In addition to this, Flashlight also features modular abstractions for working with data and training at scale, combined with a fast, lightweight Autograd. “These components are built to support general research directions, whether in deep learning or elsewhere,” said Kahn.\xa0']",
155,framework_for_information_retrieval/,https://analyticsindiamag.com/guide-to-pyterrier-a-python-framework-for-information-retrieval/,"['Information Retrieval is one of the key tasks in many natural language processing applications. The process of searching and collecting information from databases or resources based on queries or requirements, Information Retrieval (IR). The fundamental elements of an Information Retrieval system are query and document. The query is the user’s information requirement, and the document is the resource that contains the information. An efficient IR system collects the required information accurately from the document in a compute-effective manner.', 'The popular Information Retrieval frameworks are mostly written in Java, Scala, C++ and C. Though they are adaptable in many languages, end-to-end evaluation of Python-based IR models is a tedious process and needs many configuration adjustments. Further, reproducibility of the IR workflow under different environments is practically not possible with the available frameworks.', 'Machine Learning heavily relies on the high-level Python language. Deep learning models are built almost on one of the two Python frameworks: TensorFlow and PyTorch. Though most natural language processing applications are built on top of Python frameworks and libraries nowadays, there is no well-adaptable Python framework for the Information Retrieval tasks. Hence, here comes the need for a Python-based Information Retrieval framework that supports end-to-end experimentation with reproducible results and model comparisons.\xa0', 'Craig Macdonald of the University of Glasgow and Nicola Tonellotto of the University of Pisa have introduced a Python framework, named PyTerrier, for Information Retrieval. This framework proposes different pipelines as Python Classes for Information Retrieval tasks such as retrieval, Learn-to-Rank re-ranking, rewriting the query, indexing, extracting the underlying features and neural re-ranking. An end-to-end Information Retrieval system can be easily built with these pre-established pipeline elements. Moreover, a built IR architecture can be scaled or extended in the future as per the requirements.', 'An experiment architecture for comparing two different Information Retrieval models has many key components such as Ranked retrieval, Fusion, Feature extraction, LTR (Learn-to-Rank) re-ranking and Neural re-ranking. The workflow is represented in a directed acyclic graph (DAG) with complex operation sequences. The PyTerrier framework helps build such a complex DAG problem in an end-to-end trainable pipeline.\xa0', 'PyTerrier is a declarative framework with two key objects: an IR transformer and an IR operator. A transformer is an object that maps the transformation between an array of queries and the corresponding documents.\xa0', 'The basic retrieval process, for example, in PyTerrier is performed using the following Python code template.', 'Here, Q is the input query and R’ is the retrieved output document. Thus, a complex IR task can be performed with simple Python codes. Also, PyTerrier provides operator overloading for conventional math operators to perform custom IR operations.', 'The newly introduced PyTerrier Framework is instantiated on two public datasets so far: the Terrier dataset and the Ansereni dataset. More dataset implementations would be expected soon.', 'PyTerrier is available as a PyPi package. We can simply pip install it.', 'Import the library and initialize it.', 'Use one of the in-built datasets to perform the retrieval process and extract its index.', 'Extract queries as topics for the dataset.', 'Perform retrieval easily using a few commands as shown below.', 'It can be observed that the documents are retrieved and ranked. Further, the results can be saved to the disk using the write_results method available in the io class of the PyTerrier framework.', 'Now, evaluation is performed by comparing the results with the ground truth available in-built. Get the ground truth query results.', 'Evaluate the query results.', 'Evaluation results can also be obtained for per-query results. Here, the evaluation is performed based on the ‘map’ metric on all documents under query.', 'A portion of the output:', 'Find the Notebook with these code implementations here.', 'Create the environment by importing the necessary libraries and initializing the PyTerrier framework.', 'Download an in-built dataset, its indices, queries and ground truth results.', 'For ranking the queries, the standard ‘BM25’ model is used in this example. The traditional ‘TF-IDF’ model and the ‘PL2’ model are used to re-rank the query results.', 'Create a PyTerrier pipeline to perform the above said example task and make a query.', 'In the above output, the term ‘score’ represents the ranking score of the BM25 model and the term ‘features’ represents the re-ranking scores of the TF-IDF and PL2 models. However ranking at the first step and re-ranking in two successive steps consumes more time. To tackle this issue, PyTerrier introduces a method, called FeaturesBatchRetrieve. Let’s implement the method for efficient processing by ranking and re-ranking, all in one go.', 'PyTerrier has a pipeline method, called compile(), which optimizes the ranking and re-ranking processes automatically. This approach also yields the same results as above at around the same compute-time. An example implementation is as follows:', 'After performing ranking and re-ranking, a machine learning model can be built to Learn-to-Rank (LTR). Split the available data into train, validation and test sets.', 'Build a Random Forest model to perform the LTR and obtain the results.', 'Build an XGBoost model to perform the LTR and obtain the results.', 'Find the Notebook with these code implementations here.', 'We discussed the newly introduced PyTerrier framework, its architecture and its implementation for Information Retrieval tasks. We learnt how to use the framework with two example hands-on implementations for the applications, a Simple Query-Retrieval and a Learn-to-Rank machine learning model. PyTerrier has enormous algorithms and in-built datasets to perform almost any Information Retrieval task with minimal efforts. This framework is also established as a Python-built one focusing chiefly on simplicity, efficiency and reproducibility.', 'Research paper', 'Github repository', 'Indexing with PyTerrier']","'write_results'
 'io', 'FeaturesBatchRetrieve', 'compile()'"
156,to_combine_tree_boosting/,https://analyticsindiamag.com/guide-to-gpboost-a-machine-learning-library-to-combine-tree-boosting/,"['GPBoost is an approach and a software library aimed at combining tree-boosting with mixed-effects models and Gaussian Process (GP); hence the name ‘GP + Tree-Boosting’. It was introduced by Fabio Sigrist, a professor from Lucerne University of Applied Sciences and Arts in December 2020 (research paper).', 'Before going into the details of GPBoost, let us have an overview of the terms ‘Gaussian Process’, ‘Tree-Boosting’ and ‘Mixed-effects Models’ mean.', 'Gaussian Process:', 'Gaussian process (GP) is a collection of some random variables such that each finite linear combination of those variables has a normal distribution. It is a probabilistic distribution over functions possible for resolving uncertainty in Machine Learning tasks such as regression and classification. Visit this page for a detailed description of GP.', 'Tree-Boosting:', 'Tree-boosting or boosting in decision trees refers to creation of an ensemble of decision trees for improving the accuracy of a single tree classifier or regressor. In tree-boosting, each of the trees in the collection is dependent on its prior trees. As the algorithm proceeds, it learns from the residual of the preceding trees.\xa0', 'Mixed-effects Models:', 'Mixed-effects models are statistical models which contain random effects (model parameters are random variables) and fixed effects (model parameters are fixed quantities). Read in detail about mixed-effects models here.', 'Originally written in C++, the GPBoost library has a C-language API. Though it combines tree-boosting with GP and mixed-effects models, it also allows us to independently perform tree-boosting as well as using GP and mixed-effects models.', 'Tree-boosting and GP, two techniques are achieving state-of-the-art accuracy for making predictions have the following advantages, which can be combinedly leveraged using GPBoost.', 'Pros of GP and mixed-effects models:', 'Pros of tree-boosting:', 'GPBoost algorithm', 'The label/response variable for GPBoost algorithm is assumed to be of the form:', 'y = F(X) + Zb + xi \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 …(i)', 'where,', 'X: covariates/ features/ predictor variables', 'F: non-linear mean function (predictor function)', 'Zb: random effects which can comprise Gaussian process, grouped random effects or a sum of both', 'xi: independent error term', 'Training the GPBoost algorithm refers to learning the hyperparameters (called covariance parameters) of the random effects and F(X) using an ensemble of decision trees. In simple terms, the GPBoost algorithm is a boosting algorithm that iteratively learns the hyperparameters using natural gradient descent or Nesterov accelerated gradient descent and adds a decision tree to the ensemble using Newton and/or gradient boosting. Trees then learn using the library LightGBM.', 'Here’s a demonstration of combining Tree-Boosting with GP models using GPBoost Python library. The code has been implemented using Google Colab with Python 3.7.10, shap 0.39.0 and gpboost 0.5.1 versions. Step-wise explanation of the code is as follows:', '!pip install gpboost', '!pip install shap', 'num_total = num_grid_pts**2 + num_train', 'Compute total number of grid coordinates', 'Compute random samples from a normal distribution (as many as total number of data\xa0', 'points) and perform its dot product with parameter C.', 'b_total = C.dot(np.random.normal(size=num_total))', 'Prepare training data GP', 'b = b_total[(num_grid_pts*num_grid_pts):num_total]\xa0', 'Compute independent error term', 'Compute the response variable (known as ‘label’) using the equation (i).', 'y = F_X + b + xi', 'Mean square error (MSE): 0.367071629709704']","'y = F(X) + Zb + xi '
 '!pip install gpboost', '!pip install shap', 'num_total = num_grid_pts**2 + num_train', 'b_total = C.dot(np.random.normal(size=num_total))', 'b = b_total[(num_grid_pts*num_grid_pts):num_total]\xa0', 'y = F_X + b + xi', 'Mean square error (MSE): 0.367071629709704'"
157,https://analyticsindiamag.com/pykg2vec/,https://analyticsindiamag.com/pykg2vec/,"['Knowledge Graph is an ER-based (Entity-Relationship) feature representation learning approach that finds applications in various domains such as natural language processing, medical sciences, finance and e-commerce. Knowledge Graph evolves as a dense graphical network where entities of the data form the nodes and relations form the connections between those nodes. As the data size grows in a large scale, a Knowledge Graph becomes very dense and high-dimensional, demanding powerful computational resources. This issue was alleviated by introducing Knowledge Graph Embedding (KGE), which maps the high-dimensional representation into a compute-efficient low-dimensional embedded representation.\xa0', 'Many recent researches have concentrated on Knowledge Graph Embedding, and thus powerful task-focused methods have been developed. Some generalized platforms such as PyKEEN, OpenKE and AmpliGraph are introduced as libraries that support KGE models and datasets. Research and other deployment needs can be fulfilled directly using these open source libraries. These libraries make the source code readily available, enable adapting the source code to the custom dataset, help correctly parameterize the models, and compare one method against another.', 'The available open-source KGE libraries impose specific preset hyper-parameters that do not match for all models. Rather, they work for specific algorithms, dataset pipelines and benchmarks. For new datasets, these libraries mostly fail to discover the golden hyper-parameters on their own, forcing the user to try different predefined hyper-parameters to determine the right ones. These drawbacks question the generalizability of these libraries while there presents a high demand for the generalization.', 'Shih-Yuan Yu, Sujit Rokka Chhetri and Mohammad Abdullah Al Faruque of the University of California-Irvine, Arquimedes Canedo of the Siemens Corporate Technology, and Palash Goyal of the University of Southern California have introduced a robust and powerful library for Knowledge Graph Embedding, named Pykg2vec. This library overcomes previous libraries’ difficulties and provides a versatile and generalized platform for different research and other deployments.', 'The facts in a Knowledge Graph are represented in triplets in the form of (h, r, t), where h is the head entity, t is the tail entity, and r is the relation between those entities. Knowledge Graph Embeddings learns a function that maps these high-dimensional facts into low-dimensional vectors by preserving the original high-dimensional features’ quality. The original facts are usually termed the positive triplets. A few of these triplets are sampled; either their heads (?, r, t) or tails (h, r, ?) are corrupted and termed the negative triplets. The KGE model is trained to award rewards for positive triplets and penalties for negative triplets. Loss functions such as binary cross-entropy loss or logistic loss are used in this model to find the corrupted entity or to check whether a given triplet is positive or negative.\xa0', 'The library discovers the golden hyper-parameters suitable for the model-dataset pair on its own. This is termed the Golden Setting. Users can customize these settings too. This library incorporates Bayesian Optimizer to perform the hyper-parameters discovery.', 'Pykg2vec is built using Python on top of the PyTorch framework. Nevertheless, it supports TensorFlow implementation also. Official codes are provided for both the PyTorch version and the TensorFlow version.', 'and, inside the base activation command mode, provide:', 'On the other hand, if the local machine is enabled only with CPU, the following command may be of help.', 'It should be noted that training takes around 2 hours to complete in a CPU runtime. Users may opt for a GPU runtime for quick training and inference.', 'Pykg2vec is a versatile Python library for training, testing, experimenting, researching and educating the models, datasets and configurations related to the Knowledge Graph Embedding. Pykg2vec presently supports 25 state-of-the-art KGE models: SLM, ConvE, Complex, RotatE, CP, TuckER, SME, DistMult, NTN, ConvKB, TransE, TransH, TransR, TransD, TransM, KB2E, MuRP, InteractE, OctonionE, RESCAL, Analogy, ProjE, SimplE, HypER and QuatE.']",'/content/pykg2vec/'
158,library_for_tensor_learning/,https://analyticsindiamag.com/guide-to-tensorly-a-python-library-for-tensor-learning/,"['TensorLy is an open-source Python library that eases the task of performing tensor operations. It provides a high-level API for dealing with deep tensorized neural networks and tensor methods. It was created in 2015 by a senior research scientist at NVIDIA Research Group named Jean Kossaifi. It was presented at the NeurIPS workshop and was later published byJean Kossaifi, Yannis Panagakis, Anima Anandkumar and Maja Pantic in a JMLR paper in February 2019.', 'The project is supported by the following universities and organizations:', 'TensorLy makes it easy to handle tensor decomposition, tensor learning and tensor algebra. It’s robust backend system enables users to perform computations with NumPy, TensorFlow, PyTorch, MXNet, CuPy or JAX. It can run methods that can scale on multiple CPUs or GPUs.', 'TensorLy was initially built on the top of NumPy and SciPy with a soft dependency on plotting library Matplotlib. A backend system was added to it later on for combining tensor methods with deep learning and for enabling transparent switching between various libraries and platforms.. It provides a simple and efficient API such as that of scikit-learn. However, scikit-learn deals with observations in terms of vectors whereas TensorLy represent them as higher-order arrays called tensors.\xa0', 'The methods, functionalities and libraries supported be summarized in the following diagrams:', 'NOTE: TensorLy has been developed and tested only on Python 3 version.', 'pip install -U tensorly\xa0', '(option -U above is optional; use it for packages updates)', 'conda install -c tensorly tensorly', 'Here, we demonstrate how to use this library for basic tensor operations, tensor decomposition and tensor regression. Step-wise explanation for code of each of the use cases is as follows:\xa0', 'orig_tensor = tl.tensor(np.arange(40).reshape((5, 2, 4)))', 'A 3D array containing elements from 0 to 39 will be created’', 'Print the created tensor', 'unfold() method performs unfolding starting from mode 0, and unfolds till\xa0(n-1)th dimension (where, ‘n’ is the dimension of the tensor).', 'The unfolded tensor after executing above lines of code:', 'The re-folded tensor will be as follows:', 'Here, we demonstrate compression of colored image of a raccoon using parafac decomposition and Tucker decomposition techniques.', '‘Rank’ here refers to the number of directions required to describe it.']","'pip install -U tensorly\xa0'
 'conda install -c tensorly tensorly', 'orig_tensor = tl.tensor(np.arange(40).reshape((5, 2, 4)))'"
159,for_gaussian_process_models/,https://analyticsindiamag.com/guide-to-gpytorch-a-python-library-for-gaussian-process-models/,"['GPyTorch is a PyTorch-based library designed for implementing Gaussian processes. It was introduced by Jacob R. Gardner, Geoff Pleiss, David Bindel, Kilian Q. Weinberger and Andrew Gordon Wilson – researchers at Cornel University (research paper).', 'Before going into the details of GPyTorch, let us first understand what a Gaussian process means, in short.', 'In probability theory and statistics, the Gaussian process refers to a stochastic process i.e. a collection of random variables indexed by time or space in such a way that each finite collection of the random variables has a multivariate normal distribution (every finite linear combination of the variables is normally distributed).\xa0', 'You might have heard about statistical inference techniques such as Bayesian inference using which one can represent uncertainty over numeric values like the outcome of a dice roll or the height of a person. Gaussian process instead is a probability distribution over possible functions. Find a detailed description of the Gaussian process here.', 'GPyTorch enables easy creation of flexible, scalable and modular Gaussian process models. It is implemented using PyTorch. It performs GP inference via Blackbox Matrix-Matrix multiplication (BBMM).\xa0', 'Pros of GPyTorch', 'Here’s a demonstration of training an RBF kernel Gaussian process on the following function:', 'y = sin(2x) + E \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 …(i)', 'E ~ (0, 0.04)', '\xa0(where 0 is mean of the normal distribution and 0.04 is the variance)', 'The code has been implemented in Google colab with Python 3.7.10 and GPyTorch 1.4.0 versions. Step-wise explanation of the code is as follows:', '!pip install gpytorch', 'We have used exact inference – the simplest form of GP model', 'For most of the GP regression models, following objects should be constructed:', 'The two methods defined above are components of the Exact (non-variational) GP model.', 'The _init_ method takes a likelihood and the training data. It then constructs objects like mean module and kernel module required for the ‘forward’ method of the model. The ‘forward’ method takes in some data x. It returns a multivariate normal distribution with prior mean and covariance computed at x.', 'lkh = gpytorch.likelihoods.GaussianLikelihood()', 'Initialize the GP model', 'model = ExactGPModel(x_train, y_train, lkh)', 'opt = torch.optim.Adam(model.parameters(), lr=0.1)', 'l = gpytorch.mlls.ExactMarginalLogLikelihood(lkh, model)']","'!pip install gpytorch'
 'lkh = gpytorch.likelihoods.GaussianLikelihood()', 'model = ExactGPModel(x_train, y_train, lkh)', 'opt = torch.optim.Adam(model.parameters(), lr=0.1)', 'l = gpytorch.mlls.ExactMarginalLogLikelihood(lkh, model)'"
160,dont_require_gradient_computation/,https://analyticsindiamag.com/fbs-new-python-library-nevergrad-provides-a-collection-of-algorithms-that-dont-require-gradient-computation/,"['Nevergrad, an open-sourced Python3 toolkit by Facebook for developers offers an extensive collection of algorithms to avoid gradient optimization and present them in a standard ask-and-tell Python framework. The platform enables AI researchers, machine learning scientists, and enthusiasts whose work involves derivative-free optimization to implement state-of-the-art algorithms and methods to compare performance in different settings. ', 'The library includes a broad range of optimizers, such as FastGA, Covariance matrix adaptation, Particle Swarm Optimisation, Sequential quadratic programming, etc. The working of this algorithm can be described as multiple points sampled in one function space, a population of best points is selected and these new points are objected around the existing points in order to improve the current population.', 'This tool is a Python 3.6+ library and can be installed with the following code', 'pip install nevergrad', 'One can also install the master branch instead of the latest release with the following code', 'pip install git+https://github.com/facebookresearch/nevergrad@master#egg=nevergrad', 'For cloning the repository, run the following code from inside the repository folder', 'python3 setup.py develop', 'The gradient-free optimizations in Nevergrad are used to solve a variety of machine learning problems such as discussed below:', 'There are basically four goals of this package which are mentioned below:', 'There are also some other features, which are mentioned below:', 'This open-sourced Python tool for derivative-free optimization is a mathematical technique which is used for simulation-based optimization. The optimization algorithms implemented in this tool relies on a surrogate model of the unknown performance measure. The algorithms are known as derivative-free because they basically do not require the computation of the performance measure, unlike traditional optimization algorithms. ', 'Click here to get the code.', 'Zeroth-Order Optimisation (ZOOpt) provides an efficient derivative-free solver and can be easily used. This toolbox provides a Python package for single-thread optimization and a light-weighted distributed version with the help of Julia language for Python described functions. It mainly focuses on optimization problems in machine learning, addressing high-dimensional, noisy as well as large-scale problems. It does not rely on the gradient of the objective function, but instead, learns from samples of the search space. ']",
161,for_process_mining_algorithms/,https://analyticsindiamag.com/guide-to-pm4py-python-framework-for-process-mining-algorithms/,"['Processes are all around us. Any series of tasks that together achieve an objective can be called a Process. Thanks to the digital revolution copious amounts of data related to diverse processes are being generated and accumulated. In the field of Data Science, analysis and drawing insights from the operational processes is of particular importance. Modelling the process allows us to perform conformance checks and even provide us with the capability to improve the processes. This kind of extraction of insights from event data is called Process Mining. In this article let’s dive deeper into the process mining techniques with python.', 'Process Mining is the amalgamation of computational intelligence, data mining and process management. It refers to the data-oriented analysis techniques used to draw insights into organizational processes. Following is a general framework of process mining.', 'Real-world events and business processes control the software systems and generate event logs. Each log corresponds to activity along with extra information such as timestamp, type, the context of the event etc. The availability of this kind of data is crucial for the application of Process Mining. A model is built on top of this data which can present the processes occurring in an actionable way.', 'Process Mining consists of three main components: Model Discovery, Conformance checking and Model Enhancement. Discovery is the process of automatically generating a model from event logs that can explain the logs themselves without any prior knowledge. There are several algorithms that can be used for this discovery process. An Example Process Model generated by an automated platform', 'The second component of process mining is conformance checking. In this step,\xa0 we juxtapose the event logs with the process model of the same process. This reveals any non-conformances. Example: Transactions over 1 lakh rupees require the PAN card of the user. This constraint can be expressed by the process model. Then we can check all the event logs to make sure if this rule is followed.', 'In the third step, we use the process model that is discovered and the results of conformance checks to identify the process bottlenecks, circular loops and undesired aberrations in the processes. Equipped with this knowledge a new enhanced process is implemented and a target process model is built. This new process model is again enhanced using the same steps. Repeating these steps over and over results in the continuous improvement of organizational processes.', 'Pm4py is an open-source python library built by Fraunhofer Institute for Applied Information Technology to support Process Mining. Following is the command for installation.', 'This library supports tabular data input like CSV with the help of pandas. But the recommended data format for event logs is XES(EXtensible Event Stream). This is an XML based hierarchical, tag-based log storage format prescribed by IEEE as a standard.', 'Let’s load some bank transaction logs stored in xes format. Data is downloaded from this website.', 'We can see that the three most important attributes, case id, timestamp and name of the event are present. Let us reduce the number of rows by limiting the number of traces. This can be done by pm4py’s own suite of filtering functions.', 'PM4PY supports three formalisms that represent the process models: PetriNets(Place Transition Net), Directly Flow graphs and Process trees. We will confine ourselves to using Petrinets in this article. Following is the description of Petrinets published in the pm4py documentation.', 'Petrinets can be obtained using several different mining algorithms.We will use one such algorithm called alphaminer.', 'Following is an example code to perform conformance checking.We generate a model using a part of the log and then validate the entire log.']","'!pip install -U pm4py'
 "" from pm4py.objects.log.importer.xes import importer as xes_importer\n log = xes_importer.apply('/content/banktransfer(2000-all-noise).xes')\n If we prefer to use pandas to analyse the data we can convert the imported logs as follows.\n import pandas as pd\n from pm4py.objects.conversion.log import converter as log_converter\n df = log_converter.apply(log, variant=log_converter.Variants.TO_DATA_FRAME)\n df.to_csv('banktransfer')\n df "", ' from pm4py.algo.filtering.log.timestamp import timestamp_filter\n filtered_log = timestamp_filter.filter_traces_contained(log, ""2013-01-01 00:00:00"", ""2020-01-01 23:59:59"") ', ' from pm4py.algo.discovery.alpha import algorithm as alpha_miner\n net, initial_marking, final_marking = alpha_miner.apply(filtered_log) ', ' from pm4py.visualization.petrinet import visualizer as pn_visualizer\n gviz = pn_visualizer.apply(net, initial_marking, final_marking)\n pn_visualizer.view(gviz) ', ' from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n from pm4py.algo.filtering.log.auto_filter.auto_filter import apply_auto_filter\n from pm4py.algo.conformance.tokenreplay.diagnostics import duration_diagnostics\n #Generating model using only a part of the log\n filtered_log = apply_auto_filter(log)\n net, initial_marking, final_marking = inductive_miner.apply(filtered_log)\n #Checking the entire log for conformance with the model\n from pm4py.algo.conformance.tokenreplay import algorithm as token_based_replay\n parameters_tbr = {token_based_replay.Variants.TOKEN_REPLAY.value.Parameters.DISABLE_VARIANTS: True, token_based_replay.Variants.TOKEN_REPLAY.value.Parameters.ENABLE_PLTR_FITNESS: True}\n replayed_traces, place_fitness, trans_fitness, unwanted_activities = token_based_replay.apply(log, net,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0initial_marking,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0final_marking,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0parameters=parameters_tbr)\n #Displaying Diagnostics Information\n act_diagnostics = duration_diagnostics.diagnose_from_notexisting_activities(new_log, unwanted_activities)\n for act in act_diagnostics:\n \xa0\xa0\xa0\xa0print(act, act_diagnostics[act]) '"
162,library_for_knowledge_graphs/,https://analyticsindiamag.com/guide-to-ampligraph-a-machine-learning-library-for-knowledge-graphs/,"['AmpliGraph is a TensorFlow-based open-source library developed by Accenture Labs for predicting links between concepts in knowledge graphs. It is a collection of neural ML models for statistical relational learning (SRL) (also called Relational Machine Learning) – a subdiscipline of AI/ML which deals with supervised learning on knowledge graphs.', 'Before going into the details of AmpliGraph, let us have a quick look at what a knowledge graph means.', 'The knowledge graph is a diagrammatic representation showing how various entities of a system (e.g. objects, individuals, abstract concepts, events) are interlinked. There is no precise definition of a knowledge graph. In simple terms, it is a graph representing distinct entities and the relationships among them, according to a GitHub repository. It enables data integration and analysis by providing context to a system’s associated data. Visit this page to understand about knowledge graphs in detail. Following is an example of a knowledge graph:', 'Image source: GitHub', 'A graph is represented by a set of nodes representing entities and connecting edges showing relationships among them. It can be homogenous (e.g. a social network having people and their connections – all entities of a common type) or heterogeneous (e.g. graph of a university having different types of entities like students, professors, department etc. and relations like ‘studies-at’, ‘teaches-at’ and so on). Besides, a graph can be a ‘multigraph’ in which we can have multiple directed edges between one or more pairs of nodes, some of which can even form loops.\xa0', 'A university graph, mentioned as a heterogeneous graph in the example above, conveys meaningful information (known as ‘semantics’) its entities and associated relations.', 'Now that we know terminologies like ‘heterogeneous graph’, ‘multigraph’ and ‘semantics’, we can define a knowledge graph as “a heterogenous multigraph in which entities and relations have semantics specific to a particular domain”.', 'AmpliGraph library provides ML models that can create knowledge graph embeddings (KGEs), which are nothing but low-level vector representations of the entities and relations mong them constituting a knowledge graph.\xa0', 'Consider the following knowledge graph and its corresponding KGE to understand what AmpliGraph does:', 'Image source: GitHub', 'There is no direct link between certain entities through some relations in the above knowledge graph, e.g. there is no information shown for how ‘Acme Inc’ and ‘Liverpool’ can be connected through ‘basedIn’ relation. AmpliGraph combines the above KGE with some scoring function and makes predictions about new links.', 'E.g. It predicts that there is an 85% probability of Acme Inc being based in Liverpool, which can be represented as:', 'Image source: GitHub', 'Here’s a demonstration of using AmpliGraph for discovering novel relations in a GoT knowledge graph, the database for which can be downloaded from here and the graph is available at GitHub.', 'The condensed dataset looks something like this:', 'While the graph appears as follows:', 'Image source: GitHub', 'The code here has been implemented using Google colab with Python 3.7.10 and AmpliGraph 1.3.2 versions. We have used ComplEx (Complex Embeddings) model for KGE. Step-wise explanation of the code is as follows:', '!pip install ampligraph', 'Similarly, get the names of unique relations among the entities', 'train_test_split_no_unseen() creates a test set such that test samples are not unseen ones i.e. it involves only those entities and relations which are also parts of the training set.', ""ce_model.fit(X['train'], early_stopping = False)"", 'evaluate_performance() method computes rank at which each test set triple was found\xa0when the model performed link prediction.', 'sc = ce_model.predict(unseen_links)', 'probability = expit(sc)', ""create_tensorboard_visualizations(model, 'Knowledge_Graph_Embeddings')"", 'The ‘Knowledge_Graph_Embeddings’ directory should now have several files as follows:', 'To dive deeper into the AmpliGraph library, refer to the following web links:']","'!pip install ampligraph'
 ""ce_model.fit(X['train'], early_stopping = False)"", 'sc = ce_model.predict(unseen_links)', 'probability = expit(sc)', ""create_tensorboard_visualizations(model, 'Knowledge_Graph_Embeddings')"""
163,implementing_metric_learning_algorithms/,https://analyticsindiamag.com/guide-to-pytorch-metric-learning-a-library-for-implementing-metric-learning-algorithms/,"['Metric Learning is defined as learning distance functions over multiple objects. PyTorch Metric Learning (PML) is an open-source library that eases the tedious and time-consuming task of implementing various deep metric learning algorithms. It was introduced by Kevin Musgrave and Serge Belongie of Cornell Tech and Ser-Nam Lim of Facebook AI in August 2020 (research paper).', 'The flexible and modular design of the PML library enables the implementing various combinations of algorithms in the existing code. Several algorithms can also be combined for a complete train/test workflow.\xa0', 'PML provides two types of mining function:', 'pip install record-keeper tensorboard', 'The following figure gives an overview of the main modules of the PML library:', 'Components of a loss function:', 'Images’ source: Research paper', 'Here’s a demonstration of using TrainWithClassifier trainer of PML on CIFAR100 dataset. The code has been implemented in Google colab with Python 3.7.10 and torch 1.8.0 versions. Step-wise explanation of the code is as follows:', '3. Define the model', '4. Specify device on which torch.Tensor will be allocated', 'device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")', '5. Set trunk model and replace the softmax layer with an identity function', 'Here, we have used an 18 layers deep convolutional network (ResNet18)', 'emb = torch.nn.DataParallel(MLP([trunk_output_size, 64]).to(device))', '7. Training set here has the first 50 classes of the CIFAR100 dataset. Define the classifier which will take the embeddings as input and output a 50 dimensional vector.', 'classifier = torch.nn.DataParallel(MLP([64, 50])).to(device)', '8. Initialize optimizers', 'We have used Adam optimization algorithm', '9. Set the image transforms', '10. Download the original training and validation datasets', '11. Create training and validation sets that are class-disjoint', '12. Initialize class disjoint training and validation set', '13.\xa0 Initialize the loss function', '14.  Initialize the mining function', 'm = miners.MultiSimilarityMiner(epsilon=0.1)', 'Set the data loader sampler; if not specified, random sampling is used', 'Set other training parameters', '15. Form a dictionary of above defined models, optimizers, loss functions and mining functions', '16. Create training and testing hooks using logging_presets module', '17. Define a function for visualiser hook', 'UMAP (Uniform Manifold Approximation and Projection) is a dimension reduction technique that can be used for visualization.', '18. Create the tester', 'testers.GlobalEmbeddingSpaceTester() finds nearest neighbours by considering all the points in the embedding space', '19. Initialize hook for end of epoch. It performs some operation such as logging data at the end of every epoch.', '20. Model trainer\xa0', 'Since we have trunk model -> embedder model -> classifier architecture, we have used TrainWithClassifier trainer. It applies a metric loss and a classification loss to the utput of embedder network and classifier network output respectively.', '21. Model training', 'trainer.train(num_epochs=epochs)', 'Sample output plots for two epochs:', 'For a detailed understanding of the PML library, refer to the following sources:']","'device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")'
 'emb = torch.nn.DataParallel(MLP([trunk_output_size, 64]).to(device))', 'classifier = torch.nn.DataParallel(MLP([64, 50])).to(device)', 'm = miners.MultiSimilarityMiner(epsilon=0.1)', 'trainer.train(num_epochs=epochs)'"
164,for_meta_learning_research/,https://analyticsindiamag.com/guide-to-learn2learn-a-library-for-meta-learning-research/,"['learn2learn is a software library designed for meta-learning research. It was introduced by Sebastien M. R. Arnold from University of Southern California, Praateek Mahajan from Iterable Inc., Debajyoti Datta from University of Virginia, Ian Bunner from University of Waterloo and Konstantinos Saitas Zarkias from KTH Royal Institute of Technology and Research Institute of Sweden (RISE) in the year 2020 (research paper).', 'Before going into the details of learn2learn, let us first understand what meta-learning means.', 'Meta-Learning is a sub-domain of machine learning. It deals with the systematic observation of how various ML approaches perform different learning tasks. The model then learns from meta-data to learn more quickly than otherwise possible. It can absorb information from one task and efficiently generalize for accomplishing unseen tasks. Hence meta-learning is also known as ‘learning how to learn. A detailed explanation of this learning paradigm can be found here.', 'learn2learn has been built on the top of PyTorch library. Prototyping and reproducibility are the two major issues faced by meta-learning researchers which the learn2learn library tackles. Since modern meta-learning methods rely on unconventional ML frameworks, researchers are likely to commit mistakes while prototyping new tasks and algorithms. As a result, it becomes difficult to reproduce existing results. This also is a consequence of inadequate standardized implementations and benchmarks. learn2learn library solves these issues, enabling fast prototyping and correct reproducibility.\xa0', 'learn2learn provides low-level utilities and a unified interface for the creation of new algorithms and domains. It also provides standardized benchmarks and qualitative implementations of existing algorithms. Besides providing such functionalities, it remains compatible with PyTorch-based libraries such as torchvision, torchaudio, torchtext and cherry.', 'Here’s a demonstration of few-shot learning using MAML wrapper for fast-adaptation. MAML (Model-Agnostic Meta-Learning) is a model-agnostic algorithm for meta-learning i.e. it is compatible with any kind of model trained using gradient descent and is applicable to a wide range of tasks such as reinforcement learning, classification and regression. (MAML research paper). The code uses the benchmark interface for loading the mtini-ImageNet dataset.', 'The code has been implemented in Google colab using Python 3.7.10 and learn2learn 0.1.5 versions. Step-wise explanation of the code is as follows:', '!git clone https://github.com/learnables/learn2learn', '!pip install learn2learn', 'learn2learn.data.transforms is a collection of general task transformations (objects which implement the callable interface). Each transformation returns a new task description containing all samples from the dataset under consideration. A task transform modifies the list of task descriptions to create a particular task.\xa0 Among the task transforms imported above', '4.\xa0 Define a function for computing model accuracy', '5. Define a method for fast adaption of the neural network', 'The output will show the train, test and validation metrics for all 100 iterations.', 'Sample output for the first 5 iterations:', 'Note: The output may vary every time you execute the code and also depending upon the execution environment you choose.']","'!git clone https://github.com/learnables/learn2learn'
 '!pip install learn2learn'"
165,an_intels_python_framework/,https://analyticsindiamag.com/guide-to-open-federated-learning-openfl-an-intels-python-framework/,"['Open Federated Learning (OpenFL) is a Python 3 library designed for implementing a federated learning approach in Machine Learning experiments. The framework was developed by Intel Labs and Intel Internet of Things Group.\xa0', 'If you are unfamiliar with the term ‘federated learning’, read ‘What is federated learning?’, ‘How does it work?’ and ‘Advantages of federated learning approach’ sections of this article before proceeding.', 'Before going into the open-source project’s details, let us briefly overview these and some other underlying terminologies.\xa0', 'Image source: Official website', 'The overall design of the OpenFL library centres around the Federated Learning (FL) Plan. A YAML file defines the collaborators, aggregator, connections, models, data, and any other parameters that describe how the model training process will evolve.\xa0', 'Requirement: Python 3.6 or higher version', 'Here’s a demonstration of OpenFL implemented in Google colab with Python 3.7.10, Torch 1.7.1, Torchvision 0.8.2 and OpenFL 1.0 versions. Step-wise explanation of the code is as follows:', '!pip install openfl', '!pip install torch torchvision mnist', ""fx.init('torch_cnn_mnist')"", 'The available workspace templates can be found here. We have used\xa0‘Torch_cnn_mnist’ workspace, which comprises a PyTorch CNN model. It downloads the\xa0MNIST dataset and gets trained in a federation.', 'The structure of the workspace directory can be seen in the output as follows:', 'torchvision.transforms enables performing several common image transformations. Using torchvision.transforms.Compose(), many transforms (mentioned as parameter) can be chained together.', 'We are applying two transformations:\xa0', '(i) transforms.ToTensor()converts a PIL image and numpy.ndarray to a\xa0', 'tensor.', '(ii) transforms.Normalize() normalizes the image with mean and standard deviation provided as parameters.', 'Among the parameters provided to transforms.Normalize() in the above line of code. The two sequences represent means and standard deviations for each channel, respectively.', 'The training images will be downloaded from the ‘data’ directory, and transformation defined by ‘trf’ in step (6) will be applied to them.', '\xa0\xa0opt = lambda x: optim.Adam(x, lr=1e-4) #’lr’ is the learning rate', 'Sample output:', 'According to the output of step (18), the experiment will run 10 times. We can change it using ‘aggregator.settings.rounds_to_train’ parameter.', 'Sample condensed output:', 'For each round of the experiment, the output shows accuracy for locally tuned model and aggregated model as shown above.']","'!pip install openfl'
 '!pip install torch torchvision mnist', ""fx.init('torch_cnn_mnist')"", 'opt = lambda x: optim.Adam(x, lr=1e-4) #’lr’ is the learning rate'"
166,models_with_python_code/,https://analyticsindiamag.com/guide-to-pgmpy-probabilistic-graphical-models-with-python-code/,"['Probabilistic Graphical Models(PGM) are a very solid way of representing joint probability distributions on a set of random variables. It allows users to do inferences in a computationally efficient way. PGM makes use of independent conditions between the random variables to create a graph structure representing the relationships between different random variables. Further, we can calculate the joint probability distribution of these variables by combining various parameters taken from the graph.', 'Mainly, there are two types of Graph models:Bayesian Graph Models:\xa0 These models consist of Directed-Cyclic Graph(DAG) and there is always a conditional probability associated with the random variables. These types of models represent causation between the random variables.Markov Graph Models:\xa0 These models are undirected graphs and represent non-causal relationships between the random variables.', 'pgmpy is a python framework to work with these types of graph models. Several graph models and inference algorithms are implemented in pgmpy. Pgmpy also allows users to create their own inference algorithm without getting into the details of the source code of it. Let’s get started with the implementation part.', 'Requirements\xa0', 'Installation', 'Install pgmpy via pyPI', '!pip install pgmpy', 'pgmpy Demo – Create Bayesian Network', 'In this demo, we are going to create a Bayesian Network. Bayesian networks use conditional probability to represent\xa0 each node and are parameterized by it. For example : for each node is represented as P(node| Pa(node)) where Pa(node) is the parent node in the network.', 'An example of a student-model is shown below, we are going to implement it using pgmpy python library.', 'Define all the conditional probabilities\xa0 tables as shown in the diagram above. These CPD’s are formed by a method in pgmpy called TabularCPD.\xa0', 'Local Independencies : A variable which is independent of its non-descendents given its parents. It can be defined as P( X ⊥ NonDesc(X) | Pa(X)), where NonDesc(X) is the set of variables which are not descendents of X and Pa(X) is the set of variables which are parents of X.\xa0', 'Or,', 'Global Independencies : There are many\xa0 structures possible for global independencies. For two nodes, there are two ways it can be connected.\xa0', 'In the above two cases it is obvious that change in any of the nodes will affect the other. Similar cases can be shown for three nodes.', 'For computing the conditional distribution such as P(G | D=0, I=1), we need to pass an extra argument.', ""print(infer.query(['G'], evidence={'D': 'Easy', 'I': 'Intelligent'}))"", ""infer.map_query(['G'])"", 'Or,', ""infer.map_query(['G'], evidence={'D': 'Easy', 'I': 'Intelligent'})"", 'You can check the full demo here.', 'pgmpy Demo – Extensibility\xa0', 'As discussed above, pgmpy provides a method to create your own inference algorithm. In this demo, we are going to discuss the same. pgmpy contains methods like :', 'Following are the steps:', 'You can check the full demo here.', 'Conclusion', 'In this article, we have discussed the pgmpy python library which provides a simple API for working with Graphical models(bayesian model, markov model,etc. It is highly modular and quite extensible.', 'Official codes, Docs & Tutorials are available at:']","'!pip install pgmpy'
 ""print(infer.query(['G'], evidence={'D': 'Easy', 'I': 'Intelligent'}))"", ""infer.map_query(['G'])"", ""infer.map_query(['G'], evidence={'D': 'Easy', 'I': 'Intelligent'})"""
167,an_embodied_ai_project/,https://analyticsindiamag.com/hands-on-python-guide-to-allenact-an-embodied-ai-project/,"['AllenAct is an open-source project and a modular learning framework designed for researchers and technophiles associated with the domain of Embodied AI. It provides state-of-the-art reproductions of numerous embodied AI models. It also supports the expanding collection of embodied AI tasks, algorithms used to accomplish those tasks as well as environments to run them efficiently.\xa0', 'Are you unfamiliar with the term ‘Embodied AI’? Refer to the ‘Overview of Embodied AI’ section of this article before proceeding!', 'AllenAct was introduced by members of the PRIOR (Perceptual Reasoning and Interaction Research) group at the Allen Institute for Artificial Intelligence (AI2) in August, 2020. AI2 is a non-profit research institution headquartered at Seattle, Washington (United States). It was founded by a co-founder of Microsoft, Paul G. Allen in 2014 with an aim of conducting influential research in the field of AI. (Check out Paul Allen’s personal website here). AI2 is now being led by a leading AI researcher, Dr. Oren Etzioni.', '    Click here for instructions to install any of these supported environments.', 'Requirements: Working version of Python 3.6\xa0', 'Clone the GitHub repository to your local machine\xa0', 'Move to the top directory\xa0', 'cd allenact', 'Install requirements using pipenv', 'pipenv install --skip-lock --dev', 'Install requirements using pip or pip3 (as per your machine’s configuration)', 'pip install -r requirements.txt', 'Visit this page for detailed installation instructions.', 'Also read:', ' How Open-Sourcing AllenAct Provides A Substantial Growth For Embodied AI', 'Here’s a demonstration of training an embodied agent on the Point Navigation task within the RoboTHOR Embodied-AI environment.', 'Before moving on to the implementation, let us have a look at some of the underlying concepts you may be unaware of.', 'The basic thing an embodied agent must know for accomplishing any task is – how to “move around” in the work environment. This task of moving around is formulated as follows:', 'Make your agent locate a beacon somewhere in the environment. This beacon transmits its location in such a way that at any point of time, the agent can get the direction in which it needs to proceed and the euclidean distance which it needs to cover in order to reach the beacon. This particular task is termed as Point Navigation (also known as PointNav).', 'PointNav, though appears simple, is a challenging task for the trained agents to perform. The reason being, such agents are not trained in a free open-space area. Rather, the training is carried out in an environment similar to real-world situations where the agents need to navigate through various hurdles such as walls, doors etc. If they are not familiar with the floor-plan of the environment they have to move in, they need to learn how to predict the design of such structures for proper navigation.\xa0', 'An environment is something where the trained agent exists and performs actions such as moving forward, turning right and so on.\xa0', 'Here to build a Point Navigator, we have used RoboTHOR simulator. RoboTHOR has been designed to train models that can easily be transferred to a real robot, by providing a photo-realistic virtual environment and a real-world replica of the environment. It comprises 60 various virtual scenes with different floor plans – furniture and 15 validation scenes.\xa0', 'AllenAct has a class abstraction by the name Environment. It is a shallow wrapper which provides a uniform interface to the actual environment the agent works in.', 'We have used a reinforcement algorithm called DD-PPO, a decentralized and distributed variant of the PPO algorithm. We let our agent explore the environment on its own, reward it for taking actions to approach the goal and penalize it for actions that deviate it from its goal. We then do model optimization to maximize this reward.', 'We need to set up the environment in which our embodied agent will perform point navigation. Click here and get to know about the environment installation steps.', 'We have used the RoboTHOR PointNav dataset. It can be downloaded from here.', '\xa0It consists of several episodes with randomly generated start and end points for each scene. It also has cached distances between every pair of points in every scene. This helps the agent move in terms of geodesic distance i.e. the actual path distance and not the straight line distance.', 'Setup experiment config file :', 'A Python library needs to be imported for using it. On the contrary,\xa0 AllenAct is structured as a framework with a runner script called main.py. This script runs the experiment specified in a config file. This enables storing minute records of exactly which settings were used to arrive at a particular result. As RL models are generally too costly to train, this facility reduces the expenses to some extent.', 'Create a new directory under projects/.', 'Import the required libraries and classes.', 'Define an experiment config class', 'Define the task parameters', 'Set the simulator Parameters', 'Set the hardware parameters for training engine', 'Where,', 'NUM_PROCESSES is the total number of parallel processes used to train the model.', 'TRAINING_GPUS takes the IDs of the GPUS on which the model should be trained.', 'Likewise, VALIDATION_GPUS and TESTING_GPUS take the ids of the GPUS on which the validation and testing will occur respectively.', 'Define the paths to store the downloaded dataset\xa0', 'Define the sensors', 'Define the preprocessor to be used with the model', 'The preprocessor abstraction in AllenAct is designed with models which transform the raw pixels observed by the agent in the environment, into a complex embedding. Instead of the original image, the embedded version is then used as input to the trainable model.', 'Define the observation inputs to be used by the model', 'Define settings of the simulator', 'Define a method that returns name of the experiment', 'Define the model training pipeline', 'Define a method to return the hardware parameters of each process, based on the list of devices defined above', 'Define the actual model', 'We take a model from the pointnav_baselines project. It is a small convolutional neural network. It takes the output of a ResNet as its rgb input followed by a single-layered GRU. The number of different actions the agent can perform in the environment is fed to the model through the action_space parameter, which we get from the task definition.', 'Define the task sampler', 'This generates instances of tasks for our agent to perform. It reads the specified file and whenever the agent exceeds the maximum number of steps or selects the stop action, it sets the agent to the next starting locations.', 'Define a few helper functions to distribute the work if you have several GPUS and many scenes to be processed', 'Define the task sampler arguments', 'The arguments include the location of the dataset as well as a reference to the distance cache and the environmental arguments for the simulator mentioned above.', 'NOTE: Training the entire dataset takes nearly 2 days on a machine with 8 GPU!', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0However, you can train on a smaller dataset simply by changing the dataset’s\xa0', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0path in the above line of code.', 'Visit this page for the steps to test a pre-trained AllenAct model.', 'Source: https://allenact.org/tutorials/training-a-pointnav-model', 'In this article, we gave an overview of a state-of-the-art framework related to the Embodied AI domain viz. AllenAct. We also gave a demonstration of implementing PointNav task using the modular project.\xa0']","'cd allenact'
 'pipenv install --skip-lock --dev', 'pip install -r requirements.txt'"
168,learning_library_for_pytorch/,https://analyticsindiamag.com/guide-to-torchmeta-a-meta-learning-library-for-pytorch/,"['Torchmeta is an open-source meta-learning library built on top of Pytorch deep learning framework. The objective of Torchmeta is to allow easy benchmarking and reproduce the existing pipelines/ research work in meta-learning and make it accessible to larger communities. Torchmeta was first presented in a research paper called Torchmeta- A meta-learning library for PyTorch. The authors are Tristan Deleu, Tobias Würfl,\xa0 Mandana Samiei,\xa0 Joseph Paul Cohen, Yoshua Bengio. This project is supported and tested by the Montreal Institute for Learning Algorithms(MILA).', 'Torchmeta is inspired by OpenAI Gym(archive), which helped Reinforcement Learning’s progress, with access to multiple environments under a unified interface. Torchmeta provides data-loaders for most of the standard datasets for few-shot classification and regression. It also includes extensions of PyTorch called meta-modules, to simplify the creation of models compatible with classic meta-learning algorithms that sometimes require higher-order differentiation. Torchmeta is fully compatible with torchvision and PyTorch’s DataLoader.', 'Install Torchmeta via pip.', '!pip install torchmeta', 'Torchmeta automates the creation of each meta-training dataset. The data loaders in torchmeta are fully suitable with data components of PyTorch such as Dataset and DataLoader. This library provides a collection of datasets corresponding to classic few-shot classification and regression problems from the meta-learning literature.', 'Few-shot Regression', 'Most of the few-shot regression problems are simple regression having a function(y=ax+b) to give out input values. Torchmeta provides an object called MetaDataset from which meta-training sets are being inherited. Each dataset(that is inherited) corresponds to a specific set of parameters for that specific function. We can then create the dataset by sampling all the known parameters in a particular range to feed it to the function.', 'This\xa0 library currently contains 3 toy problems:', 'A Simple regression task, based on sinusoids, is shown below. It is to instantiate the meta-training set for the sine waves problem', 'You can check the full documentation here.', 'Few-shot Classification', 'For few-shot classification problems, the creation of each dataset follows two-step: First, N classes are sampled from a large collection of candidates and then k examples are chosen per class. These steps are automated by torchmeta under an object called CombinationMetaDataset(from MetaDataset).The library currently contains following few-shot image classification problems:', 'An example of how to instantiate the meta-training is shown below:', 'Training and Testing datasets splits', 'It is important to divide the dataset into a training and testing set for evaluation and meta-optimization. One thing to ensure that these train sets and test sets should not contain common instances. For this, Torchmeta introduces a wrapper over the datasets called Splitter to split the dataset. Shown below is an example of splitting the dataset via Torchmet.', 'Meta DataLoaders', 'The objects generated in Few-shot Regression & Classification can be iterated over to generate datasets. These datasets are PyTorch Dataset objects, and as such can be included as part of any standard data pipeline (combined with DataLoader). Most meta-learning algorithms operate better on batches of tasks. Torchmeta divides the dataset into batches with the help of MetaDataLoader and those batches can be iterated over.\xa0', 'In this part, we will add all of the sections discussed above in DataLoaders.', 'Models in PyTorch are created from basic components called modules and each basic module represents a layer in the neural network containing both the computational graph and its parameters. However, some meta-learning algorithms require high-order differentiation to update the parameters via backpropagation. Torchmeta also provides huge modules called MetaModules(similar to nn.module in PyTorch) for easy implementation of meta-learning algorithms and gives you an option to provide new parameters as an additional input. Metamodule treats these new parameters as a part of the computational graph and backpropagation works as expected. Point to be noted that ith no additional parameters, Torchmeta backpropagation works in a similar way to that of PyTorch with no additional parameters.', 'The figure below shows the MetaLinear module of Torchmeta with and without additional parameters. The first figure shows the initialization of the MetaLinear module. The second figure shows the MetaLinear module’s flow in a default manner and the third figure shows the flow of MetaLinear module with additional parameters.', 'Given below is the example of MetaModule(base class). These modules accept additional argument params in their forward method. The architecture of Neural Network via MetaModule is shown below.', 'In this article, we have discussed Torchmeta and its parts like DataLoader, MetaModule.\xa0', 'To learn more about Torchmeta, you can check the examples available in the repository of the project, as well as this implementation of MAML(MAML article) for a more detailed showcase of all the features of Torchmeta.', 'Official code, docs & Tutorials are available at:']","'!pip install torchmeta'
 'import torchmeta\ntorchmeta.toy.Sinusoid(num_samples_per_task=10, num_tasks=1000000, noise_std=None,\n    transform=None, target_transform=None, dataset_transform=None)', 'import torchmeta\ndataset = torchmeta.datasets.MiniImagenet(""data"", num_classes_per_task=5, meta_train=True,\ndownload=True)', 'import torchmeta \n \ndataset = torchmeta.datasets.MiniImagenet(""data"", num_classes_per_task=5,\nmeta_train=True, download=True)\ndataset = torchmeta.transforms.ClassSplitter(dataset, num_train_per_class=1,\nnum_test_per_class=15, shuffle=True)', '# Helper function,\ndataset = torchmeta.datasets.helpers.miniimagenet(""data"", shots=1, ways=5,\nmeta_train=True, download=True)\ndataloader = torchmeta.utils.data.BatchMetaDataLoader(dataset, batch_size=16)\nfor batch in dataloader:\n  train_inputs, train_labels = batch[""train""] # Size (16, 5, 3, 84, 84) & (16, 5)\n  print(\'Train inputs shape: {0}\'.format(train_inputs.shape))    # (16, 25, 1, 28, 28)\n  print(\'Train targets shape: {0}\'.format(train_labels.shape))  # (16, 25)', 'from torchmeta.datasets import Omniglot\nfrom torchmeta.transforms import Categorical, ClassSplitter, Rotation\nfrom torchvision.transforms import Compose, Resize, ToTensor\nfrom torchmeta.utils.data import BatchMetaDataLoader\n \ndataset = Omniglot(""data"",\n                   # Number of ways\n                   num_classes_per_task=5,\n                   # Resize the images to 28x28 and converts them to PyTorch tensors (from Torchvision)\n                   transform=Compose([Resize(28), ToTensor()]),\n                   # Transform the labels to integers (e.g. (""Glagolitic/character01"", ""Sanskrit/character14"", ...) to (0, 1, ...))\n                   target_transform=Categorical(num_classes=5),\n                   # Creates new virtual classes with rotated versions of the images (from Santoro et al., 2016)\n                   class_augmentations=[Rotation([90, 180, 270])],\n                   meta_train=True,\n                   download=True)\n#split the data into train and test\ndataset = ClassSplitter(dataset, shuffle=True, num_train_per_class=5, num_test_per_class=15)\n#creating batches from dataset\ndataloader = BatchMetaDataLoader(dataset, batch_size=16, num_workers=4)', ""#import the required libraries and Meta modules from torchmeta\nimport torch.nn as nn\nfrom torchmeta.modules import (MetaModule, MetaSequential,\n                               MetaConv2d, MetaLinear)\n \nclass Model(MetaModule):\n    def __init__(self, in_channels, num_classes):\n        super(Model, self).__init__()\n        #MetaSequential is similar to nn.Sequential\n        #A sequential container.\n        #Modules will be added to it in the order they are passed in the constructor.\n        #like in here MetaConv2D is passed as convulational layer and then a ReLU, MaxPool.\n        self.features = MetaSequential(MetaConv2d(in_channels, 64, 3),\n                                       nn.ReLU(),\n                                       nn.MaxPool2d(2))\n        #MetaLinear is similar to torch.nn.Linear\n        #Applies a linear transformation to the incoming data\n        self.classifier = MetaLinear(64, num_classes)\n \n    def forward(self, inputs, params=None):\n        features = self.features(inputs,\n                                 params=self.get_subdict(params, 'features'))\n        logits = self.classifier(features.view((inputs.size(0), -1)),\n                                 params=self.get_subdict(params, 'classifier'))\n        return logits"""
169,ml_model_creation_framework/,https://analyticsindiamag.com/hands-on-python-guide-to-lama-an-automatic-ml-model-creation-framework/,"['LightAutoML (LAMA) is an open-source python framework developed under Sberbak AI Lab AutoML group. It is focussed at\xa0 Automated Machine Learning providing end-to-end solutions for ML tasks. It is a light-weight and efficient framework for performing binary classification, multiclass classification and regression on tabular and text data.\xa0 Apart from the predefined pipelines, it gives an option to create easy-to-use custom pipelines using predefined blocks which includes hyperparameter tuning, data processing, advanced feature selection, automatic time utilization, automatic report creation, Graphical profiling system to find bottlenecks and data bugs.', 'Currently implemented pipelines are:', 'Motivation', 'The motivation for building LAMA is various demanding tasks in the process of the model building like:\xa0', 'Advantages', 'Following are the merits of using LAMA.', 'An example of building ML pipeline is shown below:', 'Requirements', 'Install LAMA framework via pip from PyPI.', 'Demo – Create your own pipeline', 'This demo shows how to create your own pipeline from specified blocks: pipelines for feature generation and feature selection, ML algorithms, hyperparameter optimization etc.\xa0', 'Define all the parameters', 'Initialize the profiler for report', 'Load the dataset', 'Some feature engineering steps as shown below', 'Splitting the data into train set and test set.', 'Create a binary classification task and convert pandas dataframe to AutoML’s dataframe.', 'Next step is to create a feature selector. For this, we will create a LGBM model with default parameters and initialize the simple pipeline and simple estimator and then calculate the importance of features via threshold.', '\xa0\xa0\xa0 Now, create a 1st level ML pipeline for AutoML.\xa0', '\xa0\xa0\xa0 Next is to create a 2nd level pipeline as shown in figure above.', 'Finally, create an AutoML pipeline by calling above pipelines. The automl pipeline contains:', 'Now, fit the model on train data with the target column\xa0 “TARGET” and get OOF predictions.', 'Note: Might take 2-3 minutes to train.', 'After this, we will analyze the feature selection performance of the AutoML model. The code snippet for this is available here. Next, we will predict the output on test data and calculate the scores.', 'Since, we have turned on the profile decorator in Step1, we will now build a report called profiling AutoML. The arrows shown in the report are interactive so you can go as deep as you want.\xa0', 'You can check the full demo here.', 'Demo – AutoML pipeline Preset', 'This demo shows how to use LightAutoML presets (both standalone and time utilized variants) for solving ML tasks on tabular data. Using presets you can solve binary classification, multiclass classification and regression tasks. Let’s dig in the code part!', 'One part of this step is to create a task, set up common rules and create an AutoML instance, fit the model on data, predict on test data and then check scores.', 'Create the binary classification task.', '\xa0\xa0\xa0 Set up common rules as explained below.', 'Create an AutoML instance of TabularAutoML, fit and predict the model on training data via fit_predict\xa0 method.', 'Next step is to predict on test data and check scores. The code is the same as explained above and available here. After this, we will create an AutoML Profiling Report. The code snippet is available here. Download the .html \xa0file and check the report. A snap of the report is shown below.', 'Now, we will repeat the above steps of creating an automl instance but with time utilization then fit and predict the model on training data. Steps are the same as for without time utilization. Predict the output of test data and check scores.', 'The generated profile report is shown below.', 'You can check the full demo here.', 'Demo – Multitask Class', 'This demo shows the AutoML pipeline for the Multiclass Classification. The steps for it are as follows:', 'First, create a timer so that the model won’t go out of time limit.', 'Next, create a feature selector.', 'After this, we will create LGBM pipelines. It is the same as we discussed above with a small change i.e., with timer.', 'Create a linear pipeline for AutoML. This is also the same as we discussed above.', 'Now create a task of multiclass classification and convert the pandas dataframe to AutoML’s dataframe.', 'Combine the prediction of above two models(LGBM pipeline and linear pipeline) with the help of a weight blender.', '\xa0\xa0\xa0 Create an autoML instance and pass above pipelines in it and train the model.', 'Lastly, predict the test data and check scores. Calculate AUC score for each class and build a report using Profile() method, as shown above.', 'You can check the full demo, here.', 'Conclusion', 'In this article, we have discussed about LightAutoML(LAMA) and its demo for', 'LAMA is an open source framework for developers, data scientists and data analytics. It can work with a dataset, where each row is an instance of data with its features and target variable. The library is under development for dealing with multi table data sets and sequences. Further, automatic creation of interpretable models can be separately done by AutoWoE library.']","'!pip install lightautoml\n! pip install albumentations==0.4.6'
 '# Standard python libraries\nimport os\nimport time\nimport logging\n \n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nimport torch\n# Imports from our package\nimport lightautoml\nfrom lightautoml.automl.base import AutoML\nfrom lightautoml.ml_algo.boost_lgbm import BoostLGBM\nfrom lightautoml.ml_algo.tuning.optuna import OptunaTuner\nfrom lightautoml.pipelines.features.lgb_pipeline import LGBSimpleFeatures\nfrom lightautoml.pipelines.ml.base import MLPipeline\nfrom lightautoml.pipelines.selection.importance_based import ImportanceCutoffSelector, ModelBasedImportanceEstimator\nfrom lightautoml.reader.base import PandasToPandasReader\nfrom lightautoml.tasks import Task\nfrom lightautoml.utils.profiler import Profiler\nfrom lightautoml.automl.blend import WeightedBlender\nfrom lightautoml.dataset.roles import DatetimeRole\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.report import ReportDeco', ""#define all the parameters\nN_THREADS = 8 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTARGET_NAME = 'TARGET' # Target column name"", ""# Change profiling decorators settings\n#By default, profiling decorators are turned off for speed and memory reduction. \n#If you want to see profiling report after using LAMA, you need to turn on the decorators using command below:\n#create a Profile object to get the profile report\np = Profiler()\np.change_deco_settings({'enabled': True})"", ""#load the dataset\n%%time\ndata = pd.read_csv('https://raw.githubusercontent.com/sberbank-ai-lab/LightAutoML/master/example_data/test_data_files/sampled_app_train.csv')\ndata.head()"", ""# (Optional Step)\n# This Cell shows some user feature preparations to create task more difficult. Some feature engineering\n \n%%time\n#creating a new columns to make the existing features more understandable \ndata['BIRTH_DATE'] = (np.datetime64('2018-01-01') + data['DAYS_BIRTH'].astype(np.dtype('timedelta64[D]'))).astype(str)\ndata['EMP_DATE'] = (np.datetime64('2018-01-01') + np.clip(data['DAYS_EMPLOYED'], None, 0).astype(np.dtype('timedelta64[D]'))\n                    ).astype(str)\n#creating three new columns\ndata['constant'] = 1\ndata['allnan'] = np.nan\ndata['report_dt'] = np.datetime64('2018-01-01')\n#drop 'DAYS_BIRTH' and 'DAYS_EMPLOYE' column from dataset\ndata.drop(['DAYS_BIRTH', 'DAYS_EMPLOYED'], axis=1, inplace=True)"", ""# (Optional Step) Data splitting for train-test\n# Block below can be omitted if you are going to train model only or you have specific train and test files:\n%%time\ntrain_data, test_data = train_test_split(data, \n                                         test_size=TEST_SIZE, \n                                         stratify=data[TARGET_NAME],                                          random_state=RANDOM_STATE)\nlogging.info('Data splitted. Parts sizes: train_data = {}, test_data = {}'\n              .format(train_data.shape, test_data.shape))"", ""# Step 1. Create Task and PandasReader\n%%time\n#We are going to do a binary classification on the given dataset\ntask = Task('binary')\n#PandasToPandasReader convert pd.DataFrame to AutoML's PandasDataset.\nreader = PandasToPandasReader(task, cv=N_FOLDS, random_state=RANDOM_STATE)"", ""# Create feature selector (if necessary)\n# We basically achieved that by creating light gbm and letting the feature importance from that to choose the best features.\n# Now, I don't know much about lgbm; but as that is not a uniquely better algorithm; \n#therefore this may serve as a bottleneck for the performance of the automl model as it depends on that\n%%time\n#create a lightGBM model with default parameters as shown below\nmodel0 = BoostLGBM(\n    default_params={'learning_rate': 0.05, 'num_leaves': 64, 'seed': 42, 'num_threads': N_THREADS}\n)\n#Creates simple pipeline for tree based models.\n#Simple but is ok for select features Numeric stay as is, Datetime transforms to numeric,\n#Categorical label encoding Maps input to output features exactly one-to-one\npipe0 = LGBSimpleFeatures()\n#Base class for performing feature selection using model feature importances.\nmbie = ModelBasedImportanceEstimator()\n#Selector based on importance threshold.\n#It is important that data which passed to .fit should be ok to fit ml_algo or preprocessing pipeline should be defined.\nselector = ImportanceCutoffSelector(pipe0, model0, mbie, cutoff=0)"", ""# Create 1st level ML pipeline for AutoML\n%%time \n#simple feature pipeline\npipe = LGBSimpleFeatures()\n#initializing OptunaTuner for hyperparameter optimization\nparams_tuner1 = OptunaTuner(n_trials=20, timeout=30) # stop after 20 iterations or after 30 seconds \n#LGBM model with OptunaTuner\nmodel1 = BoostLGBM(\n    default_params={'learning_rate': 0.05, 'num_leaves': 128, 'seed': 1, 'num_threads': N_THREADS}\n)\n#Simple LGBM model with heuristic parameter\nmodel2 = BoostLGBM(\n    default_params={'learning_rate': 0.025, 'num_leaves': 64, 'seed': 2, 'num_threads': N_THREADS}\n)\n#Created two layers for the pipeline and then add them together to create the pipe(shown below):\npipeline_lvl1 = MLPipeline([\n    (model1, params_tuner1),\n    model2\n], pre_selection=selector, features_pipeline=pipe, post_selection=None)"", ""# Create 2nd level ML pipeline for AutoML\n%%time\n#creating another simple pipeline for features\npipe1 = LGBSimpleFeatures()\n#creating another LGBM without tuning parameters\nmodel = BoostLGBM(\n    default_params={'learning_rate': 0.05, 'num_leaves': 64, 'max_bin': 1024, 'seed': 3, 'num_threads': N_THREADS},\n    freeze_defaults=True\n)\n#Merging above two pipelines into one pipeline\npipeline_lvl2 = MLPipeline([model], pre_selection=None, features_pipeline=pipe1, post_selection=None)"", '%%time \nautoml = AutoML(reader, [\n    [pipeline_lvl1],\n    [pipeline_lvl2],\n], skip_conn=False)', ""# Train AutoML on loaded data\n%%time \n#Now, fit the model on train data with target column as “TARGET” and get OOF predictions.\noof_pred = automl.fit_predict(train_data, roles={'target': TARGET_NAME})\nlogging.info('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))"", ""# Predict to test data and check scores\n# \n%%time\ntest_pred = automl.predict(test_data)\nlogging.info('Prediction for test data:\\n{}\\nShape = {}'\n              .format(test_pred, test_pred.shape))\nlogging.info('Check scores...')\nlogging.info('OOF score: {}'.format(roc_auc_score(train_data[TARGET_NAME].values, oof_pred.data[:, 0])))\nlogging.info('TEST score: {}'.format(roc_auc_score(test_data[TARGET_NAME].values, test_pred.data[:, 0])))"", ""%%time\n#create task for binary classification\ntask = Task('binary', )"", ""%%time\n#setup common rules - marking up variables to determine their place in pipeline, \n#AutoML can do this automatically for all other variables except for target variables.\nroles = {'target': TARGET_NAME,\n         DatetimeRole(base_date=True, seasonality=(), base_feats=False): 'report_dt',\n         }"", '#create an AutoML instance\n%%time \n#the only required parameter is the ""task""\nTIMEOUT = 300 # Time in seconds for automl run\nautoml = TabularAutoML(task = task, \n                       timeout = TIMEOUT,\n                       general_params = {\'nested_cv\': False, \'use_algos\': [[\'linear_l2\', \'lgb\', \'lgb_tuned\']]},\n                       reader_params = {\'cv\': N_FOLDS, \'random_state\': RANDOM_STATE},\n                       tuning_params = {\'max_tuning_iter\': 20, \'max_tuning_time\': 30},\n                       lgb_params = {\'default_params\': {\'num_threads\': N_THREADS}})\noof_pred = automl.fit_predict(train_data, roles = roles)\nlogging.info(\'oof_pred:\\n{}\\nShape = {}\'.format(oof_pred, oof_pred.shape))', ""# Create AutoML with time utilization\n# Below we are going to create specific AutoML preset for TIMEOUT utilization (try to spend it as much as possible):\n%%time \n \nautoml = TabularUtilizedAutoML(task = task, \n                       timeout = TIMEOUT,\n                       general_params = {'nested_cv': False, 'use_algos': [['linear_l2', 'lgb', 'lgb_tuned']]},\n                       reader_params = {'cv': N_FOLDS, 'random_state': RANDOM_STATE},\n                       tuning_params = {'max_tuning_iter': 20, 'max_tuning_time': 30},\n                       lgb_params = {'default_params': {'num_threads': N_THREADS}})\noof_pred = automl.fit_predict(train_data, roles = roles)\nlogging.info('oof_pred:\\n{}\\nShape = {}'.format(oof_pred, oof_pred.shape))"", '#create fake multiclass target\ndata[TARGET_NAME] = np.where(np.random.rand(data.shape[0]) > .5, 2, data[TARGET_NAME].values)\ndata[TARGET_NAME].value_counts()', '# Create Timer for pipeline\n# Here we are going to use strict timer for AutoML pipeline, which helps not to go outside the limit:\n%%time\ntimer = PipelineTimer(600, mode=2)', ""# Create feature selector\n%%time\ntimer_gbm = timer.get_task_timer('gbm') # Get task timer from pipeline timer \n#creating simple feature pipeline\nfeat_sel_0 = LGBSimpleFeatures()\n#creating a model instance with timer\nmod_sel_0 = BoostLGBM(timer=timer_gbm)\n#initailzing the feature importance estimator\nimp_sel_0 = ModelBasedImportanceEstimator()\n#selecting important feature on the basis on threshold\nselector_0 = ImportanceCutoffSelector(feat_sel_0, mod_sel_0, imp_sel_0, cutoff=0, )"", ""##LGBAdvancedPipeline creates advanced pipeline for trees based models.\n# Includes:\n#         different cats and numbers handling according to role params.\n#         dates handling - extracting seasons and create datediffs.\n#         create categorical intersections.\n \n%%time \nfeats_gbm_0 = LGBAdvancedPipeline(top_intersections=4, \n                                  output_categories=True, \n                                  feats_imp=imp_sel_0)\ntimer_gbm_0 = timer.get_task_timer('gbm')\ntimer_gbm_1 = timer.get_task_timer('gbm')\n#creating two LGBM model, one with Optune Tuner and another one simple\ngbm_0 = BoostLGBM(timer=timer_gbm_0)\ngbm_1 = BoostLGBM(timer=timer_gbm_1)\n \ntuner_0 = OptunaTuner(n_trials=20, timeout=30, fit_on_holdout=True)\ngbm_lvl0 = MLPipeline([\n        (gbm_0, tuner_0),\n        gbm_1\n    ],\n    pre_selection=selector_0,\n    features_pipeline=feats_gbm_0, \n    post_selection=None\n)"", ""#LinearFeatures creates pipeline for linear models and nnets.\n#https://lightautoml.readthedocs.io/en/latest/pythonapi/lightautoml.pipelines.features.linear_pipeline.html?highlight=LinearFeatures#lightautoml.pipelines.features.linear_pipeline.LinearFeatures\nfeats_reg_0 = LinearFeatures(output_categories=True, \n                             sparse_ohe='auto')\n \ntimer_reg = timer.get_task_timer('reg')\n#LBFGS L2 regression based on torch\nreg_0 = LinearLBFGS(timer=timer_reg)\n#Adding above two pipelines into one\nreg_lvl0 = MLPipeline([\n        reg_0\n    ],\n    pre_selection=None,\n    features_pipeline=feats_reg_0, \n    post_selection=None\n)"", ""#  Create multiclass task and reader\n%%time \ntask = Task('multiclass', metric = 'crossentropy', ) \nreader = PandasToPandasReader(task = task, samples = None, max_nan_rate = 1, max_constant_rate = 1,\n                              advanced_roles = True, drop_score_co = -1, n_jobs = 4)"", '# Create blender for 2nd level\n# To combine predictions from different models into one vector we use WeightedBlender:\n%%time\nblender = WeightedBlender()', '# Create AutoML pipeline\n%%time\nautoml = AutoML(reader=reader, levels=[\n    [gbm_lvl0, reg_lvl0]\n], timer=timer, blender=blender, skip_conn=False)'"
170,wandb_with_python_implementation/,https://analyticsindiamag.com/hands-on-guide-to-weights-and-biases-wandb-with-python-implementation/,"['Everything in Data Science begins with the given data to experiment with and a big amount of time is usually spent on data modeling; tracking all the results and visualizing all the data for every run. Sometimes, this whole process can be a tough grind. Training a model, especially deep learning models is a tedious task. Larger the size of the training model, the more time it will take to run. This causes hindrance in training as experiments with different architectures and hyperparameters can be aggravating when a single run takes several hours or days to complete.', 'Most experiments which include aggressive training are not even published, and researchers waste resources running the same experiments over and over. Fortunately, plenty of tools and platforms have been developed recently to track the real-time performance of models for different executions. One of such tools is Weights and Biases(Wandb). Wandb organize your and analyze your machine learning experiments. It is lighter than a tensorboard toolkit. With a few lines of code, wandb saves your model’s hyperparameters and output metrics and gives you all visual charts like for training, comparison of model, accuracy, etc. It automatically tracks the state of your code, system metrics and configuration parameters.', 'Wandb is open source and free for academic research. It supports all of the most common graphs and visualizations and it also provides an API that allows users to extract any information saved during run time.This platform provides a service called Benchmarks that allow people to share their implementation for a specific task. This helps people who are new to a specific task, as this toolkit already saves what all approaches have been done earlier on it and provides the implementation along with its performance scores. It provides many tools for logging such as:', 'It also provides python API. Its Python API supports many of the popular frameworks like jupyter, pytorch, keras, tensorflow, etc.', 'Many of the infrastructure supported by it are:', 'The key features of this platforms are:', 'Let’s get started with the implementation part', 'Requirements', 'It will ask you to provide an API from your wandb profile, click on the link and copy the API and paste it here.', 'Model Training', 'Let’s take an example of image classification. You can take any dataset you want. In this session, we will be using a simple\xa0 Multilayer Perceptron(MLP) model to classify the images of MNIST dataset. We will be using the Pytorch framework along with wandb. Since our main focus is Wandb so the explanation of that particular code is given below, you can check out the MLP model code here. Just before training the model we have to integrate the training model in wandb.\xa0', 'Just before the training, define wandb.init(). It will start a process that syncs metrics in real time. In this particular function, you can give your name, project name and attach notes/tags, if any. Then after creating an object of the model, we have to put that model to watch function so that wandb can log the network. It can be done with the help of wandb.watch(model).\xa0', 'For the next step, we will start training and validating the model normally. After calculating these four values: loss value of training data(loss_train), accuracy of training data(acc_train), loss value of valid data(loss_valid), accuracy value of valid data(acc_valid), we will pass them into wandb.log() to log a dictionary of metrics or custom objects to a step.', 'With the completion of training process, the output will provide a link to wandb interface where all your saved metrics have been converted into interactive graph. The graphs below are the models logs, you can check the interactive version of these graphs, here. The model logs will tell you how the metrics of our model changed from epoch to epoch.', 'And similarly, it will also provide system logs which provide the information like GPU consumption, CPU utilization,etc. An example of it is shown below. You can check out the interactive version, here.', 'Apart from that, it also saves Model information of our neural network. You can check the demo here.', 'A very useful section is Logs, which shows all the shell output during the training process. It is incredibly useful to check for warnings and errors even if we don’t have access to the terminal anymore.', 'And finally the Files section which will contain all the pretrained models.', 'Now if we replace the MLP model with a simple Logistic Regression model and fit this model to MNIST data set again, then this time wandb gives the comparison charts(grouped charts) of both models applied on the same dataset. Link for the code snippet is here. An example of it shown below. Interactive version of it is available here.', 'Hyperparameter search through Wandb', 'Searching for correct hyperparameters in high dimensional space can be tricky sometimes. Hyperparameter Sweep provides an efficient way to do this with just a few lines of code. They enable this by automatically searching through combinations of hyperparameter values (e.g. learning rate, batch size, number of hidden layers, optimizer type) to find the most optimal values. In this section, we will see a hyperparameter sweep tutorial.\xa0', 'For this example, we will be taking the MLP model and MNIST dataset again(discussed above). For hyperparameter sweep, we will define a dictionary called sweep_config containing all the hyperparameters(learning_rate, batch_size, etc) for the given model.', 'Now, initialize the sweep.', 'Now, create a function built_dataset() and add “batch_size” as its parameter. This function will download the MNIST data, transform it into numbers and then divide into the required batch sizes. The code snippet is available here. Now we will create another function train() which will be responsible for fitting the model to the data for all the combinations of hyperparameters and will integrate all the configuration to wandb. For that, we will initialize wandb.init() and will pass the default configuration. Rest of the procedure for training the model is the same as described earlier.', 'Now, just run the sweep agent. This process may take time as wandb will check for different hyperparameters.', 'At this link, you can check all the interactive graphs created by wandb. Some of them are discussed below.', 'Parallel Coordinates Plot : This plot maps hyperparameter values to model metrics. It provides us the quick solution for checking the combination of hyperparameters so as to give the best model performance.', 'Hyperparameter Importance Plot:The hyperparameter importance plot surfaces which hyperparameters were the best predictors of, and highly correlated to desirable values for your metrics.', 'Conclusion', 'In this session, we have covered all the basics of Weights and Bias(Wandb). We have seen python implementation of using wandb API in our existing codes. We have also discussed group charts (different models applied on the same dataset). Lastly, we have seen the hyperparameter search through sweep. Some of the advanced topic that wandb covers are:', 'These visualizations can help you save both time and resources and are thereby worthy of further exploration.']","'!pip install wandb'
 'import wandb\n!wandb login', '#selecting the device \ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\nprint(device)\n# Initialize WandB \nwandb.init(name=\'MLP method\', \n           project=\'introductory_project\',\n           notes=\'This is a introductory project\', \n           tags=[\'MNIST dataset\', \'Test Run\'])\n\n# specify loss function (categorical cross-entropy)\nerror = nn.CrossEntropyLoss()\n#initializing the model\nmodel = MLP().to(device)\n# specify optimizer \noptimizer = optim.Adam(model.parameters())\n\n\n# Log the network weight histograms (optional)\nwandb.watch(model)\n\nnum_epochs = 10\nstart_time = time.time()\nfor epoch in range(1, num_epochs+1):\n    #calculation is done with the help of helper functions\n    loss_train, acc_train = train(model, error, optimizer, train_loader)\n    loss_valid, acc_valid = validate(model, error, valid_loader)\n    \n    \n    print(f\'\\tTrain Loss: {loss_train:.3f} | Train Acc: {acc_train*100:.2f}%\')\n    print(f\'\\t Val. Loss: {loss_valid:.3f} |  Val. Acc: {acc_valid*100:.2f}%\')\n\n    # Log the loss and accuracy values at the end of each epoch\n    wandb.log({\n        ""Epoch"": epoch,\n        ""Train Loss"": loss_train,\n        ""Train Acc"": acc_train,\n        ""Valid Loss"": loss_valid,\n        ""Valid Acc"": acc_valid})\n\nprint(""Time Elapsed : {:.4f}s"".format(time.time() - start_time))', ""#define a sweep dictionary containing all the hyperparameters\nsweep_config = {\n    'method': 'random', #grid, random\n    'metric': {\n      'name': 'loss',\n      'goal': 'minimize'   \n    },\n    'parameters': {\n        'epochs': {\n            'values': [2, 5, 10, 15]\n        },\n        'batch_size': {\n            'values': [256, 128, 64, 32]\n        },\n        'learning_rate': {\n            'values': [1e-2, 1e-3, 1e-4, 3e-4, 3e-5, 1e-5]\n        },\n        'fc_layer_size':{\n            'values':[128,256,512]\n        },\n        'optimizer': {\n            'values': ['adam', 'sgd']\n        },\n    }\n}"", '#Now initialize the sweep \nsweep_id = wandb.sweep(sweep_config, project=""sweep_introduction"")', 'def train():\n    # Default values for hyper-parameters we\'re going to sweep over\n    config_defaults = {\n        \'epochs\': 5,\n        \'batch_size\': 128,\n        \'learning_rate\': 1e-3,\n        \'optimizer\': \'adam\',\n        \'fc_layer_size\': 128,\n        \'dropout\': 0.5,\n    }\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    # Initialize a new wandb run\n    wandb.init(config=config_defaults)\n    \n    # Config is a variable that holds and saves hyperparameters and inputs\n    config = wandb.config\n    \n    # Define the model architecture\n    network = nn.Sequential(\n     nn.Flatten(start_dim=1)\n    ,nn.Linear(784, config.fc_layer_size)\n    ,nn.ReLU()\n    ,nn.Linear(config.fc_layer_size, config.fc_layer_size)\n    ,nn.ReLU()\n    ,nn.Linear(config.fc_layer_size, 10)\n     )    \n    #building dataset with the given batch_size\n    train_loader = build_dataset(config.batch_size)\n    # Define the optimizer\n    if config.optimizer==\'sgd\':\n      optimizer = optim.SGD(network.parameters(),lr=config.learning_rate, momentum=0.9)\n    elif config.optimizer==\'adam\':\n      optimizer = optim.Adam(network.parameters(),lr=config.learning_rate)\n\n    network.train()\n    network = network.to(device)\n    for i in range(config.epochs):\n      closs= 0\n      for batch_idx, (data, target) in enumerate(train_loader):\n          data, target = data.to(device), target.to(device)\n          optimizer.zero_grad()\n          output = network(data)\n          loss = F.nll_loss(output, target)\n          loss.backward()\n          closs = closs + loss.item()\n          optimizer.step()\n          wandb.log({""batch loss"":loss.item()})\n      wandb.log({""loss"":closs/config.batch_size}) ', '#run the sweep agent\nwandb.agent(sweep_id, train)'"
171,traditional_machine_learning_models/,https://analyticsindiamag.com/guide-to-hummingbird-a-microsofts-library-for-expediting-traditional-machine-learning-models/,"['Conventional Machine Learning algorithms such as Linear Regression, Logistic Regression and Decision Tree are extensively used for a variety of real-world applications. For an easy-to-handle implementation of these algorithms, there are several ML libraries and toolkits available such as scikit-learn, h2o, ML.NET etc. However, they can run only on CPU environments.\xa0', 'Though the performance of above mentioned traditional ML frameworks can be significantly improved using multi-core parallel processing, they cannot take advantage of hardware acceleration. On the contrary, the recent advancements in the field of Deep Learning has made it possible to use hardware accelerators such as GPUs and TPUs for implementing neural networks. With an aim of enabling the traditional ML libraries to take advantage of hardware acceleration and optimizations implemented for the neural networks without restructuring the model, Microsoft launched a library named Hummingbird.', 'Widely used Deep Learning frameworks such as PyTorch, ONNX Runtime and TensorFlow use a single abstraction called tensors as the basic unit of any computation. Due to lack of any such common abstraction, traditional ML libraries require (m*n) number of implementations to achieve hardware acceleration (where m and n denote the number of operators in the computation and the number of hardware backends available respectively). This makes the libraries computationally more expensive to use than the optimized neural networks. Hummingbird library resolves this issue by converting the conventional ML pipelines into tensor-oriented computations. The traditional ML models can then be deployed faster in real-time.', 'Its currently available version can convert the ML models to PyTorch, ONNX, TVM and TorchScript but not to Keras, an extensively used open-source Deep Learning library built on top of TensorFlow.', 'The following code has been implemented using GPU and Python 3.6 in Google Colab notebook. The dataset winequality_red used in the code is available on Kaggle. The classification task is to label the wine quality for each instance as good or bad depending upon whether it is above 6.5 or not respectively.', 'Hummingbird can be installed using pip command as:', 'pip install humming-ml', ""data=pd.read_csv('winequality-red.csv')"", 'x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.75,random_state=42)', 'model=RandomForestClassifier(n_estimators=300)', 'model.fit(x_train,y_train)', '%%time', 'y_pred=model.predict(np.array(x_test))', 'The output noted after implementing the adobe lines of code is as follows:', 'CPU times : user 60.6 ms, sys : 23 s, total : 60.7 ms', 'Wall time : 64.2 ms', ""model_torch=convert(model,'pytorch')"", ""model_torch.to('cuda')"", 'y_pred_torch=model_torch.predict(np.array(x_test))', 'After using the Hummingbird library, the output noted is as follows:', 'CPU times : user 9.41 ms, sys : 944 s, total : 10.4 ms', 'Wall time : 14.8 ms', 'The execution time reduced from 60.7 ms before using the Hummingbird library to 10.4 ms after using it. The conversion of computations into tensors done by Hummingbird noticeably reduced the execution time. Thus the Humming library can hardware accelerate the conventional ML models and make them capable of giving faster results at the pace of neural network systems.', 'Microsoft’s work is in progress for extending the functionalities of the Hummingbird library so that it can support more ML models, neural network backends and operators. The scope of this useful library is thus expected to widen in near future.', 'Refer to the following sources for detailed information of the library and Jupyter Notebook for the above explained code.']","'pip install humming-ml'
 ""data=pd.read_csv('winequality-red.csv')"", 'x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.75,random_state=42)', 'model=RandomForestClassifier(n_estimators=300)', 'model.fit(x_train,y_train)', '%%time', 'y_pred=model.predict(np.array(x_test))', ""model_torch=convert(model,'pytorch')"", ""model_torch.to('cuda')"""
172,analysis_and_ml_automation/,https://analyticsindiamag.com/lets-learn-dabl-a-python-tool-for-data-analysis-and-ml-automation/,"['Prior to training a machine learning or deep learning model, it is important to cleanse, pre-process and analyse the dataset at hand. Processes like dealing with missing values, converting text data into numbers and so on are all part of the pre-processing phase. More often than not, these processes come across as being repetitive and monotonous. Although there are tools for automating this process, they behave like a black box and do not give intuition about how they changed the data. To overcome this problem, python introduced a library called dabl – Data Analysis Baseline library. Dabl can be used to automate many of the tasks that seem repetitive in the early stages of model development. This was developed quite recently and the latest version of Dabl was released earlier this year. The number of available features currently are less, but the development process is happening at a good pace at Dabl.\xa0', 'In this article, we will use this tool for data pre-processing, visualisation and analysis as well as model development. Let’s get started.\xa0', 'To use dabl to perform data analysis we need to first install the package. You can install this using the pip command as\xa0', 'pip install dabl', 'Once the installation is done, let us go ahead and pick a dataset. I will select a sample dataset from Kaggle. You can click this link to download the data. I have chosen the diabetes dataset. It is a small dataset which will make it easy to understand how dabl works.\xa0', 'After downloading the dataset, let us import the important libraries and look at our dataset.\xa0', 'Usually, after looking at the dataset you would get into the data cleaning process by trying to identify missing rows, identify the erroneous data and understand the datatypes of the columns.\xa0 These processes are made easy using dabl by automating these.\xa0', 'db_clean = dabl.clean(db_data, verbose=1)', 'We have a list of detected feature types for the dataset given. These types indicate the following.', 'Continuous: This is the number of columns containing continuous values and columns with high cardinality.\xa0', 'Dirty_float: Float variables that sometimes take string values are called dirty_float.\xa0', 'Low_card_int: Columns that contain integers with low cardinality fall under this category.\xa0', 'Categorical: This is the number of columns containing pandas categorical values in a string, integer or floating-point formats.\xa0', 'Date: Columns with data in them. These are currently not handled by dabl.', 'free_string: string data types which contain multiple unique values are labelled as free_string.\xa0', 'Useless: Constant or integer values that do not match with any of the categories are given a name useless.\xa0', 'For more information about the feature types it has identified you can do the following step.\xa0', 'type_info = dabl.detect_types(db_clean)', 'type_info', 'Here, we can clearly see which column of the dataset is of which data type. We can also change the type to meet our needs and requirements. For example, the column named Pregnancies is labelled neither as continuous nor as categorical and since the values in the column are single integer values we can make them into categorical values.', 'db_clean = dabl.clean(db_data, type_hints={""Pregnancies"": ""categorical""})', 'We have successfully converted the column into a categorical one.\xa0', 'The next part before training of the model is to visualise the data. Using visualisation tools like matplotlib or seaborn is effective but dabl makes the process very simple and displays a wide range of plots for a single line of code. As dabl detects feature types and automatically cleans the data this makes analysing the data extremely fast.', 'Using the plot() method you can plot all your features against the target. In our dataset, the column Outcome is the target.\xa0', ""dabl.plot(db_clean, 'Outcome')"", 'Dabl first automatically identifies and drops any outliers present in the dataset. It then identifies what type of data is present in the target (whether is categorical or continuous) and then displays the appropriate graph. Since ours is a categorical target, the output is a bar graph containing the count of 0s and 1s. Dabl also calculates and displays Linear discriminant analysis scores for the training set.\xa0', 'The next graph is to identify the distribution of each feature against our target. As you can see below, each feature is plotted as a histogram against our target and the number of features that lead to 1 and 0 are shown in orange and blue respectively.\xa0', 'The next graph is the scatter plot of the different combinations of data that exists in the dataset. For example, the feature glucose will be plotted against all the other columns and the distribution is shown below.\xa0', 'In order to increase the efficiency and speed of training, dabl automatically performs PCA on the dataset and also shows the distribution to us. The next graph is the discriminant PCA graph for all the features in the dataset. It also displays the variance and cumulative variance for the dataset.\xa0', 'The final graph displayed here is the linear discriminant analysis which is done by combining the features against the target.\xa0', 'It is clear that in a single line of code we are able to analyse the data in different ways that would usually be done in multiple steps and with code redundancy. But dabl is not repetitive and is an automated way to make data visualisation easy and simple to use.\xa0', 'Dabl intends to speed up the process of model training and provides a low code environment to train models. It takes up very little time and memory to train models using dabl. But, as mentioned earlier, this is a recently developed library and provides basic methods for machine learning training. Here I will be using a simple classifier model to train the diabetes dataset.\xa0', 'The simple classifier method performs training on the most commonly used classification models and produces accuracy, precision, recall and roc_auc for the data.\xa0', 'Not only this, but it also identifies the best model giving the best results on your dataset and displays it.\xa0', 'Similar to classification, you can also use a simple regressor model for regression type of problem.\xa0']","'pip install dabl'
 'db_clean = dabl.clean(db_data, verbose=1)', 'type_info = dabl.detect_types(db_clean)', 'type_info', 'db_clean = dabl.clean(db_data, type_hints={""Pregnancies"": ""categorical""})', ""dabl.plot(db_clean, 'Outcome')"""
173,experiments_with_comet_ml/,https://analyticsindiamag.com/how-to-supercharge-your-machine-learning-experiments-with-comet-ml/,"['Comet.ml is a Machine Learning experimentation platform which AI researchers and data scientists use to track, compare and explain their ML experiments. It allows ML practitioners to keep track of their databases, history of performed experiments, code modifications and production models. It enables model reproduction, easy maintenance of ML workflow and smooth collaboration throughout the iterative process of ML lifecycle. It also performs model optimization using Bayesian hyperparameter optimization algorithm and thereby reduces the overhead of tuning your model manually.\xa0', 'Comet was introduced by a private organization named Comet ML Inc. (also known as CometML). It was founded by Gideon Mendels in 2017 and is headquartered at New York (USA). Some of the leading companies leveraging the Comet platform are: Google, Ancestry, Cern, Uber and Boeing.', 'Following are the SDKs and APIs featured by Comet:SDKs and APIs:', 'Comet organizes your code runs based on three major concepts:', 'A workspace contains projects each os which is a collection of experiments.Let’s have an overview of each of them.', 'On creating an account, Comet provides you with a default workspace which comprises your private and public projects. You can create your own workspaces other than the default one. Each workspace has its own set of collaborators (contributors working jointly on a project). Projects in a workspace are automatically shared with all the collaborators in that public. Enabling ‘public’ sharing, you can even share your projects with the outside world.', 'A project is a set of ML experiments. Each project is categorized into one of the following two types:', 'A project is divided into 5 sections as follows:', 'An experiment is a unit of measurable research which represents a single run of your code. Each experiment is given a random ID by default which you can change to make it human-readable. A totally customizable tabular form of an experiment is known as an ‘Experiment Table’. An experiment has a set of ‘Experiment Tabs’ such as Charts Tab, HTML Tab, Hyperparameters Tab, Metrics Tab and so on, each having a peculiar functionality. For instance, Hyperparameters Tab and Metrics Tab store the ML model’s hyperparametes and evaluation metrics logged during your experiment.', 'Within a project, you have a ‘Project Visualizations’ section which enables viewing and comparing performance across different experiments. Besides, ‘Query Builder’ section of a project allows you to choose which experiments should be shown in the Project Visualizations and the Experiment Table.', 'Visit this page to know about workspaces, projects and experiments in detail along with the ways to handle the functionalities provided by each of them.', 'Comet facilitates Automatic Logging for several extensively used Python ML frameworks some of which are listed below. Click on the corresponding link to get a quick tutorial on how to incorporate Comet while implementing that framework.', 'keras, lightgbm, Uber’s ludwig, matplotlib, mlflow, pyspark, pytorch, pytorch-lightning, scikit-learn, shap, tensorflow, tensorflow model analysis, HuggingFace’s transformers.', 'Want to reduce the time it takes to train your neural networks? Try parallelized neural network training! This lets you evaluate multiple hyperparameter options faster. Learn how to implement Parallelized Training in our new report: https://t.co/GypH2aMHRr#NeuralNetworks', 'Working on object detection? You can use Comet to create custom vizzes that make it MUCH easier to debug your models. Add bounding boxes with your specifications easily. Read more: https://t.co/xvd1LI3R7B#ObjectDetection #ComputerVision #MachineLearning pic.twitter.com/wgIakXF7Rf', 'comet_ml can be installed using pip command as follows:', 'pip install comet_ml', 'Import the required libraries', 'Create an experiment with your API key\xa0', 'Click here to learn about the properties and methods associated with an Experiment object.', 'Initialize model’s parameters', 'Get to know about the Adam optimizer and ReLU activation function.', 'Load the MNIST digit classification dataset using load_data() function and split it into training set and test set', '(x_train, y_train), (x_test, y_test) = mnist.load_data()', 'Reshape the 1D train and test set', 'Convert the data types to real', 'Perform normalization', 'Print the number of samples in training set and test set', 'Convert class vectors to binary class matrices', 'Define a dictionary for parameters to be logged', 'Instantiate the sequential model', 'model = Sequential()', 'Add dense layers in the network', 'print(model.summary())', 'Compile the model', 'Log metrics with the prefix ‘train_’', 'Experiment.train() marks beginning and end of the train phase. It provides a namespace for logging the training parameters.', 'Log metrics with the prefix ‘test_’', 'Experiment.test() marks beginning and end of the test phase. It provides a namespace for logging the testing metrics.', 'Log the metrics as key:value pairs in a dictionary\xa0\xa0', 'experiment.log_metrics(metrics)', 'Log a dictionary-like object of parameters', 'experiment.log_parameters(params)', 'Create and log a hash of your data', 'experiment.log_dataset_hash(x_train)\xa0', 'Source: https://www.comet.ml/docs/python-sdk/keras', 'Import the required libraries', 'Define the hyperparameters\xa0', 'Instantiate an Experiment object', 'experiment = Experiment(project_name=""my_project"")', 'Log the hyperparameters', 'experiment.log_parameters(hyper_params)', 'Load the MNIST dataset', 'Define the model training and testing pipelines using DataLoader utility', 'Define many-to-one RNN model', 'Forward propagation', 'Call RNN()', 'Define loss function and optimizer', 'Get to know about Cross Entropy Loss.', 'Train the Model', 'Log epoch accuracy to Comet.ml; step is each epoch ', 'Test the model', '\xa0\xa0\xa0\xa0Log accuracy', '\xa0\xa0\xa0\xa0experiment.log_metric(""accuracy"", correct / total)', '\xa0\xa0\xa0\xa0Print the logged accuracy', ""\xa0\xa0\xa0\xa0print('Test Accuracy: %d %%' % (100 * correct / total))"", 'Source: https://www.comet.ml/docs/python-sdk/pytorch']","'pip install comet_ml'
 '(x_train, y_train), (x_test, y_test) = mnist.load_data()', 'model = Sequential()', 'print(model.summary())', 'experiment.log_metrics(metrics)', 'experiment.log_parameters(params)', 'experiment.log_dataset_hash(x_train)\xa0', 'experiment = Experiment(project_name=""my_project"")', 'experiment.log_parameters(hyper_params)', 'experiment.log_metric(""accuracy"", correct / total)', ""print('Test Accuracy: %d %%' % (100 * correct / total))"""
174,a_standalone_web_application/,https://analyticsindiamag.com/complete-guide-to-voila-to-turn-a-jupyter-notebook-into-a-standalone-web-application/,"['Voila, an open-source python library that is used to turn the jupyter notebook into a standalone web application It supports widgets to create interactive dashboards, reports, etc. Voila launches a kernel when it is connected to a notebook and executes all the cells but it does not stop the kernel there so that the user can interact with the output.', 'Voila converts the jupyter notebook into HTML and returns it to the user as a dashboard or report with all the inputs excluded and the outputs included. Voila supports all the python libraries for widgets such as bqplot, plotly, ipywidgets, etc.', 'Voila is framework and language agnostic which means that voila can work with any jupyter kernel whether it is C++ or Python, etc. because it is built on jupyter standard protocols.\xa0', 'Voila is useful in many ways because of its extensibility and usability, it can serve as a solution for Data Science or Business Analyst professionals to share their work which is relevant for the end-user or client. In this article, we will explore how easily and effortlessly we can use Voila to create reports and dashboards.\xa0', 'Like any other library, we will install Voila using pip install voila. After installing voila we need to open the jupyter notebook to check that a new tab named Voila is added in the toolbar.', 'As Voila supports Plotly so we will be using plotly in this article, other than that we will be using pandas for loading the data.', 'import plotly.express as px', 'import plotly.graph_objects as go', 'import pandas as pd', 'We will be using stock data of a firm which can be downloaded from any financial website like Yahoo Financial, I have downloaded data for ‘Biocon Pharmaceuticals’ and stored it in a CSV file. The data contains Date, Opening Price, Closing price, etc.', 'df = pd.read_csv(‘biocon.csv’)', 'df.head()', '\xa0\xa0\xa0\xa0\xa0c. Adding Some Information regarding Stock', 'We will add some information regarding the company whose dataset we are working on i.e. Biocon Pharmaceuticals so that our dashboard/application will be informative.\xa0', 'info = ”’Biocon Limited is a globally recognized, innovation-led organization that is enabling access to high quality, advanced therapies for diseases that are chronic, where medical needs are largely unmet and treatment costs are high. We are driven by the belief that the pharmaceutical industry has a humanitarian responsibility to provide essential drugs to patients who are in need and to do so with the power of innovation. In line with this belief, Biocon has developed and commercialized a differentiated portfolio of novel biologics, biosimilars, and complex small molecule APIs in India and several key global markets, as well as, generic formulations in the U.S. and Europe. We are a leading global player for biosimilars and APIs for statins, immunosuppressants and other speciality molecules, with customers in over 120 countries.”’', 'print(info)', '\xa0\xa0\xa0\xa0\xa0d. Using Plotly to create Charts', 'We will create different plots that are used to analyze the stock market data. We will use the markdowns for giving the name of the charts so that it reflex in the dashboard/application.', ""fig1 = go.Figure(data=[go.Candlestick(x=df['Date'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0open=df['Open'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0high=df['High'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0low=df['Low'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0close=df['Close'])])"", 'fig1.show()', ""fig2 = px.line(df, x='Date', y='High')"", 'fig2.show()', ""fig3 = go.Figure(data=go.Ohlc(x=df['Date'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0open=df['Open'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0high=df['High'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0low=df['Low'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0close=df['Close']))"", 'fig3.show()', ""fig4 = px.area(df, x=df['Date'], y=df['High'])"", 'fig4.show()', 'Converting Notebook to Web Application', 'Now we will use Voila to create the dashboard/application in just a single click. As seen above now we will click the Voila tab in the toolbar section to launch voila. As soon as we click Voila it will launch voila in a new window and start executing the commands one by one.', 'After Execution, the Dashboard/Application is created which will display all the outputs without the code. I have added markdowns for different sections as you will see below in the images of the Voila Dashboard/Application.', 'In the above images, we saw how Voila rendered our jupyter notebook and converted it into a dashboard/application. All the graphs are created using plotly so these graphs are highly interactive and downloadable. We can also launch voila using the command prompt by using Voila <filename.ipynb>', 'Similarly, we can create a different application using different Python libraries which are used to control data using widgets.\xa0\xa0', 'Conclusion:\xa0\xa0']","'import plotly.express as px'
 'import plotly.graph_objects as go', 'import pandas as pd', 'df = pd.read_csv(‘biocon.csv’)', 'df.head()', 'print(info)', ""fig1 = go.Figure(data=[go.Candlestick(x=df['Date'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0open=df['Open'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0high=df['High'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0low=df['Low'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0close=df['Close'])])"", 'fig1.show()', ""fig2 = px.line(df, x='Date', y='High')"", 'fig2.show()', ""fig3 = go.Figure(data=go.Ohlc(x=df['Date'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0open=df['Open'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0high=df['High'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0low=df['Low'],"", ""\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0close=df['Close']))"", 'fig3.show()', ""fig4 = px.area(df, x=df['Date'], y=df['High'])"", 'fig4.show()'"
175,analysis_using_py_feat/,https://analyticsindiamag.com/a-guide-to-facial-expression-analysis-using-py-feat/,"['Facial expression analysis is the act of automatically detecting, collecting, and analyzing facial muscle movement and changes that reflect certain human mental states and situations. We will talk about this technique in this article, as well as a Python-based utility called Py-FEAT, which helps with identifying, preprocessing, analyzing, and visualizing facial expression data. Below are the major points that we are going to discuss in this article.', 'Let’sLet’s first understand facial expression analysis.', 'A facial expression is made up of one or more motions or postures of the muscles beneath the skin of the face. These motions, according to one set of controversial ideas, communicate an individual’s emotional state to observers. Facial expressions are an example of nonverbal communication. They are the most prevalent means by which humans exchange social information, but they are also found in most other mammals and certain other species.', 'Facial expressions can disclose information about an individual’s inner mental state and provide nonverbal channels for interpersonal and cross-species communication. Getting a consensus on how to effectively portray and measure facial expressions has been one of the most difficult aspects of researching them. The Facial Affect Coding System (FACS) is one of the most widely used techniques for accurately measuring the intensity of groupings of facial muscles known as action units (AUs).\xa0', 'Automated methods based on computer vision techniques have been developed as a potential tool for extracting representations of face emotions from images, videos, and depth cameras both within and outside the laboratory. Participants can be freed from burdensome wires and engage in tasks such as watching a movie or conversing casually.\xa0', 'Aside from AUs, computer vision approaches have introduced alternate spaces for representing facial expressions, such as facial landmarks or lower dimensional latent representations. These techniques can predict the intensity of emotions and other affective states such as pain, discern between genuine and fake expressions, detect signs of depression, infer qualities such as personality or political orientations, and anticipate the development of interpersonal relationships.', 'The Python Facial Expression Analysis Toolbox (Py-Feat), free and open-source software for analyzing facial expression data. It, like OpenFace, provides tools for extracting facial features, but it also includes modules for preprocessing, analyzing, and displaying facial expression data (see the pipeline in Figure below). Py-Feat is intended to cater to unique sorts of users. Py-Feat assists computer vision researchers by allowing them to communicate their cutting-edge models to a wider audience and quickly compare their models to others.\xa0', 'Face analysis begins with the capture of face photographs or videos using a recording device such as webcams, camcorders, head-mounted cameras, or 360 cameras, as seen above. After recording the face, Py-Feat is used to detect facial attributes such as facial landmarks, action units, and emotions, and the results can be compared using picture overlays and bar graphs.\xa0', 'Additional features can be extracted from the detection data, such as Histogram of Oriented Gradients or multi-wavelet decomposition. The data can then be evaluated using statistical methods such as t-tests, regressions, and intersubject correlations inside the toolbox.\xa0', 'Face images can be generated from models of action unit activations using visualization tools that display vector fields indicating landmark movements and heatmaps of facial muscle activations.', 'Py-Feat offers a Detector module for detecting facial expression features (such as faces, facial landmarks, AU activations, and emotional expressions) in face pictures and videos, as well as a Fex data class with methods for preprocessing, analyzing, and visualizing facial expression data. In the following section, we’ll look at how we can get face expression details for some of the movie scenes.', 'Using the Detector class, we’ll try to detect emotions from various movie scenes in this section. This class takes models for\xa0', 'These models are modular in nature, allowing users to choose which algorithms to apply for each detection task based on their accuracy and speed requirements. Now let’s get started by installing and importing dependencies.', 'Define the detector class as shown below,', 'Now load the image,', 'Now we can initialize the detector class by its method for image-based inference by detect_image()\xa0', '# get prediction', 'image_prediction = detector.detect_image(test_image)', 'Now by this, we can access the various action units that the model has detected and also emotions that are being inferred by the detector.\xa0', 'In a similar way, the method not only does the inference for the single but also multiple from image and video files. Examples are included in the notebook.\xa0']","'!pip install py-feat\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n \nfrom feat.tests.utils import get_test_data_path\nfrom feat import Detector\n'
 '# define the models\nface_model = ""retinaface""\nlandmark_model = ""mobilenet""\nau_model = ""rf""\nemotion_model = ""resmasknet""\ndetector = Detector(face_model = face_model, landmark_model = landmark_model, au_model = au_model, emotion_model = emotion_model)\n', '# load and visualize the image\ntest_image = os.path.join(\'/content/\', ""home_alone4.jpg"")\nf, ax = plt.subplots()\nim = Image.open(test_image)\nax.imshow(im)\n'"
176,vision_tasks_using_visionkg/,https://analyticsindiamag.com/how-to-query-data-for-computer-vision-tasks-using-visionkg/,"['We mostly know SQL as a query language that facilitates us to query relational data from almost any database, but when it comes to gathering data for computer vision-related tasks, we have to depend on a distinct host of those data sets such as ImageNet, COCO, etc. Recently researchers have proposed a framework called VisionKG, which can integrate those datasets seamlessly. So in this article, we will discuss the VisionKG in detail and will see how it can query the dataset like COCO and ImageNet. Below are the major points to be discussed in this article.', 'Let’s first discuss what this framework brings out.\xa0', 'It is a unified dataset framework in data-centric AI. Not only are diverse datasets from one AI domain integrated and linked together within this framework, but so are datasets from multiple AI areas.\xa0', 'Existing resources, such as ConceptNet and Wikidata, have a similar purpose in that they integrate data from various sources and make it public, but they instead focus on certain application domains, and none of them is linked to databases in other areas, such as computer vision. Scene graphs, on the other hand, were introduced in the computer vision fields to model the relationship between identified items in photos.', 'They lack cross-domain compatibility, however, and cannot be queried using common query languages. As a result, current resources should be streamlined, and new resources should be easily incorporated. It is quite advantageous, for example, it can aid in the prevention of distribution shifts and the development of more robust models for training and testing.\xa0', 'Furthermore, the transition from model tinkering to a deep knowledge of data necessitates that datasets be better organized. Furthermore, the term “data” should be expanded by this approach to include not only training data but also abstract information, such as commonsense or causal relationships.', 'In a nutshell, it is a single framework for various datasets that allows them to be readily merged and queried, for example, using standard query languages.', 'To realize the concepts stated above, researchers created VisionKG, a unified knowledge graph for CV datasets (e.g, COCO). VisionKG is a knowledge graph based on the Resource Description Framework (RDF) that contains RDF statements describing the metadata of pictures and the semantics of their annotations.\xa0', 'The World Wide Web Consortium (W3C) recommends RDF as a standardized data model for semantic data integration and as a formal representation for shared human-machine understanding. As a result, RDF may be used to represent numerous semantic structures of prominent label taxonomies like Wordnet, ConceptNet, and Freebase, which are utilized in a variety of CV datasets including Imagenet and OpenImage.', 'The process of creating VisionKG is depicted in the diagram above. It begins by gathering CV datasets and extracting annotation labels from them. 1. It follows the Linked Data principles and uses the RDF data model to create a unified data model for the annotation labels and visual features. Uniform Resource Identifiers (URIs) are used to name data entities (such as images, boxes, and labels). The RDF data model allows for the expression of data using triples of the form, <subject, predicate, object>.', 'To describe “an image contains a bounding box for a person” in the COCO dataset, we must first assign unique URIs for the image and the bounding box, e.g., vision.semkg.org/img01 and vision.semkg.org/box01, to create the following triples: <img01, hasBox, box01>,<box01, hasObject, obj01>, <obj01, rdf: type, Person>.', 'Predefined predicates include hasBox, hasObject, and rdf:type, with rdf:type expressing that an object/image belongs to a specific class/type in the knowledge base, such as Person. We can also add metadata and semantic annotations to the images, such as where the images came from or the relationships between the boxes in an image (as shown in the 2nd step on the above figure).', 'Datasets and analysis, in particular, can be performed using rich semantic query languages such as SPARQL. The SPARQL query language allows users to describe queries using RDF statements that are similar to SQL statements. The first application is to obtain mixed-datasets in an elegant manner using VisionKG.\xa0', 'For example, instead of the more complex query (as shown in below figure snippet 3) that covers all possible cases: a pedestrian in KITTI or man in Visual Genome, one can query for images of people from COCO, KITTI, and Visual Genome using a simple query (as shown in below figure snippet 4).', 'The diagram above depicts an example of label mapping in COCO, KITTI, and Visual Genome to knowledge base classes (depicted in 1 section). Labels are being expanded to reflect the class hierarchy (depicted in section 2). And two equivalent queries for retrieving images from the COCO, KITTI, and Visual Genome datasets that contain Person (depicted in section 3,4 section).', 'In this section, we’ll take a look at some query samples of VisionKG by which we can extract data on the fly by using Python.\xa0', 'Let’s quickly set up the environment and import dependencies.', 'Next, we can define a variable called query which holds the query statements. Here our query is about getting images of cars and trucks.', 'Next, the above-defined query needs to be passed in the VisionKG’s API as below.', 'All the query outcomes are stored in the result variables, which is a dictionary of metadata of the queried image. Now we’ll plot some of the samples of queried images.\xa0', 'The result looks like this:']","'from google.colab import output\n# install our vision api\nfrom google.colab import output\n# install our vision api\n!python -m pip install git+https://github.com/cqels/vision.git --force\noutput.clear()\n \n# import SemkgAPI\nfrom vision_utils import semkg_api
 data\nfrom skimage import io\nimport matplotlib.pyplot as plt\n\n', '# Query string\nquery_=\'\'\'#Give me 100 images containing car and truck\nprefix cv:<http://vision.semkg.org/onto/v0.1/>\nSELECT DISTINCT ?image\nWHERE {\n    ?ann1 a cv:Annotation.\n    ?ann1 cv:isAnnotationOfImage ?image.\n    ?ann1 cv:hasAnnotatedObject ?obj1.\n    ?obj1 cv:hasLabel ""car"".\n    ?ann2 a cv:Annotation.\n    ?ann2 cv:isAnnotationOfImage ?image.\n    ?ann2 cv:hasAnnotatedObject ?obj2.\n    ?obj2 cv:hasLabel ""truck"".\n    ?image cv:hasLocalPath ?localPath.\n}\nLIMIT 100\'\'\'\n', '#query and return result\nresult=semkg_api.query(query_)\n', '#display sample images\nrows=3\ncols=4\nf, ax_arr = plt.subplots(rows, cols, figsize=(16,8))\nfor j, row in enumerate(ax_arr):\n    for i, ax in enumerate(row):\n        if j*cols+i < len(result[\'images\']):\n            image = io.imread(semkg_api.SEMKG_IMAGES_HOST + result[\'images\'][j*cols+i][\'image_path\'])\n            ax.imshow(image)\n            ax.axis(\'off\')\n \nf.suptitle(""Sample images from the query result"", fontsize=16)\nplt.show()\n'"
177,major_computer_vision_tasks/,https://analyticsindiamag.com/chainercv-tutorial-a-tool-for-major-computer-vision-tasks/,"['Although there has been substantial progress and continued development of deep learning in the field of computer vision, we still lack a toolbox or library that incorporates all of the computer vision modes, such as object detection, etc. So, in this article, we will talk about ChainerCV, a library that has a variety of models that are required for computer vision-related tasks. The important points to be explored in this article are listed below.', 'First, we will understand some of the major tasks of computer vision.', 'Object detection is a computer vision technology used to recognize and locate things in photos and movies. Object detection, in particular, builds bounding boxes around identified items, letting us understand where they are in the scene (and how they move). Object detection and image recognition are commonly confounded.', 'Image recognition is used to label a picture. A snapshot of an Apple is labelled with the Apple. A picture of numerous apples still has the title Apple. Object detection, on the other hand, surrounds each Apple with a box and labels it with the name Apple. The model predicts each object’s location as well as the label that should be applied. Object detection, in this sense, adds to the amount of information available about an image.', 'In more traditional ML-based systems, computer vision algorithms are used to detect groups of pixels that may belong to an object by examining various elements of a picture, such as the colour histogram or edges. These attributes are then fed into a regression model that forecasts the object’s location and label.', 'Convolutional neural networks (CNNs) are used in deep learning-based techniques to do end-to-end, unsupervised object detection, eliminating the requirement to define and extract attributes separately. Understanding the fundamental principle of CNN check out this article.', 'The process of separating an image into sections with similar features is known as image segmentation. The components of the image into which you divide it are known as Image Objects. It’s the initial step in the photo analysis process. Without picture segmentation, computer vision applications would be almost impossible.', 'Image segmentation is a subfield of digital picture processing that focuses on splitting an image into different segments based on the features and properties of those portions. The primary goal of image segmentation is to simplify the image so that it can be examined more easily. For supervised and unsupervised training in machine learning, we can use the labels created from image segmentation.', 'Image segmentation is an extension of image classification in which we do localization in addition to classification. The model pinpoints where a corresponding object is present by delineating the object’s boundary, making image segmentation a superset of image classification.', 'ChainerCV supports algorithms for solving tasks in the field of computer vision, such as object detection while prioritizing usability and predictable performance. This makes it ideal for developers who aren’t computer vision experts to use as a building block in larger software projects like robotic software systems.\xa0', 'Building new neural network models using existing architectures as building blocks has become increasingly popular in recent years. Object detection algorithms are used in tasks like instance segmentation and scene graph generation, which rely on them to locate objects in images.', 'The algorithms developed by ChainerCV can be used to build software that solves complex computer vision problems.', 'Training a network is an important part of any machine learning algorithm, and ChainerCV makes it simple. In many cases, users require a machine learning model that can perform well on a specific dataset. When a pre-trained model isn’t enough for the users’ tasks, they must retrain the model using their own datasets.\xa0', 'In such cases, ChainerCV provides reference implementations for training models that can be used as a starting point for writing new training code. Pretrained models can also be used in conjunction with the users’ dataset to fine-tune the model. ChainerCV also includes a dataset loader, a prediction evaluator, and visualization tools for training a model.', 'Next, we’ll discuss what are the functional models of ChainerCV.', 'ChainerCV currently supports object detection and semantic segmentation using networks. Faster R-CNN and Single Shot Multibox Detector (SSD) meta-architectures can be used to group architectures in ChainerCV detection.\xa0', 'Faster R-CNN uses an external neural network called Region Proposal Networks to propose a crop for the input image and then performs classification on that crop. SSD attempts to reduce the extra time spent running Region Proposal Networks by directly predicting bounding box classes and coordinates. These meta-architectures are then turned into more concrete networks with various feature extractors and head architectures.', 'SegNet is one of the semantic segmentation models. The architecture is encoder-decoder in nature. A module for calculating loss has been separated from a network that predicts a probability map. This design allows the loss to be reused in other semantic segmentation model implementations, which we plan to add in the future.\xa0', 'The Models for a specific task are created with a common interface in mind. For instance, detection models support a prediction method that takes images and generates bounding boxes around regions where objects are predicted to be found.', 'After having this brief discussion related to functional models of ChianerCV now we’ll take a look at its implementation.\xa0', 'Let’s first install ChainerCV via pip as', '! pip install chainercv', 'Here we’ll perform object detection and below are the minimum dependencies that need to be imported.\xa0', 'Below is the image on which we are performing object detection.', 'Now by using the below few lines of codes we can detect horse and person for the above image.\xa0']",'! pip install chainercv'
178,and_recognition_using_mmocr/,https://analyticsindiamag.com/a-guide-to-text-detection-and-recognition-using-mmocr/,"['Optical character recognition (OCR) is a sort of image conversion that basically extracts text from a given image, a document photo, etc. Various applications and technologies, such as Adobe Acrobat and the ML-based tool, such as Tesseract OCR, have been developed to aid with this process. In this article, we will go over tasks performed in the OCR method. Thereafter, we will look into MMOCR, a Python-based application that centralizes all OCR-related operations. Below are major points listed that are to be discussed in this article.', 'Let’s first discuss text detection.\xa0\xa0', 'Text detection is the technique of detecting text in a picture and then enclosing it with a rectangular bounding box. Text can be detected using image-based or frequency-based algorithms.', 'Image-based approaches are used to segment images into several segments. Each segment is made up of pixels that have comparable qualities and are connected. The statistical features of related components are used to categorize and shape the text. Machine learning techniques such as support vector machines and convolutional neural networks are used to classify the components as text or non-text. Below one is an example of text detection.', 'The high-frequency coefficients are extracted using discrete Fourier transform (DFT) or discrete wavelet transform (DWT) in frequency-based approaches. The text in an image is believed to have high-frequency components, and picking only the high-frequency coefficients separates the text from the non-text regions.', 'For a given image there are text and non-text regions that have different textual qualities, region-based approaches partition images into small sections using windows and search these regions for the presence of text using texture or morphological operations. Some techniques categorize text and non-text using a 64 x 32-pixel window and the Modest AdaBoost classifier on 16 different spatial scales of the image, taking into account substantial changes in text size.', 'The text recognition stage transforms text pictures into a string of characters or sentences. Words are an elementary entity used by humans for visual recognition, hence converting images of text into words is critical.\xa0', 'Character recognition and word recognition are two different techniques of recognition. Character recognition algorithms separate a text image into several single-character cutouts. For these strategies, character separation between adjacent characters is crucial.', 'Character recognition using the Optical Character Recognition module (OCR) is used in the recognition process, where images are first segmented into k classes, followed by binary text image hypothesis generation, which passes through connected components analysis and the greyscale consistency constraint module before being fed to OCR.\xa0', 'A classifier based on Support Vector Machine (SVM) is utilized for character recognition since SVM supports multi-class classification well.', 'Word recognition recognizes words from text images by combining character recognition outputs with language models or lexicons. In the case of degraded characters, it can be employed. Word recognition is a superior approach to character recognition for situations with a restricted number of word possibilities in input photos.', 'The MMOCR stands for MultiMedia Optical Character Recognition which is a python-based toolbox that combines all the modalities as we discussed above required for a complete end-to-end solution in the OCR field.\xa0', 'MMOCR, in particular, offers a pipeline for text detection and recognition, as well as downstream tasks like named entity recognition and critical information extraction. MMOCR has 14 cutting-edge algorithms, which is much more than any other open-source OCR project, such as Tesseract OCR.\xa0', 'The toolbox now includes seven text detection methods, five text recognition methods, one key information method, and one named entity recognition method.', 'Let’s now see how we can make use of this tool practically. By running the below script we can install all the dependencies that are required to run this tool if you encountered any issues refer to this official installation guide.', 'We’ll first perform text detection for that import MMOCR class from the installed repository as below and inside this class, the various methods can be initialized such as detection, recognition, and understanding.', 'In the above method, we have defined the path for the output image to the default colab directory with the name of the output file and it results in the detection informs bounding boxes as we can see below.\xa0', 'Similarly, when combining detection and recognition, we need to initialize both det and recog inside the MMOCR class as below.', 'Inside the ocr.readtext we set print_reult to True which gives us Jason format of the result and for this task we have used a bill receipt,\xa0', 'Jason result,', 'JPG result,']","'# installing pytorch prebuilt\n! pip install torch==1.6.0 torchvision==0.7.0\n \n# install mmcv (computer vision based library)\n! pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/torch1.6.0/index.html\n \n# now below install mmdet
 mmocr\n! pip install mmdet\n \n! git clone https://github.com/open-mmlab/mmocr.git\n%cd mmocr\n! pip install -r requirements.txt\n! pip install -v -e . \n \n!export PYTHONPATH=$(pwd):$PYTHONPATH\n', ""from mmocr.utils.ocr import MMOCR\nocr = MMOCR(det='TextSnake', recog=None)\nresults = ocr.readtext('/content/street-sign-board-500x500.jpg', output='/content/street.jpg', export='/content/')"", ""# detection+recognition\nocr = MMOCR(det='PS_CTW', recog='SAR')\n \n# Inference\nresults = ocr.readtext('/content/ealistic-shop-receipt-paper-payment-bill.jpg',export='/content/',output='/content/bill.jpg', print_result=True)\n"""
179,image_matting_in_python/,https://analyticsindiamag.com/a-beginners-guide-to-image-matting-in-python/,"['Image matting is a common image and video editing and composition technique. Traditionally, convolutional neural networks are used to infer the alpha matte from the entire input image and an associated trimap. These methods define the state-of-the-art in image processing, video editing, and filmmaking applications. So, in this article, we will talk about image matting and the procedures involved with this. Finally, we will see how PyMatting, a Python-based toolbox, combines these procedures and produces the desired result. Following are the major points to be discussed in this article.\xa0', 'Let’s start the discussion by understanding what image matting is.', 'The task of extracting interesting targets from a static image or a video sequence is known as image matting. It has played a significant role in many images and video editing applications. Image compositing is the inverse procedure of image matting. Although studies on image matting techniques have surfaced, the traditional methods rely primarily on optical means.\xa0', 'With the advancement of computer and digital photography technology, most image matting operations are now performed using digital image processing software. Current image matting techniques can be divided into two categories based on image characteristics: blue screen matting, where the image background is user-defined, and natural image matting, where the background is arbitrary and unknown.', 'There are two types of traditional matting methods. One example is sampling-based methods.', 'Given an unknown pixel, these methods sample matched pixels from the foreground and background regions and then find the best combination of these pixels to predict the unknown pixel’s alpha value. These techniques include boundary sampling, ray casting sampling, and so on.\xa0', 'Propagation-based methods are another type. These methods include the Poisson equation-based method, random walks for interactive matting, and closed-form matting, which formulates a cost function based on local smoothness and then solves a linear equation system to find the globally optimized alpha matte.', 'In image segmentation, there is a broad class of objects that contain characteristics about specific shapes such as human body hair, animal hair, peacock feathers, and spider webs because these special objects are usually narrower than one pixel. Also included are some translucent objects such as clouds, waterfall, glasses, plastic bags, and flames, which are made up of two parts: foreground objects and background colours.\xa0', 'How does the above-mentioned segmentation for these special objects come to fruition? Some traditional image segmentation algorithms typically fail to get started when attempting to solve these problems. Digital image matting technology for transparency/elongated objects may be the best solution in this case.\xa0', 'To properly extract semantically meaningful foreground objects, the user can manually label an input image into three parts prior to matte pulling, namely foreground, background, and unknown regions in the image. The trimap is made up of three parts, as illustrated in Fig. 1 (b). The image matting problem is thus simplified for a given trimap to estimate the foreground colours, background colours, and alpha values for pixels in unknown regions based on the known foreground and background pixels.', 'In contrast to the trimap method, which requires the marking of all image regions, the strokes method only requires the specification of a few foregrounds and background scribbles in the appropriate image regions, as shown in Fig. 1 (c). These marked scribbles are considered input in stroke-based algorithms and are used to extract the alpha matte. Strokes-based algorithms require less user interaction and operation than trimap-based algorithms.', 'The crucial step in the trimap method is to create an accurate trimap for the desired foreground in the given image. To achieve satisfactory matting results, the unknown regions around the foreground boundaries are expected to be specified as finely as possible. This is due to the fact that an accurate trimap can provide more detailed information about the background and foreground, which improves matting accuracy.', 'The trimap method has a significant advantage in that drawing the trimap is a natural operation that allows the user to intuitively determine where to place the labels. However, creating a high-quality trimap is usually a time-consuming process, especially for images with a lot of details like hairs and leaves.', 'In comparison, the strokes method provides more flexibility because it does not require a strict interactive operation. However, the results of strokes-based image matting methods are typically dependent on the locations of the user-specified scribbles.', 'Making matters worse, the user may be unaware of which labels can produce better matting results, and improper labelling operations can degrade matting quality. Furthermore, unknown image regions generated by the stroke methods are typically larger than those generated by the trimap methods, increasing the overall computation.', 'In this section, we will take a look at Python-based Toolbox which aims to construct or extract an object from images or video sequences using the alpha matting technique. And it also aims to be computationally efficient and easy to use.\xa0', 'The process involves two inputs from the user: the first is the original image and the second is the Trimap representation of the object within the input image supplied. Here I’m using a dataset from alphamatting.com where we can use a variety of sample images with trimap representation.\xa0\xa0', 'The toolbox can be installed using pip as ! pip install pymatting', 'To know more about the working of the toolbox kindly check the official technical report and Git-Hub repository. Following are the images that we have chosen to obtain a cutout and trimap representation.\xa0', 'Input Trimap for Foreground extraction', 'Now the simplest way to obtain a foreground image using this toolbox is done using the cutout method, which just takes the above two images and name, location to store the result.\xa0', 'And here is the result,']","'from pymatting import cutout\n \ncutout(\n   # input image path\n   ""/content/GT25.png""
\n   # input trimap path\n   ""/content/GT25_tri1.png"",\n   # output cutout path\n   ""cutout1.png"")\n'"
180,framework_for_object_detection/,https://analyticsindiamag.com/a-hands-on-guide-to-icevision-framework-for-object-detection/,"['IceVision is a framework for object detection which allows us to perform object detection in a variety of ways using various pre-trained models provided by this framework. It also offers data curation features along with a dashboard for exploratory data analysis. The best feature it has is that it provides an end-to-end deep learning workflow that allows the practitioners to train networks with easy-to-use robust high-performance libraries such as PyTorch-Lightning and FastAI. In this article, we are going to discuss the IceVision framework for object detection with hands-on implementation. The major points that we will discuss here are listed below.', 'Table of Contents', 'Let’s begin the discussion by understanding what IceVision is.', 'What is IceVision?', 'IceVision is a framework that allows us to preprocess our data for object detection and train a model for object detection on the data so that using the model we can make inferences on the data. The framework provides layered connections between deep learning engines, libraries and models. Also, the framework has datasets that can be used for learning the basic implementation of the IceVision frameworks for object detection where the models under the framework are built using the libraries like TorchVision and Ultralytics YOLO.\xa0', 'We can select from many models built on the framework and also switch between them very easily. Basically using the IceVision, we can train a model according to the datasets and after that, we can change the datasets or model as per our requirement. According to its official GitHub profile, some of the features of IceVision are listed below.', '\xa0In the next part of the article, we are going to see a basic example of implementing IceVision framework.', 'Installing IceVision', 'Let’s start with the installation which can be done by using the following lines of codes.', '!wget https://raw.githubusercontent.com/airctic/IceVision/master/IceVision_install.sh', 'The above-given lines of code will let us have the packages of\xa0 Torch, TorchVision, IceVision framework, IceData,\xa0 MMDetection, YOLOv5 and EfficientDet. After gathering, we can install them using the following line of code.\xa0', 'Since we are using Google Colab we have some of the requirements like torch and TorchVision already installed in the environment. We can also change the installation target to cuda10 or CPU. Now we can restart our kernel using the restart button on the runtime panel of the notebook or we can simply use the Ctrl + m button for that.', 'Data Preparation\xa0', 'For moving forward to the modelling, we are required to have records using which we can build a model. In this section of the article, we will discuss how we can prepare data for modelling using the IceVision framework.\xa0', 'Importing Libraries\xa0', 'We can import all the components of the IceVision framework using the following line of code.', 'from IceVision.all import *', 'Download and Prepare a Dataset', 'Now we can take our steps to the modelling side. Before going for the modelling, we are required to have a dataset for this purpose. We have a data set called Fridge Objects dataset with 134 images belonging to the four classes:', 'Using the IceVision module for data import, we can import our data using this link.', 'Import the Data', 'Parse the Data', 'Using the parser module of the framework, we can load the annotation file and split the data into the training and testing, and validation parts. The submodule under the parser helps in annotating for the common errors in the data.\xa0\xa0\xa0', 'Using the following lines of code we can split the data into training and validation datasets.\xa0', 'Creating Datasets with Augmentations and Transform\xa0', 'As we know that data augmentation and transformation help in making a model well trained and perform accurately on the data. This framework also provides this facility where the Albumentations library helps in defining and executing transformations. There are various transformations provided in the framework. In this article, we are using the aug_tfms module for the transformation of the image which helps the model to get transformations like rotation, cropping, horizontal flips, and more.', 'Let’s define a function for transformation', 'Using the function with data\xa0', 'Let’s visualize the data after augmentation is performed.', 'training\xa0 data', 'validation data', 'Model Building', '\xa0Before training a model we are required to instantiate the model variable. Make the data according to the model and various procedures to follow before any modelling procedure. So let’s start with the pre modelling procedure.\xa0', 'Pre-Modelling Procedures\xa0', 'In order to build a model using the IceVision framework, we are required to select libraries, models, and backbones for the model. Also, it is mandatory for us to choose these all from the given options under the framework.', 'Here we are using the RetinaNet model with the backbone of \xa0 resnet50_fpn_1x. Which can be specified by using the following line of codes.', 'Now we can instantiate the model using the following lines of code.\xa0\xa0', 'Since we have various options of models and backbone we are required to make the data according to the model. Till now we have seen how we can call the data and make changes on the data. For editing data according to the model, the framework provides the facility of data loaders using which, we can make changes on the data for modelling purposes.', 'Let’s visualize the batch for validation in the loader.', 'Now we can track the progress of the training using the FastAI and PyTorch lighting for which we can use the framework provided metric class. We are just required to instantiate a variable that can hold the metrics under it.', 'Training\xa0', 'Now the above-defined metrics can be used for training the model using the FastAI or PyTorch-lightning. Both will support the same metrics.\xa0', 'Training using fastai', 'Tuning the Model', 'The above-given output is some of the results from tunning of a model where the most optimal result is highlighted. In the tabular results, we have a measure of training and validation losses with the metrics which we have chosen to track the training.\xa0', 'We can also train the model using the PyTorch Lightning. The procedure is almost same but the coding part for PyTorch lightening is different. We can use the following line of codes for training the model using the pytorch lightening:', 'We can instantiate the model using the following lines of codes:', 'Also, we can check the results using the following lines of codes:', 'The above-given output is the final result of the process we used for object detection using the IceVision framework. We can see that it is working well. We can use it for our projects because it is an open-source framework.\xa0\xa0', 'Final Words', 'In this article, we have seen an overview of the IceVision framework for object detection. Along with that, we have also seen how we can use models and data from the framework and how we can make a whole process work for the object detection task. I encourage users to follow the framework more and try to perform other tasks related to computer vision problems.']","'!bash IceVision_install.sh cuda11 master'
 '\xa0import icedata', 'path = icedata.fridge.load_data()', '# Create the parser', 'parser = parsers.VOCBBoxParser(annotations_dir=path / ""odFridgeObjects/annotations"", images_dir=path / ""odFridgeObjects/images"")', '# Parse annotations to create records', 'train, valid = parser.parse()', 'parser.class_map', 'train_trans = tfms.A.Adapter([*tfms.A.aug_tfms(size=384, presize=512), tfms.A.Normalize()])', 'valid_trans = tfms.A.Adapter([*tfms.A.resize_and_pad(384), tfms.A.Normalize()])', 'train_data = Dataset(train, train_tfms)', 'valid_data = Dataset(valid, valid_tfms)', 'vis = [train_data[1] for _ in range(8)]', 'print(""training\xa0 data"")', 'show_samples(vis, ncols=4)', 'vis = [valid_data[1] for _ in range(8)]', 'print(""validation data"")', 'show_samples(vis, ncols=4)', 'model_type = models.mmdet.retinanet', 'backbone = model_type.backbones.resnet50_fpn_1x(pretrained=True)', 'model = model_type.model(backbone=backbone(pretrained=True), num_classes=len(parser.class_map), **extra_args)\xa0', '# Data Loaders', 'train_load = model_type.train_dl(train_data, batch_size=8, num_workers=4, shuffle=True)', 'valid_load = model_type.valid_dl(valid_data, batch_size=8, num_workers=4, shuffle=False)', 'model_type.show_batch(first(valid_load), ncols=4)', 'metrics = [COCOMetric(metric_type=COCOMetricType.bbox)]', 'training = model_type.fastai.learner(dls=[train_load, valid_load], model=model, metrics=metrics)', 'training.fine_tune(20, 0.00158, freeze_epochs=1)', 'class LightModel(model_type.lightning.ModelAdapter):', '\xa0\xa0\xa0\xa0def configure_optimizers(self):', '\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0return Adam(self.parameters(), lr=1e-4)', 'light_model = LightModel(model, metrics=metrics)', 'trainer = pl.Trainer(max_epochs=5, gpus=1)', 'trainer.fit(light_model, train_load, valid_load)', 'model_type.show_results(model, valid_ds, detection_threshold=.5)'"
181,image_transformations_with_opencv/,https://analyticsindiamag.com/complete-tutorial-on-image-transformations-with-opencv/,"['In today’s scenario, image processing and computer vision are the subjects of attraction in the data science world. If we consider an image as data, we can extract a lot of information like the objects presented in an image, how many colors, and the pixel configurations of the image. And also, in various cases of machine learning, images take part as an informative member of the process. Many image processing applications in the machine learning field like object detection,\xa0 face recognition, threat detection, etc. Before going into the modeling part, I recommend working with the editing part of the image. There can be a number of basic operations we can perform in an image. This article is mainly focused on the following processes:', 'Between all these things, we will also have some basic reading knowledge, displaying and saving the image, and rotating and resizing an image using OpenCV-python.', 'Let’s start with the installation of the OpenCV-Python.', '!pip install opencv-python', 'As I am using google Colab, it already provides the OpenCV installed in the notebook environment.', 'Reading the image.', '(Note – cv2.imshow() is disabled in Colab, because it causes Jupyter sessions', 'to crash; see https://github.com/jupyter/notebook/issues/3935. As a substitution, consider using from google.colab.patches import cv2_imshow )', 'Displaying the image:', 'From the above lines of codes, you can read and display whatever image you want.', 'Checking the data structure:', 'Here we can see the shape and size of the image. Since OpenCV uses a NumPy data structure to store the images, it shows the type of data as numpy.ndarray. We can also change the color space of the image. For example, the image we have read is in BGR color space.', 'There are various color spaces available in the package. Here we have changed the space using the cv2.COLOR_BGR2GRAY function. Converting the image color space BGR to RGB.', 'Here we have seen when we read an image using OpenCV by default, its color space is set on the BGR color space and using the function, we can change the colour space.', 'Saving the image.', 'Here you must be wondering why I used the cv2.IMWRITE_PNG_COMPRESSION function in saving the image. When I imported the image, it was a jpg format photo, and we saved it as a png format image.', 'In image processing, the color space refers to the space where the patterns of the color are organized in different manners. Combining a color model and a mapping function makes a color space. The color model helps represent the pixel values in tuples, and the mapping function maps the color model to set all colors that can be represented. There are around 190 color spaces present in the OpenCV, out of which we can choose according to the requirements. For more information about the list, you can go through this link.', 'Next in the article, we will see how we can perform editing in any image structure. The first thing we are performing is called image translation.', 'In computer vision or image processing, shifting an image into a frame is considered as the image translation.', 'Let’s see how we can perform that.', 'Displaying the translated image:', 'Here in the output, we can see that we have shifted the image in the frame. To understand the code part first, we need to go through the warpaffine function. It takes a matrix as a parameter in the matrix we give x = 70, which means we are telling the function to shift the image 70 units on the right side and y= 110, which means we are telling the function to shift the image 110 units downwards.\xa0\xa0', 'In the third argument, where we mentioned the num_cols and num_rows, we told the function to crop the image by two units from both x and y sides.', 'We can also set the image without cropping the image in the middle of the frame.', 'Here in the input, we have told the function to shift the image upward and to the left side and added the total value of x and y from the translation matrix in num-row and num_col to avoid the cropping of the image.', 'Next, in the article, we will see how we can rotate and scale the image.', 'In this section we will try to rotate the image by a certain angle:', 'Here in the codes, we have used the getRotationMatrix function to define the parameter required in the warpAffine function to tell the function to make a matrix that can give a required rotation angle( here it is 30 degrees) with shrinkage of the image by 40%.\xa0\xa0\xa0', 'This is a commonly used method in the computer vision and image processing area where we try to resize the image according to the requirement. Roughly we perform two things in the image scaling: either we enlarge the image or we shrink the image; in OpenCV, we have function resize; using this, we can perform the scaling of the image.', 'Shrinking the image:', 'Enlarge the image size.', 'Here in the code, we resize the image to (350, 300), and after this, we enlarged the shrunk image by a factor of 1.5. We had used INTER_AREA interpolation when we were shrinking the image, and then we used CUBIC interpolation when enlarging the image so that the quality of pixels will not be harmed while resizing.', 'In image processing, image transformation can be defined as having control on its dimensional points to edit the images by moving them into three dimensional or two-dimensional space. Next in the article will perform some basic transformations of 2D images. Before going into the implementation of image transformation, let’s see what is euclidean transformation. Euclidean transformation is a type of geometric transformation that causes changes in the dimensions and angles without causing the change in the basic structure-like area. Image Transformation works based on euclidean transformation, where the image can look shifted or rotated, but the structure of pixels and matrix will remain the same.', 'Roughly we can divide image transformation into two sections:', 'Affine transformation', 'As the name suggests in this transformation, preserving parallel relationships is one of the main concepts of this kind of transformation where lines will remain the same. Still, the square can change into a rectangle or parallelogram. It works by preserving the lengths and angles. The following example will give a better view of the Affine transformation where I am implementing it using the getAffineTransformation function.', 'Displaying the image and transformed image.', 'Here we can say how we have changed the image. The image we have used was in the rectangle shape, and after transformation, it became a parallelogram. We included two control points in the affine transform matrix and told the getAffineTransform function to map those points in different places.', 'We can also make a mirror image of the original image.\xa0', 'The mapping of the points in this transformation will look like the following image.', 'Where we just selected the outermost points of the image as the control points.', 'Projective Transformation:', 'As seen in the Affine transformation, we have less control in shifting the points, but in projective transformation, we have the freedom to move and shift the control points. It works on the projective view option where we see an object from its every plane. For example, a square image on paper from the front side looks like a square, but it will look like a trapezoid from the slight right or left side.', 'Implementation of the projective transformation will explain it more.', 'Displaying the image:', 'Let’s imagine you see the image(not transformed) from the upside of the screen; the normal image looks like this. This editing of the image mostly helps in the object detection program. By projective view, we extract most of the features available in the image. In this transformation, we have changed a rectangular into a trapezoid. We have freedom here to shift our angles and the points; we can even make the image look like a triangle.', 'We have seen how the transformation works, but what if we need to randomly put these effects in a single picture, or do we just need to shift the control points in the space randomly? Then we require more control over the movements. We have seen that projective transformation increased the level of control, but still, some restrictions were there. In projective transformation, we were working with dimensions, lengths and angles. Now we will see how we can work on randomly selected points from the image to change the surface of the image, which will put the wave-like edits in the image.\xa0', 'Displaying the image.', 'cv2_imshow(img_output)', 'Here we have pushed a sine wave into the image, vertically moving on the surface of the image. This is how we can perform the image wrapping. Using a cos wave, we can also put a horizontal wave into the image.\xa0']","'!pip install opencv-python'
 ""import cv2\nimage = cv2.imread('/content/drive/MyDrive/Yugesh/image wraping/DSCN9772.JPG')"", 'from google.colab.patches import cv2_imshow\ncv2_imshow(image)\ncv2.waitKey()', 'print(image.shape)\nprint(image.size)\nprint(type(image))', 'gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\ncv2_imshow(gray)', 'rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\ncv2_imshow(rgb)', ""cv2.imwrite('/content/drive/MyDrive/Yugesh/image wraping/output.png', rgb, [cv2.IMWRITE_PNG_COMPRESSION])"", 'num_rows, num_cols = image.shape[:2]\ntranslation_matrix = np.float32([ [1,0,70], [0,1,110] ])\nimg_translation = cv2.warpAffine(image, translation_matrix, (num_cols, num_rows), cv2.INTER_LINEAR)', 'cv2_imshow(img_translation)', 'translation_matrix = np.float32([ [1,0,-30], [0,1,-50] ])\nimg_translation = cv2.warpAffine(img_translation, translation_matrix, (num_cols + 70 + 30, num_rows + 110 + 50))\ncv2_imshow(img_translation)', 'img_rotation = cv2.warpAffine(image, cv2.getRotationMatrix2D((num_cols/2, num_rows/2), 30, 0.6), (num_cols, num_rows))', 'img_shrinked = cv2.resize(image,(350, 300), interpolation = cv2.INTER_AREA)\ncv2_imshow(img_shrinked)', 'img_enlarged = cv2.resize(img_shrinked,None,fx=1.5, fy=1.5, interpolation = cv2.INTER_CUBIC)\ncv2_imshow(img_enlarged)', 'src_points = np.float32([[0,0], [num_cols-1,0], [0,num_rows-1]])\ndst_points = np.float32([[0,0], [int(0.6*(num_cols-1)),0], [int(0.4*(num_cols-1)),num_rows-1]])\nmatrix = cv2.getAffineTransform(src_points, dst_points)\nimg_afftran = cv2.warpAffine(image, matrix, (num_cols,num_rows))', 'cv2_imshow(image)\ncv2_imshow(img_afftran)', 'src_points = np.float32([[0,0], [num_cols-1,0], [0,num_rows-1]])\ndst_points = np.float32([[num_cols-1,0], [0,0], [num_cols-1,num_rows-1]])\nmatrix = cv2.getAffineTransform(src_points, dst_points)\nimg_afftran = cv2.warpAffine(image, matrix, (num_cols,num_rows))\ncv2_imshow(image)\ncv2_imshow(img_afftran)', 'src_points = np.float32([[0,0], [num_cols-1,0], [0,num_rows-1], [num_cols-1,num_rows-1]])\ndst_points = np.float32([[0,0], [num_cols-1,0], [int(0.33*num_cols),num_rows-1], [int(0.66*num_cols),num_rows-1]])\nprojective_matrix = cv2.getPerspectiveTransform(src_points, dst_points)\nimg_protran = cv2.warpPerspective(image, projective_matrix, (num_cols,num_rows))', 'import math\nrows, cols = image.shape[:2]\n\n# Vertical wave\xa0\n\nimg_output = np.zeros(img_afftran.shape, dtype=image.dtype)\xa0\nfor i in range(rows):\xa0\n\xa0\xa0\xa0\xa0for j in range(cols):\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0offset_x = int(25.0 * math.sin(2 * 3.14 * i / 180))\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0offset_y = 0\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if j+offset_x < rows:\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0img_output[i,j] = img_afftran[i,(j+offset_x)%cols]\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0else:\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0img_output[i,j] = 0\xa0', 'cv2_imshow(img_output)'"
182,with_opencv_in_python/,https://analyticsindiamag.com/image-processing-with-opencv-in-python/,"['Machine learning enthusiasts always have a craze for Computer Vision. Practical Computer Vision tasks require basic knowledge and skills of image processing. Image processing is carried out in different stages of Computer Vision applications such as preprocessing images, deep learning modeling and post-processing. Image processing is extensively used in video datasets compared to image datasets. Image processing finds a crucial place in the deep learning domain with the growing size of image and video data and the increase in digital solution needs.', 'OpenCV is one of the famously used open-source Python libraries meant exclusively for Computer Vision. Modules and methods available in OpenCV allow users to perform image processing with a few lines of codes. In this tutorial, we discuss different image processing techniques of OpenCV with hands-on Python codes.', 'This article assumes that the users are familiar with the basics of the OpenCV library. The following articles give a quick kick-start to fulfil the prerequisites of this tutorial.\xa0', 'Getting Started with OpenCV in Python', 'Real-time GUI Interactions with OpenCV in Python', 'Thresholding is the process of forcing a certain or all the pixel values either to zero or to the maximum possible value. Various thresholding techniques are available to process images. Thresholding can be performed either with a grayscale image or a colour image.', 'Binary thresholding makes pixel values either to zero or 255 based on the threshold value provided. The pixel values below the threshold value are set to zero, and the pixel values above the threshold value are set to 255. We can recall that 0 refers to black and 255 refers to white in a grayscale image. Thus a grayscale image applied with binary thresholding will become a black-and-white-only image. On the other hand, a colour image applied with binary threshold values will only have black, blue, green, red or any combination of the latter three colours. For instance, combination B+G+R gives white colour, and R+G gives yellow colour.', 'Inverse binary thresholding is the inverted process of binary thresholding. The pixel values above the threshold are set to zero and the pixel values below the threshold are set to 255. The resulting image will become a black-and-white-only image if the input is a grayscale image.', 'In truncated thresholding, the pixel values below the threshold are left unaltered, and all other values are set to the threshold value.', 'Threshold-to-zero is the thresholding process of setting the pixel values below the threshold to zero while the pixel values above the threshold are left unaltered.', 'Threshold-to-zero-inverse is the thresholding process of setting the pixel values above the threshold to zero while the pixel values below the threshold are left unaltered.', 'For better understanding, we perform all the above-discussed thresholding processes on a grayscale image.', 'The fixed threshold does not yield good results when there is a difference in illumination on the object of interest. For instance, in the above binary thresholding process, the portion of the human face with more illumination is represented by white pixels, while the portion with less illumination is represented by black pixels. This issue can be tackled by incorporating adaptive thresholding. Adaptive thresholding determines the threshold values locally. It takes as arguments the number of local neighborhood pixels (an odd-valued kernel) and the constant pixel value by which each pixel is subtracted from. The two types of adaptive threshold determination are Mean (arithmetic mean of neighborhood pixels) and Gaussian (Gaussian mean to exclude noises).', 'Image smoothing is an important image processing technique that performs blurring and noise filtering in an image. It finds applications in preprocessing and postprocessing of deep learning models. In general, smoothing is performed by a 2D kernel of a specific size on each channel of the image. The kernel average of neighborhoods yields the resulting image. The famous smoothing algorithms in use are Blur, Gaussian Blur, Median Blur and Bilateral Filter.', 'Blur method is the simple filter that homogeneously applies its kernel to calculate the local weighted average.', 'Gaussian Blurring is the process of removing Gaussian noise from an image. This method takes the image, the kernel size and the standard deviation as arguments.', 'Median blur is known for its salt-and-pepper noise removal. Unwanted small white and black dots on an image is removed with this tool.', 'Bilateral Filter is used when there is a need for both noise filtering and edges retention. This method detects sharp edges and keeps it as such without any blur.', 'Image gradients are the rate of change of pixel values in either x or y direction or in both x and y directions. It helps identify the sudden changes in pixel values. In other words, it helps detect the edges. If the gradient is applied in x-direction, vertical edges are detected. If the gradient is applied in y-direction, horizontal edges are detected.', 'In OpenCV, Laplacian gradient detects edges both horizontally and vertically, while Sobel gradient detects edges either horizontally or vertically.', 'Vertical and horizontal edges can be blended together to generate an image gradient with all edges.', 'Canny Edge Detection is a powerful edge detection algorithm that performs Gaussian noise filtering, Sobel based horizontal and vertical edges detection, non-maximum suppression to remove unwanted edge points and hysteresis thresholding with two limiting thresholds to have thin and strong edges.', 'By changing the minimum and maximum threshold values, we can improve the results.', 'Image contours are the continuous shape outlines present in an image. OpenCV detects the contours present in an image and collects its coordinates as a list. The collected contours can be drawn over the original image back.', 'Contours are the fundamental building blocks for object shape detection, motion detection and image segmentation. Contours are collected in a Python list. We can draw a particular contour or contours by indexing or slicing those contours alone.', 'In this tutorial, we discussed how image processing is performed with OpenCV in Python. We studied the following methods with hands-on codes and visualizations:', 'With a basic understanding of image processing, users can dive deeper into real-world Computer Vision problems.']",""" # read an image in grayscale\n img = cv2.imread('daria.jpg'
 0)\n img = cv2.resize(img, (320,225))\n # apply various thresholds\n val, th1 = cv2.threshold(img, 110, 255, cv2.THRESH_BINARY)\n val, th2 = cv2.threshold(img, 110, 255, cv2.THRESH_BINARY_INV)\n val, th3 = cv2.threshold(img, 110, 255, cv2.THRESH_TRUNC)\n val, th4 = cv2.threshold(img, 110, 255, cv2.THRESH_TOZERO)\n val, th5 = cv2.threshold(img, 110, 255, cv2.THRESH_TOZERO_INV)\n # display the images\n cv2.imshow('Original', img)\n cv2.imshow('THRESH_BINARY', th1)\n cv2.imshow('THRESH_BINARY_INV', th2)\n cv2.imshow('THRESH_TRUNC', th3)\n cv2.imshow('THRESH_TOZERO', th4)\n cv2.imshow('THRESH_TOZERO_INV', th5)\n cv2.waitKey(0)\n cv2.destroyAllWindows() "", "" img = cv2.imread('daria.jpg', 0)\n img = cv2.resize(img, (320,225))\n # apply various adaptive thresholds\n th1 = cv2.adaptiveThreshold(img, 255, \\\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0cv2.ADAPTIVE_THRESH_MEAN_C, \\\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0cv2.THRESH_BINARY, 7, 4)\n th2 = cv2.adaptiveThreshold(img, 255, \\\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \\\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0cv2.THRESH_BINARY, 7, 4)\n # display the images\n cv2.imshow('ADAPTIVE_THRESHOLD_MEAN', th1)\n cv2.imshow('ADAPTIVE_THRESHOLD_GAUSSIAN', th2)\n cv2.waitKey(0)\n cv2.destroyAllWindows() "", "" img = cv2.imread('sharon.jpg', 1)\n img = cv2.resize(img, (300,300))\n # Apply blur\n img1 = cv2.blur(img,(3,3))\n # display the images\n cv2.imshow('Original', img)\n cv2.imshow('Blur', img1)\n cv2.waitKey(0)\n cv2.destroyAllWindows() "", "" img = cv2.imread('keiron.jpg', 1)\n img = cv2.resize(img, (320,210))\n # Apply Gaussian blur\n img1 = cv2.GaussianBlur(img,(5,5),2)\n # display the images\n cv2.imshow('Original', img)\n cv2.imshow('Gaussian', img1)\n cv2.waitKey(0)\n cv2.destroyAllWindows() "", "" img = cv2.imread('tree.png', 0)\n # Apply median blur\n img1 = cv2.medianBlur(img,3)\n # display the images\n cv2.imshow('Original', img)\n cv2.imshow('Median', img1)\n cv2.waitKey(0)\n cv2.destroyAllWindows() "", "" img = cv2.imread('keiron.jpg', 1)\n img = cv2.resize(img, (320,210))\n # Apply Bilateral Filter\n img1 = cv2.bilateralFilter(img,7,100,100)\n # display the images\n cv2.imshow('Original', img)\n cv2.imshow('Bilateral', img1)\n cv2.waitKey(0)\n cv2.destroyAllWindows() "", "" import numpy as np\n img = cv2.imread('chessboard.jpg', 0)\n img = cv2.resize(img, (300,200))\n # Laplacian image gradient\n lap = np.uint8(np.absolute(cv2.Laplacian(img,cv2.CV_64F, ksize=1)))\n # display the images\n cv2.imshow('Original', img)\n cv2.imshow('Lpalacian', lap)\n cv2.waitKey(0)\n cv2.destroyAllWindows() "", "" img = cv2.imread('chessboard.jpg', 0)\n img = cv2.resize(img, (300,200))\n # Sobel image gradient\n vertical = np.uint8(np.absolute(cv2.Sobel(img,cv2.CV_64F, 1,0, ksize=1)))\n horizon = np.uint8(np.absolute(cv2.Sobel(img,cv2.CV_64F, 0,1, ksize=1)))\n # display the images\n cv2.imshow('Vertical', vertical)\n cv2.imshow('Horizontal', horizon)\n cv2.waitKey(0)\n cv2.destroyAllWindows() "", "" Sobel = cv2.bitwise_or(vertical, horizon)\n cv2.imshow('Sobel', Sobel)\n cv2.waitKey(0)\n cv2.destroyAllWindows() "", "" img = cv2.imread('chessboard.jpg', 0)\n img = cv2.resize(img, (450,300))\n def null(x):\n \xa0\xa0\xa0\xa0pass\n # create trackbars to control threshold values\n cv2.namedWindow('Canny')\n cv2.resizeWindow('Canny', (450,300))\n cv2.createTrackbar('MIN', 'Canny', 80,255, null)\n cv2.createTrackbar('MAX', 'Canny', 120,255, null)\n while True:\n \xa0\xa0\xa0\xa0# get Trackbar position\n \xa0\xa0\xa0\xa0a = cv2.getTrackbarPos('MIN', 'Canny')\n \xa0\xa0\xa0\xa0b = cv2.getTrackbarPos('MAX', 'Canny')\n \xa0\xa0\xa0\xa0# Canny Edge detection\n \xa0\xa0\xa0\xa0# arguments: image, min_val, max_val\n \xa0\xa0\xa0\xa0canny = cv2.Canny(img,a,b)\n \xa0\xa0\xa0\xa0# display the images\n \xa0\xa0\xa0\xa0cv2.imshow('Canny', canny)\n \xa0\xa0\xa0\xa0k = cv2.waitKey(1) & 0xFF\n \xa0\xa0\xa0\xa0if k == ord('q'):\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0break\n cv2.destroyAllWindows() "", "" img = cv2.imread('valerie.jpg', 1)\n img = cv2.resize(img, (320,480))\n # show original image\n cv2.imshow('Original', img)\n # binary thresholding\n gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n val,th = cv2.threshold(gray, 127,255,cv2.THRESH_BINARY)\n # find contours\n contours,_ = cv2.findContours(th,\xa0\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0cv2.RETR_TREE,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0cv2.CHAIN_APPROX_NONE)\n # draw contours on original image\n # arguments: image, contours list, index of contour, colour, thickness\n cv2.drawContours(img, contours, -1, (0,0,255),1)\n cv2.imshow('Contour', img)\n cv2.waitKey(0)\n cv2.destroyAllWindows() "", "" # slice the contours list\n face = contours[455:465]\n cv2.drawContours(img, face, -1, (0,0,255),1)\n cv2.imshow('Contour', img)\n cv2.waitKey(0)\n cv2.destroyAllWindows() """
183,with_opencv_in_python/,https://analyticsindiamag.com/getting-started-with-opencv-in-python/,"['OpenCV is a powerful and versatile open-source library for Computer Vision tasks. It is supported in many languages, including Python, Java and C++. It is packed with more than 2500 algorithms to perform almost any Computer Vision task with just a single library. OpenCV is well known for its interactive windows and real-time processing capabilities.\xa0', 'In this quick tutorial, basic image processing and video processing with this library are discussed using Python codes. It should be noted that Jupyter Notebook environments such as Colab and JupyterLab clash with some interactive methods of OpenCV. However, these issues can be easily overcome with the help of some external libraries. Nevertheless, the codes discussed in this tutorial are carried out in a Spyder environment for the best performance.', 'Install the library in your local machine to have fun with images and videos. Install OpenCV Python from the PyPi package.\xa0', 'Before starting the image processing, make sure that the working directory has an image. In OpenCV, an image can be read as a color or a grayscale image with corresponding flags. Read a coloured image with the imread method as shown below:', 'Argument 1 in the above imread method is a flag that directs that the image has to be read in colour. Flag 0 refers to grayscale image reading. The read image can be displayed in a window using imshow method.', 'OpenCV allows performing multiple inputs and outputs simultaneously through multiple windows. The waitKey method is used to inform OpenCV the time duration over which a window can be kept open. This method takes time as an argument in milliseconds. If 0 is provided as the argument, the user should close the window manually. It can be noted that the window has a name of its own. Therefore, when there comes a window into play, it must be named. Here, the window name is ‘Image Window’. destroyAllWindows method is used to force all the open windows to close at once.', 'Write an image to the local directory using the imwrite method. The method takes two arguments: name of image about to be written and the read/processed image.', 'A video is a collection of frames displaying at speed termed frames per second (fps). Each frame is an image. Excluding the specific properties such as frames per second and codec format, OpenCV processes videos the same way it processes images. OpenCV reads a video either from a file or directly from the device’s camera. This feature makes OpenCV the de facto choice for navigation systems such as robots and drones, embedded systems such as Raspberry Pi and Arduino, and autonomous vehicles.', 'A frame in video output:', 'Video plays until there is a frame to read in the file. If we wish to read the device’s camera, the file name in the VideoCapture method should be replaced with number 0 (zero). In that case, the user needs to enable keyboard shortcut to stop capturing. The following example shows camera capturing along with keyboard shortcut of letter ‘q’ to interrupt capturing.', 'Writing a video into a file needs some properties such as frame width, frame height and frame rate (fps). The following codes enable us to learn the input video properties.', 'Write the video file to disk in any format you wish. It is required that the right fourcc codec format, frame size and frame rate should be provided as arguments to enable proper saving of the video file. Here, in the below example code, the file is saved in the working directory in the name of ‘output.mp4’.', 'In OpenCV, shapes such as a line, arrowed line, circle, rectangle, ellipse and polygons can be drawn over an image. Start and end coordinates, color of the shape, thickness of the border line are the common parameters in drawing a shape. It should be noted that OpenCV supports colours in BGR format, in contrast to most libraries such as Matplotlib and Seaborn where they support colours in RGB format.', 'Draw a vertical green coloured line on the image', 'Draw a blue coloured circle and a red coloured rectangle on top of it.\xa0', 'Text can be written on an image. Its location, size, font and colour can be customized as per the user’s wish.\xa0', 'Running date-time on videos can be made in real-time using Python’s datetime library and OpenCV’s putText method. The below code example shows real-time date-time display on a video. It is especially useful in real-time camera capturing.', 'We discussed that colour images in OpenCV are read and processed in BGR colour format. An image can be split into three separate images for each of the Blue, Green and Red channels. On the other hand, the split image can be back to original colour or different channel combinations as shown in the example below.', 'A portion of an image can be extracted or replaced with a similar-sized image patch or any simple math alterations similar to that. Here, we perform some replacements in an image with its own sub-portion in an example.', 'Two or more images can be merged either by simple addition or by weighted addition. However, adding together images should be the same size and have the same number of channels.\xa0', 'In this tutorial, we discussed the OpenCV library and its fundamental implementation in Python.\xa0', 'We discussed loading an image or a video file and saving them to the disk. Further, we drew some standard shapes such as line, circle and rectangle over an image. We looked at writing texts, including real-time running texts of date and time. We learned how OpenCV handles colour channels (B,G,R) and split and merged in different colour combinations. Finally, we discussed alterations in an image and merging different images to create a new interactive image.']","'imread'
 'imshow', 'waitKey', 'destroyAllWindows', 'imwrite', 'VideoCapture', 'putText'"
184,image_analysis_python_library/,https://analyticsindiamag.com/guide-to-layoutparser-a-document-image-analysis-python-library/,"['Documents containing a combination of texts, images, tables, codes, etc., in complex layouts are digitally saved in image format. Analyzing and extracting useful information out of these image documents is performed with the help of machine learning. This supervised task is termed as Document Image Analysis (DIA). The popular DIA tasks in practical use include:\xa0', 'There have been a few task-specific applications such as OCR (Optical Character Recognition) in real-world usage over decades. However, a library that provides all DIA tasks in one place became an important need of document analysis society, such as historical researchers and social science analysts. For instance, a screenshot image of an old newspaper’s page may contain historic research-centred contents in the form of tables, charts, texts and photographs. An OCR reader can be used to extract texts but cannot read other information. Moreover, an OCR reader may miss to recognize the text layouts and mix texts from different layouts in its output. A separate method will be required to extract information from tables, charts and so on.', 'The evolution of deep learning-based convolutional neural networks has begun to try to give solutions to the need of an integrated Document Image Analysis system. However, the practical implementation of recent successful deep learning models has faced some challenges. High-level DIA parameters are not always explicitly processed by deep learning frameworks. This makes customization of pre-trained models difficult. Popular models are trained on a particular set of annotated document images. Documents do not possess any common template and formats and are limited only by human creativity. This needs to collect task-specific annotated document images, preprocess them according to the model requirements, and fine-tune the model with those images in case of custom implementations of a popular model. The deep learning network part and the DIA part are usually trained separately to make customized fine-tuning difficult, tedious, and time-consuming.', 'To this end, Zejiang Shen of the Allen Institute of AI, Ruochen Zhang of the Brown University, Melissa Dell and Jacob Carlson of the Harvard University, Benjamin Charles Germain Lee of the University of Washington, and Weining Li of the University of Waterloo have introduced LayoutParser, a Python library for Document Image Analysis. This library has a Model Zoo with a great collection of pre-trained deep learning models with an off-the-shelf implementation strategy. This library has a unified architecture to adapt any DIA model. Apart from the usage of pre-trained models, LayoutParser provides tools for customization and fine-tuning as per need. Further, data preparation tools- for tasks such as document image annotation and data preprocessing tools are readily available in this library. The library aims at quality models and pipelines distribution with reproducibility, reusability and extensibility through a continuously improving community platform.', 'LayoutParser performs one or more of the following DIA usages:', 'To store a layout in memory and retrieve it back, LayoutParser offers unified data structures. Three key components in the LayoutParser data structure are Coordinate, TextBlock, and Layout. Unique operations are defined in LayoutParser to process the library-defined data structures.\xa0', 'We discuss the code implementation and two practical applications of the library in the sequel.', 'Install the LayoutParser library and its dependencies from the PyPi packages.', 'Import the libraries and modules.', 'Deploy a pre-trained Detectron2 model configured for layout parsing.', 'Now the model is ready for inference. Download the source files from the official repository to obtain a sample image to perform inference on it.', 'Change directory to read the example data.', 'Read the ‘paper-image.jpg’ and display it.', 'Predict the layouts in the above image using the pre-trained model.', 'Display the image with predicted layouts over it.', 'This Colab Notebook contains the above example code implementations.', 'Install the LayoutParser and its dependencies. In addition, install an OCR engine. Here, we use the TesseractOCR engine to recognize text and its location.', 'Import the necessary libraries and modules.', 'Load the pre-trained TesseractOCR engine.', 'Prepare data from the source code. Download the source files from the source repository and change the directory to denote the example images path.', 'Read the image and display it to have an idea of how it looks.', 'Detect text characters with the OCR engine. Collect the text along with its bounding box details for plotting and post-processing.', 'Plot the original image along with bounding boxes on recognized texts.', 'We can recognize that the output texts are reproduced with Engine-specified fonts and sizes. Thus the system has recognized texts and their locations precisely. Further, we can post-process these texts in a column-wise manner or row-wise manner as per need.', 'This Colab Notebook contains the above example code implementations.']",""" %%bash\n pip install -U layoutparser\n # install detectron2\n pip install 'git+https://github.com/facebookresearch/detectron2.git@v0.4#egg=detectron2'\xa0\n # install OCR module\n pip install layoutparser[ocr]\xa0\xa0\xa0\xa0\xa0\xa0 ""
 ' import layoutparser as lp\n import matplotlib.pyplot as plt\n import matplotlib\n %matplotlib inline\n import cv2 ', ' model = lp.Detectron2LayoutModel(\'lp://PubLayNet/faster_rcnn_R_50_FPN_3x/config\',\xa0\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0extra_config=[""MODEL.ROI_HEADS.SCORE_THRESH_TEST"", 0.8],\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0label_map={0: ""Text"", 1: ""Title"", 2: ""List"", 3:""Table"", 4:""Figure""}) ', '!git clone https://github.com/Layout-Parser/layout-parser.git', ' %cd /content/layout-parser/examples/data/\n !ls -p ', ' img = cv2.imread(""/content/layout-parser/examples/data/paper-image.jpg"")\n # convert BGR image into RGB format\n image = img[..., ::-1]\n # display image\n plt.figure(figsize=(12,16))\n plt.imshow(image)\n plt.xticks([])\n plt.yticks([])\n plt.show() ', 'layout = model.detect(image)', 'lp.draw_box(image, layout, box_width=3)', ' %%bash\n pip install -U layoutparser\n pip install layoutparser[ocr]\xa0\xa0\xa0\xa0\xa0\xa0\n sudo apt install tesseract-ocr\n sudo apt install libtesseract-dev ', ' import layoutparser as lp\n import numpy as np\n import pandas as pd\n import matplotlib.pyplot as plt\n import matplotlib\n %matplotlib inline\n import cv2 ', 'model = lp.TesseractAgent()', ' !git clone https://github.com/Layout-Parser/layout-parser.git\n %cd /content/layout-parser/examples/data/\n !ls -p ', "" image = cv2.imread('example-table.jpeg')\n # display image\n plt.figure(figsize=(12,16))\n plt.imshow(image)\n plt.xticks([])\n plt.yticks([])\n plt.show() "", ' res = model.detect(image, return_response=True)\n # collect text and its bounding boxes\n ocr\xa0 = model.gather_data(res, lp.TesseractFeatureType(4))\xa0 ', ' lp.draw_text(image, ocr, font_size=12, with_box_on_text=True,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0text_box_width=1) '"
185,object_detection_python_toolbox/,https://analyticsindiamag.com/guide-to-mmdetection-an-object-detection-python-toolbox/,"['MMDetection is a Python toolbox built as a codebase exclusively for object detection and instance segmentation tasks. It is built in a modular way with PyTorch implementation. There are numerous methods available for object detection and instance segmentation collected from various well-acclaimed models. It enables quick training and inference with quality. On the other hand, the toolbox contains weights for more than 200 pre-trained networks, making the toolbox an instant solution in the object detection domain.', 'MMDetection behaves as a benchmark with the flexibility to reimplement the existing methods or to develop a new detector with the modules available. The major feature of the toolbox is that it contains simple modular components of a typical object detection framework using which one can build custom pipelines or a custom model. Building a new detector framework on top of an existing framework and comparing its performance is easily possible with this toolbox’s benchmarking capabilities.', 'Since MMDetection is a toolbox containing many pre-built models and each model has its own architecture, this toolbox defines a general architecture that can adapt to any model. This general architecture comprises the following parts:', 'A Backbone is the part of the architecture that transforms the input images into raw feature maps. A Neck connects the Backbone with heads and performs reconfigurations and refinements on the raw feature maps so that heads can further process them. A DenseHead is a part that processes the dense locations of the feature maps fed by Neck. An RoIExtractor is the part of the architecture that identifies the region of interest (RoI) and extracts RoI features from the feature maps. An RoIHead is a part that takes RoI features as its input and makes the predictions such as bounding boxes classification or mask prediction as per the task assigned.', 'The whole network is built as a series of pipelines so that end-to-end training is made simple with any kind of network. During training, the whole network is traversed in the forward and backward directions over iterations.\xa0', 'MMDetection contains high-quality codebases for many popular models and task-oriented modules. Find below the list of fully-built models and custom-adaptable methods that the MMDetection toolbox supports. The list grows continuously with the inclusion of new models and methods.', 'MMDetection runs better with a CUDA GPU runtime in a PyTorch implementation. The following code references the official tutorial of MMDetection. Check for NVIDIA CUDA compiler and GCC with the following commands.', 'Install dependencies required to create the environment.', 'Install MMDetection from the source repository.', 'Create the environment by importing necessary packages.\xa0', 'Load a pre-trained Mask-RCNN model, trained on the COCO dataset, from the official website.', 'Load a checkpoint of the pre-trained model and initialize the detector.', 'Infer the predictions on a sample outdoor image using the loaded detector.', 'Plot the result using the in-built plotting method.', 'Let’s build a model and train it on the KITTI_tiny dataset.', 'Each image is supported with a label annotation file in which the annotations of objects present in the image are provided along with the location. Read the annotation file corresponding to an image sample.', 'The first column indicates the class of the object, and the 5th to 8th columns indicate the bounding boxes.', 'Develop a data generation Python class to convert the data format suitable for training and inference.\xa0', 'Modify the model configurations to suit fast training on the prepared dataset.', 'Train a new detector model with the preprocessed dataset and modified configurations.', 'Test the fully-trained model on a test image.', 'Find the notebook with the above code implementation here.', 'With many competing models, users struggle to choose the right one for their requirements. MMDetection behaves as a benchmarking platform and compares different models under identical conditions.']","' # Check nvcc version\n !nvcc -V\n # Check GCC version\n !gcc --version '
 ' !pip install -U torch==1.5.1+cu101 torchvision==0.6.1+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n # install mmcv-full thus we could use CUDA operators\n !pip install mmcv-full ', ' # Install mmdetection\n !rm -rf mmdetection\n !git clone https://github.com/open-mmlab/mmdetection.git\n %cd mmdetection\n !pip install -e .\n # install Pillow 7.0.0 back in order to avoid bug in colab\n !pip install Pillow==7.0.0 ', ' # Check Pytorch installation\n import torch, torchvision\n print(torch.__version__, torch.cuda.is_available())\n # Check MMDetection installation\n import mmdet\n print(mmdet.__version__)\n # Check mmcv installation\n from mmcv.ops import get_compiling_cuda_version, get_compiler_version\n print(get_compiling_cuda_version())\n print(get_compiler_version()) ', ' !mkdir checkpoints\n !wget -c http://download.openmmlab.com/mmdetection/v2.0/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth \\\n \xa0\xa0\xa0\xa0\xa0\xa0-O checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth ', "" from mmdet.apis import inference_detector, init_detector, show_result_pyplot\n # choose to use a config and initialize the detector\n config = 'configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco.py'\n # setup a checkpoint file to load\n checkpoint = 'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n # initialize the detector\n model = init_detector(config, checkpoint, device='cuda:0') "", "" # Use the detector to do inference\n img = 'demo/demo.jpg'\n result = inference_detector(model, img) "", "" # Let's plot the result\n show_result_pyplot(model, img, result, score_thr=0.3) "", ' # download, decompress the data\n !wget https://download.openmmlab.com/mmdetection/data/kitti_tiny.zip\n !unzip kitti_tiny.zip > /dev/null ', ' # Check the label of a single image\n !cat kitti_tiny/training/label_2/000000.txt ', "" import os.path as osp\n import mmcv\n import numpy as np\n from mmdet.datasets.builder import DATASETS\n from mmdet.datasets.custom import CustomDataset\n @DATASETS.register_module()\n class KittiTinyDataset(CustomDataset):\n \xa0\xa0\xa0\xa0CLASSES = ('Car', 'Pedestrian', 'Cyclist')\n \xa0\xa0\xa0\xa0def load_annotations(self, ann_file):\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0cat2label = {k: i for i, k in enumerate(self.CLASSES)}\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0# load image list from file\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0image_list = mmcv.list_from_file(self.ann_file)\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0data_infos = []\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0# convert annotations to middle format\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0for image_id in image_list:\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0filename = f'{self.img_prefix}/{image_id}.jpeg'\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0image = mmcv.imread(filename)\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0height, width = image.shape[:2]\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0data_info = dict(filename=f'{image_id}.jpeg', width=width, height=height)\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0# load annotations\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0label_prefix = self.img_prefix.replace('image_2', 'label_2')\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0lines = mmcv.list_from_file(osp.join(label_prefix, f'{image_id}.txt'))\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0content = [line.strip().split(' ') for line in lines]\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0bbox_names = [x[0] for x in content]\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0bboxes = [[float(info) for info in x[4:8]] for x in content]\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0gt_bboxes = []\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0gt_labels = []\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0gt_bboxes_ignore = []\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0gt_labels_ignore = []\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0# filter 'DontCare'\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0for bbox_name, bbox in zip(bbox_names, bboxes):\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0if bbox_name in cat2label:\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0gt_labels.append(cat2label[bbox_name])\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0gt_bboxes.append(bbox)\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0else:\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0gt_labels_ignore.append(-1)\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0gt_bboxes_ignore.append(bbox)\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0data_anno = dict(\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0bboxes=np.array(gt_bboxes, dtype=np.float32).reshape(-1, 4),\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0labels=np.array(gt_labels, dtype=np.long),\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0bboxes_ignore=np.array(gt_bboxes_ignore,\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0dtype=np.float32).reshape(-1, 4),\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0labels_ignore=np.array(gt_labels_ignore, dtype=np.long))\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0data_info.update(ann=data_anno)\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0data_infos.append(data_info)\n \xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0return data_infos "", "" from mmcv import Config\n from mmdet.apis import set_random_seed\n cfg = Config.fromfile('./configs/faster_rcnn/faster_rcnn_r50_caffe_fpn_mstrain_1x_coco.py')\n # Modify dataset type and path\n cfg.dataset_type = 'KittiTinyDataset'\n cfg.data_root = 'kitti_tiny/'\n cfg.data.test.type = 'KittiTinyDataset'\n cfg.data.test.data_root = 'kitti_tiny/'\n cfg.data.test.ann_file = 'train.txt'\n cfg.data.test.img_prefix = 'training/image_2'\n cfg.data.train.type = 'KittiTinyDataset'\n cfg.data.train.data_root = 'kitti_tiny/'\n cfg.data.train.ann_file = 'train.txt'\n cfg.data.train.img_prefix = 'training/image_2'\n cfg.data.val.type = 'KittiTinyDataset'\n cfg.data.val.data_root = 'kitti_tiny/'\n cfg.data.val.ann_file = 'val.txt'\n cfg.data.val.img_prefix = 'training/image_2'\n # modify num classes of the model in box head\n cfg.model.roi_head.bbox_head.num_classes = 3\n # We can still use the pre-trained Mask RCNN model though we do not need to\n # use the mask branch\n cfg.load_from = 'checkpoints/mask_rcnn_r50_caffe_fpn_mstrain-poly_3x_coco_bbox_mAP-0.408__segm_mAP-0.37_20200504_163245-42aa3d00.pth'\n # Set up working dir to save files and logs.\n cfg.work_dir = './tutorial_exps'\n # The original learning rate (LR) is set for 8-GPU training.\n # We divide it by 8 since we only use one GPU.\n cfg.optimizer.lr = 0.02 / 8\n cfg.lr_config.warmup = None\n cfg.log_config.interval = 10\n # Change the evaluation metric since we use customized dataset.\n cfg.evaluation.metric = 'mAP'\n # We can set the evaluation interval to reduce the evaluation times\n cfg.evaluation.interval = 12\n # We can set the checkpoint saving interval to reduce the storage cost\n cfg.checkpoint_config.interval = 12\n # Set seed thus the results are more reproducible\n cfg.seed = 0\n set_random_seed(0, deterministic=False)\n cfg.gpu_ids = range(1) "", "" from mmdet.datasets import build_dataset\n from mmdet.models import build_detector\n from mmdet.apis import train_detector\n # Build dataset\n datasets = [build_dataset(cfg.data.train)]\n # Build the detector\n model = build_detector(\n \xa0\xa0\xa0\xa0cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n # Add an attribute for visualization convenience\n model.CLASSES = datasets[0].CLASSES\n # Create work_dir\n mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n train_detector(model, datasets, cfg, distributed=False, validate=True) "", "" img = mmcv.imread('kitti_tiny/training/image_2/000068.jpeg')\n model.cfg = cfg\n result = inference_detector(model, img)\n show_result_pyplot(model, img, result) """
186,on_guide_to_albumentation/,https://analyticsindiamag.com/hands-on-guide-to-albumentation/,"['The performance of a deep learning model is influenced by large datasets and diversity of the dataset. But, there might be situations where the dataset is simply not large enough or diverse enough. In such cases, data augmentation is used. Data augmentation is a technique that enables you to significantly increase the diversity of data available for training models, without actually collecting new data. Although deep learning models come with inbuilt methods to augment the data, these can be inefficient or lacking some required functionality.\xa0', 'In this article, we will learn about an augmentation package for machine learning specifically using the PyTorch framework called Albumentation.', 'Albumentation is a fast image augmentation library and easy to use with other libraries as a wrapper. The package is written on NumPy, OpenCV, and imgaug. What makes this library different is the number of data augmentation techniques that are available. While most of the augmentation libraries include techniques like cropping, flipping, rotating and scaling, albumentation provides a range of very extensive image augmentation techniques like contrast, blur and channel shuffle. Here is the range of augmentations that can be performed.\xa0', 'The above image was downloaded from the official GitHub repository of albumentations library.', 'The reason this library gained popularity in a small period of time is because of the features it offers. Some of the reasons why this library is better are:', 'As mentioned earlier, this library gives a wide range of transformations other than the ones commonly used in other libraries. Let us see how we can implement these transformations on one image.\xa0', 'Let us start with a single transformation. I have chosen a random image from google and will perform a horizontal flip.\xa0', 'The real power of albumentation is in pipelining different transformations for the image at once. Let us implement this pipeline. I will pipeline\xa0', 'As you can see, all of these transformations are applied in a pipeline and in a really quick and efficient way.\xa0', 'Another interesting feature of this is called the OneOf method. Here, the transformation defined in the OneOf block is assigned with probabilities. These are normalized and the transformation with the highest normalized value is selected and applied on the image. This way, there is more efficiency in applying suitable transformations.\xa0', 'In this example above, the one of the method has motion blur, median blur and blur with assigned probabilities. Let us normalize this to see which has the highest probability.\xa0', 'Motion blur = (0.2 )/(0.2+0.3+0.1) =0.3', 'Median blur = (0.3)/(0.2+0.3+0.1)=0.5', 'Blur = (0.1)/(0.2+0.3+0.1)=0.17', 'The above calculations make it clear that the median blur will be applied. This way of pipelining increased the way the CPU is used.\xa0', 'Here is another example where I have applied multiple transformations on an image using albumentation.']","""import random\nimport cv2\nfrom matplotlib import pyplot as plt\nimport albumentations as A\ndef view_transform(image):\n\xa0\xa0\xa0\xa0plt.figure(figsize=(5
 5))\n\xa0\xa0\xa0\xa0plt.axis('off')\n\xa0\xa0\xa0\xa0plt.imshow(image)\nfigure = cv2.imread(‘image.jpg’)\nfigure = cv2.cvtColor(figure, cv2.COLOR_BGR2RGB)\nview_transform(figure)"", ""transform = A.HorizontalFlip(p=0.5)\nrandom.seed(7)\naugmented_image = transform(image=figure)['image']\nview_transform(augmented_image)"", ""transform = A.Compose([\n\xa0\xa0\xa0\xa0A.CLAHE(),\n\xa0\xa0\xa0\xa0A.RandomRotate90(),\n\xa0\xa0\xa0\xa0A.Transpose(),\n\xa0\xa0\xa0\xa0A.Cutout(num_holes=1, max_h_size=16,max_w_size = 16,p=1),\n\xa0\xa0\xa0\xa0A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=.75),\n\xa0\xa0\xa0\xa0A.Blur(blur_limit=3),\n\xa0\xa0\xa0\xa0A.OpticalDistortion(),\n])\nrandom.seed(42)\xa0\naugmented_image = transform(image=figure)['image']\nview_transform(augmented_image)"", ""transform = A.Compose([\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.RandomRotate90(),\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.Flip(),\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.Transpose(),\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.OneOf([\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.MotionBlur(p=.2),\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.MedianBlur(blur_limit=3, p=0.3),\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.Blur(blur_limit=3, p=0.1),\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0], p=0.2),\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.2, rotate_limit=45, p=0.2),\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.OneOf([\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.OpticalDistortion(p=0.3),\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.GridDistortion(p=.1),\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0], p=0.2),\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.OneOf([\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.CLAHE(clip_limit=2),\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.RandomBrightnessContrast(),\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0], p=0.3),\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0A.HueSaturationValue(p=0.3),\n\xa0\xa0\xa0\xa0])\nrandom.seed(42)\xa0\naugmented_image = transform(image=figure)['image']\nview_transform(augmented_image)"""
187,library_for_image_processing/,https://analyticsindiamag.com/hands-on-guide-to-pillow-python-library-for-image-processing/,"['Image processing is performing operations on an image to enhance it or to extract some useful information and to know about the attributes of the image. A systematic study of the image to find out some useful information about that image is known as image processing.', 'Python allows image processing using different libraries and one of them is Pillow, which is an open-source Python Imaging Library that adds image processing capabilities to your Python interpreter.\xa0', 'Pillow offers all types of capabilities like image transformation, rotation, resizing, statistics of the image, etc. It supports the extensive file format and is designed to fast access the data stored in pixels.\xa0', 'In this article, we explore Pillow and learn about its image processing capabilities.', 'Like any other library, we will first install pillow using pip install pillow', 'We will start by importing Image function from PIL. This function is used to process the image and perform operations on it.', 'from PIL import Image', 'We can work on any image of our choice of any format. I am using an image of a bird that I downloaded from google.\xa0', 'img = Image.open(""sparrow.jpg"")', 'img', 'Lets us start the image processing by knowing the mode, size, and the format of the image.\xa0', 'Pillow has inbuilt functions to know the basic properties of the image. Lets us see how we use them to draw some information.', ""print('Format:',img.format)"", ""print('Size',img.size)"", ""print('Mode:', img.mode)"", 'In the cropping section, we will crop the image by creating a box of a particular size and cut it out of the image.\xa0', 'box1 = (200, 200, 800, 800)', 'crop1 = img.crop(box1)', 'crop', 'As you can see according to the size of the box we have a cut out from the image which is the cropped version.', 'As we already know from above that the image is in RGB mode, in this step we will split it into ‘L’ mode i.e. a single channel Luminance.', 'R, G, B = img.split()', 'R', 'The image is split into three parts with a single channel ‘L’.', 'Let us analyze different transformation functions. Starting with resizing the image.', 'resized_image = img.resize((256, 256))', 'resized_image', 'rotated_image = img.rotate(90)', 'rotated_image', 'tran_image = img.transpose(Image.FLIP_TOP_BOTTOM)', 'tran_image', ""img1 = im.convert('1')"", 'img1', 'Let us analyze different filters that are defined under the Pillow library.', 'from PIL import ImageFilter', 'blurred = img.filter(ImageFilter.BLUR)', 'blurred', 'sharped= img.filter(ImageFilter.BLUR)', 'sharped', 'edges = sharped.filter(ImageFilter.FIND_EDGES)', 'edges', 'en_edges = sharped.filter(ImageFilter.EDGE_ENHANCE)', 'en_edges', 'emb = sharped.filter(ImageFilter.EMBOSS)', 'emb', 'This is just an introduction to the image processing capabilities of the Pillow. All these functions are helpful in the initial stage of image processing. There are many advanced functions that can be used at a later stage.']","'from PIL import Image'
 'img = Image.open(""sparrow.jpg"")', 'img', ""print('Format:',img.format)"", ""print('Size',img.size)"", ""print('Mode:', img.mode)"", 'box1 = (200, 200, 800, 800)', 'crop1 = img.crop(box1)', 'crop', 'R, G, B = img.split()', 'R', 'resized_image = img.resize((256, 256))', 'resized_image', 'rotated_image = img.rotate(90)', 'rotated_image', 'tran_image = img.transpose(Image.FLIP_TOP_BOTTOM)', 'tran_image', ""img1 = im.convert('1')"", 'img1', 'from PIL import ImageFilter', 'blurred = img.filter(ImageFilter.BLUR)', 'blurred', 'sharped= img.filter(ImageFilter.BLUR)', 'sharped', 'edges = sharped.filter(ImageFilter.FIND_EDGES)', 'edges', 'en_edges = sharped.filter(ImageFilter.EDGE_ENHANCE)', 'en_edges', 'emb = sharped.filter(ImageFilter.EMBOSS)', 'emb'"
188,for_self_supervised_learning/,https://analyticsindiamag.com/guide-to-vissl-vision-library-for-self-supervised-learning/,"['VISSL\xa0 is a computer VIsion library for state-of-the-art Self-Supervised Learning research. This framework is based on PyTorch. The key idea of this library is to speed up the self-supervised learning process from handling a new design to the evaluation part, VISSL does it all. Following are the characteristic of VISSL framework:', 'Requirements\xa0', 'Installation', 'For google colab notebook, following are the instructions to install VISSL.', 'Check this link for other methods of installation.', 'Quick Start with VISSL', 'This quick-start demo will show the training with VISSL framework and YAML configuration.\xa0', '\xa0\xa0\xa0 To understand the YAML file in more detail, check this link.', '!wget https://dl.fbaipublicfiles.com/vissl/tutorials/run_distributed_engines.py', '\xa0\xa0\xa0 You can verify whether the dataset is registered with VISSL by following commands:', 'The trained model is available at checkpoints/model_final_checkpoint_phase2.torch. This command will dump all the training logs, checkpoints and metrics in ./checkpoints directory.', '\xa0\xa0\xa0 In the above command,', 'You can check the full demo here.', 'Conclusion', 'In this article, we have discussed VISSL framework and its basics. All the advanced tutorials are available at this link.']",'!wget https://dl.fbaipublicfiles.com/vissl/tutorials/run_distributed_engines.py'
189,https://analyticsindiamag.com/torchio_3d_medical_imaging/,https://analyticsindiamag.com/torchio-3d-medical-imaging/,"['TorchIO is a PyTorch based deep learning library written in Python for medical imaging. It is used for 3D medical image loading, preprocessing, augmenting, and sampling. This project is supported by the School of Biomedical Engineering & Imaging Sciences (BMEIS) (King’s College London) and the Wellcome / EPSRC Centre for Interventional and Surgical Sciences (WEISS) (University College London). It has been developed being inspired by a previously existing Tensorflow library NiftyNet, which is no longer maintained and has shifted its development mostly towards a new project named MONAI.\xa0', 'A group of researchers namely Fernando Pérez-García, Rachel Sparks, Sebastien Ourselin released it in their paper named “TorchIO: a Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning”.', 'Paper link – https://arxiv.org/pdf/2003.04696.pdf', 'GitHub repo – https://github.com/fepegar/torchio', 'Documentation – https://torchio.readthedocs.io/', 'To efficiently manage large 3D images, Torchio uses popular medical image processing libraries SimpleITK and NiBabel. It helps in creating complete medical imaging pipelines by providing different features such as intensity and spatial transforms, multiple generic, magnetic-resonance-imaging-specific operations, random affine transformations, domain-specific simulation of intensity artefacts for MRI magnetic field inhomogeneity or k-space motion artefacts.', 'A medical image is a representation of 3D tensor containing voxel data and a 2D matrix of the spatial information. These datasets are stored in the Neuroimaging Informatics Technology Initiative (NIfTI) or Data Imaging and Communications in Medicine (DICOM) formats, and generally read and processed by medical imaging frameworks such as SimpleITK or NiBabel. Deep learning methods typically require large amounts of annotated data, which is hard to gather in case of clinical data due to patient privacy, the financial and time-consuming collecting data and annotating them. Data augmentation can be used to virtually increase the size of the training dataset by applying different transformation techniques to each training sequence while preserving annotations. But this is not the case for 3D image reading and transformations. Moreover, medical images are mostly in grayscale; hence applying colourspace transforms is not an option. Some other options being cropping and scaling, but applying these can be tricky and may destroy important spatial relationships.', 'Installation', 'The latest version is pip installable:\xa0 pip install torchio', 'To get plots, install Matplotlib along with Torchio: pip install torchio[plot]', '3D U-Net to perform brain segmentation from T1-weighted MRI using the Information eXtraction from Images (IXI) dataset, a publicly available dataset with 600 subjects.\xa0', '# importing libraries', '# Load Dataset', 'A subject is a data structure used to store images and associated metadata.', '#transformation', '#spatial transform', 'For complete implementation of transformations and augmentations visit this notebook.', '# normalization – using the transforms to normalize our images intensity.', '# HistogramStandardization', '100%|██████████| 566/566 [00:05<00:00, 100.76it/s]Trained landmarks: [\xa0 0.\xa0 \xa0 \xa0 0.002 \xa0 0.108 \xa0 0.227 \xa0 0.467 \xa0 2.014\xa0 15.205\xa0 34.297\xa0 49.664', '\xa0\xa055.569\xa0 61.178\xa0 74.005 100. \xa0 ]', 'For complete implementation visit here.', '# patch-based to train using image patches randomly extracted from the volumes', '# pretrained model', 'Sagittal L-R    80        Coronal P-A     128     Axial I-S     128', 'For complete implementation visit this notebook.', '3D Slicer', 'Apart from command line tools, TorchIO provides a 3D Slicer extension package which is a no-code platform for quick experimentation and visualization.\xa0', 'End Notes']","'pip install torchio'
 'pip install torchio[plot]'"
190,tool_for_image_processing/,https://analyticsindiamag.com/complete-guide-on-pgmagick-python-tool-for-image-processing/,"['Image Processing is used to extract useful information from an Image or an Image dataset. In image processing, we try and study different aspects of images and scientifically extract and analyze the information provided by the image.', 'Pgmagick is an open-source python library and a wrapper for GraphicsMagick, which is a robust collection of tools and libraries to read, write, and manipulate an image. It supports around 88 formats of image.\xa0\xa0', 'In this article, we will explore Pgmagick and its image processing capabilities. We will see how helpful and easy it is to extract useful information from images.', 'According to the official page of pgmagick currently, there is no official support for pgmagick on the Windows platform but you can download an unofficial binary version for windows.\xa0', 'After downloading the binary version you need to install it using pip install <path>, the path is the location where you downloaded the binary version.', 'We only need to import pgmagick for the image processing functions.', 'from pgmagick import Image as Imn', 'We need to select an image on which we will perform different image processing functions. I have an image of an animal that I am going to use for processing.', ""img =Imapi('tiger.jpg')"", 'Now we will perform some image processing functions on the image we have loaded, pgmagick provides a variety of functions. We will try to cover most of these functions.', 'We will start by detecting the size of the image using the size function.', 'print(img.width, img.height)', 'The Sharpen function sharpens the image which takes the radius value of the', 'sharpness as a parameter. We will store every processed image as a new image. The output image is stored in the same directory where Python is running.', 'img.sharpen(10)', ""img.write('sharped.jpg')"", 'Blur function is used to blur the image. It requires the radius of blur and sigma of the object as the parameter.', ' img.blur(15,5)', ""img.write('blurred.jpg')"", 'Detecting the edges is one of the main functions provided by most of the image processing libraries. Pgmagick also has an edge detection function.', ' img.edge(2)', ""img.write('edges.jpg')"", 'Swirl function swirls the image from the center according to the angle we provide as a parameter.\xa0', 'img.swirl(30)', ""img.write('swirlled.jpg')"", 'Implode function implodes the pixel of the images around the center, we just need to pass the radius of the implode.', 'img.implode(1)', 'img.write(""imploded.jpg"")', 'Gamma function changes the level of the color format. Our image is in RGB format and we will change the color level.', 'img.gamma(1,4,8)', ""img.write('gam.jpg')"", 'Solarize function negates all the pixels and converts the images into a solarized image.', 'img.solarize(20)', ""img.write('solarized.jpg')"", 'Flip and flop functions are used to invert the images from top to bottom and left to right respectively.', 'img.flip()', ""img.write('flipped.jpg')"", ' img.flop()', ""img.write('flopped.jpg')"", 'The Emboss function replaces each pixel of the image to give a 3D effect.\xa0', 'img.emboss(10)', ""img.write('embossed.jpg')"", 'Histogram equalization processes the contrast of the image and sets it according to image histogram. Equalize function is used for histogram equalization.', 'img.equalize()', ""img.write('equalized.jpg')"", 'The spread function is used to displace the pixel of the image according to a threshold value and shear function is used to slide the image along the x and the y-axis we just need to pass the degree of sliding for both x and y-axis.', 'img.spread(10)', ""img.write('spreaded.jpg')"", 'img.shear(23,56)', ""img.write('sheared.jpg')"", 'We can use pgmagick for image creation purposes also. It supports different types of image creation functions.', 'from pgmagick.api import Image as Imapi', ""img = Imapi((200, 200), 'aqua')"", ""img.write('back_aq1.jpg')"", 'We can create animated gifs using pgmagick but it has certain limitations while working on the windows system. As we are using the binary file which is not official some of the functions are not properly working.', 'from pgmagick import Image, ImageList, Geometry, Color', 'imgs = ImageList()', ""for color in ('violet', 'black', 'aqua', 'magenta', 'yellow'):"", '\xa0\xa0\xa0\xa0imgs.append(Image(Geometry(400, 400), Color(color)))', 'imgs.animationDelayImages(50)', 'imgs.scaleImages(Geometry(100, 100))', ""imgs.writeImages('animate.gif')\xa0"", 'The code above will create an animated gif with the name ‘animate.gif’ in your system.']","'from pgmagick import Image as Imn'
 ""img =Imapi('tiger.jpg')"", 'print(img.width, img.height)', 'img.sharpen(10)', ""img.write('sharped.jpg')"", 'img.blur(15,5)', ""img.write('blurred.jpg')"", ' img.edge(2)', ""img.write('edges.jpg')"", 'img.swirl(30)', ""img.write('swirlled.jpg')"", 'img.implode(1)', 'img.write(""imploded.jpg"")', 'img.gamma(1,4,8)', ""img.write('gam.jpg')"", 'img.solarize(20)', ""img.write('solarized.jpg')"", 'img.flip()', ""img.write('flipped.jpg')"", 'img.flop()', ""img.write('flopped.jpg')"", 'img.emboss(10)', ""img.write('embossed.jpg')"", 'img.equalize()', ""img.write('equalized.jpg')"", 'img.spread(10)', ""img.write('spreaded.jpg')"", 'img.shear(23,56)', ""img.write('sheared.jpg')"", 'from pgmagick.api import Image as Imapi', ""img = Imapi((200, 200), 'aqua')"", ""img.write('back_aq1.jpg')"", 'from pgmagick import Image, ImageList, Geometry, Color', 'imgs = ImageList()', ""for color in ('violet', 'black', 'aqua', 'magenta', 'yellow'):"", 'imgs.append(Image(Geometry(400, 400), Color(color)))', 'imgs.animationDelayImages(50)', 'imgs.scaleImages(Geometry(100, 100))', ""imgs.writeImages('animate.gif')"""
191,opencv_inspired_pytorch_framework/,https://analyticsindiamag.com/guide-to-kornia-an-opencv-inspired-pytorch-framework/,"['Kornia is an open-source Python library inspired by OpenCV designed to handle generic Computer Vision tasks. It was introduced by Edgar Riba, Dmytro Mishkin, Daniel Ponsa, Ethan Rublee and Gary Bradski in October, 2019 (research paper).\xa0', 'Kornia leverages PyTorch library at its backend in terms of model’s efficiency and reverse-mode auto-differentiation for defining and computing complex functions’ gradients. It comprises a subset of packages having operators that act as an input to neural networks for performing a wide range of tasks such as image transformations, depth estimation, epipolar geometry, filtering and edge-detection applicable on high-dimensional tensors. Unlike conventional CPU-based CV libraries such as torchvision and scikit-image, several standard deep learning functions can be implemented on GPUs using Kornia.\xa0', 'Kornia bridges the gap between two simple yet powerful libraries namely, OpenCV and PyTorch. Though solely based on traditional CV solutions like torchvision, tf.image, PIL and skimage, it enables differentiable programming for CV applications by utilizing the crucial properties of PyTorch like GPU hardware acceleration, differentiability and distributed data-flows.', 'The following table compares Kornia with some important CV libraries/modules:', 'Source of above summary: Research paper', 'Here, we demonstrate three use cases of Kornia – blurring a custom image, changing its color space, and adjusting its colors. The code is test on Google colab with Python 3.7.10 and Kornia 0.4.1 versions.', 'We have used the following image for demonstration:', 'Image source', 'Step-wise explanation of the code is as follows:', 'First, install the library using pip command.', '!pip install kornia', 'Import required libraries and modules', 'gaussian = kornia.filters.GaussianBlur2d((11, 11), (10.5, 10.5))', 'Where, (11,11) is the size of the kernel and (1.05,10.5) is the standard deviation of the kernel', 'blur_image: torch.tensor = gaussian(tensor_image.float())', 'final_blur_image: np.ndarray = kornia.tensor_to_image(blur_image.byte())', ""bgr_image: np.ndarray = cv2.imread('img2.jpg', cv2.IMREAD_COLOR)"", 'tensor_bgr: torch.Tensor = kornia.image_to_tensor(bgr_image, keepdim=False)', ""bgr_image: np.ndarray = cv2.imread('img2.jpg', cv2.IMREAD_COLOR)"", 'tensor_bgr: torch.Tensor = kornia.image_to_tensor(bgr_image)', 'Convert the image from BGR to RGB', 'tensor_rgb: torch.Tensor = kornia.bgr_to_rgb(tensor_bgr)', 'Define a function to create a batch of images', 'Display the batch of RGB images', 'imshow(tensor_rgb)', 'To get in-depth understanding of the Kornia library, refer to the following web links:']","'!pip install kornia'
 'gaussian = kornia.filters.GaussianBlur2d((11, 11), (10.5, 10.5))', 'blur_image: torch.tensor = gaussian(tensor_image.float())', 'final_blur_image: np.ndarray = kornia.tensor_to_image(blur_image.byte())', ""bgr_image: np.ndarray = cv2.imread('img2.jpg', cv2.IMREAD_COLOR)"", 'tensor_bgr: torch.Tensor = kornia.image_to_tensor(bgr_image, keepdim=False)', ""bgr_image: np.ndarray = cv2.imread('img2.jpg', cv2.IMREAD_COLOR)"", 'tensor_bgr: torch.Tensor = kornia.image_to_tensor(bgr_image)', 'tensor_rgb: torch.Tensor = kornia.bgr_to_rgb(tensor_bgr)', 'imshow(tensor_rgb)'"
192,text_detection_in_images/,https://analyticsindiamag.com/hands-on-tutorial-on-easyocr-for-scene-text-detection-in-images/,"['Optical Character Reader or Optical Character Recognition (OCR) is a technique used to convert the text in visuals to machine-encoded text. These visuals could be printed documents(invoices, bank statements, restaurant bills), or placards(sign-boards, traffic symbols ), or handwritten text. Converting these visuals to text could be handy for information extraction, scanning books or documents and making PDFs, storing in the system or working with it online such as text to speech(this could be of great help to visually impaired people), used widely in autonomous vehicles to interpret various things. OCR is an emerging technology that is enhancing for better accuracy in performance.', 'EasyOCR is a python package that allows the image to be converted to text. It is by far the easiest way to implement OCR and has access to over 70+ languages including English, Chinese, Japanese, Korean, Hindi, many more are being added. EasyOCR is created by the Jaided AI company.', 'In this article, we will be discussing how to implement OCR using EasyOCR. Let’s start by discussing EasyOCR and installing it for our use.', 'EasyOCR is built with Python and Pytorch deep learning library, having a GPU could speed up the whole process of detection. The detection part is using the CRAFT algorithm and the Recognition model is CRNN. It is composed of 3 main components, feature extraction (we are currently using Resnet), sequence labelling (LSTM) and decoding (CTC). EasyOCR doesn’t have many software dependencies, it can directly be used with its API.', '\xa0pip install easyocr', 'Now we’re ready to start our detection process.', 'EasyOCR can process multiple languages at the same time provided they are compatible with each other.', 'The Reader class is the base class for EasyOCR which contains a list of language codes and other parameters such as GPU that is by default set to True. This needs to run only once to load the necessary models. Model weights are automatically downloaded or can be manually downloaded as well.', 'Then comes the readtext method which is the main method for Reader class.', 'Let’s read text from the below image:', 'The output shows 4 bounding box coordinates(x,y) of the text along with the identified text and confidence score.', 'This kind of output could be difficult for non-developers to read hence we can pass the detail parameter as 0 for simpler output.', ""result = reader.readtext('/content/aim.png', detail = 0)"", ""OUTPUT - ['Analytics India', 'MAGAZINE']"", 'In the code, we’ve set the language as ‘en’ meaning English.\xa0\xa0', 'Images can be directly read from URLs also:', ""res = reader.readtext('https://3v6x691yvn532gp2411ezrib-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/imagetext09.jpg')"", 'Another important parameter is the paragraph, by setting it True EasyOCR will combine the results.', 'EasyOCR works with BGR images with OpenCV unlike tesseract which needs to be converted in RGB\xa0', 'In the above code, ‘ja’ stands for Japanese.', 'This is a Spanish Traffic symbol ALTO which means stop\xa0']","'\xa0pip install easyocr'
 ""result = reader.readtext('/content/aim.png', detail = 0)"", ""OUTPUT - ['Analytics India', 'MAGAZINE']"", ""res = reader.readtext('https://3v6x691yvn532gp2411ezrib-wpengine.netdna-ssl.com/wp-content/uploads/2019/05/imagetext09.jpg')"""
193,explainable_clustering_using_classix/,https://analyticsindiamag.com/how-to-perform-fast-and-explainable-clustering-using-classix/,"['A cluster is a group of homogeneous objects; in other words, objects with similar properties are collected in one cluster, while things with dissimilar properties are collected in another. Clustering is the process of categorizing objects into a number of groups in which the objects in each group are substantially similar to those in other groups. Various clustering algorithms have been used so far like K-Means clustering, mean-shift clustering, etc. But in this article, we will discuss the toolbox, named CLASSIX, for clustering which does clustering more precisely, fast but also explains how it is carried. Below are the major points listed that are to be discussed in this post.\xa0\xa0\xa0', 'Let’s first discuss clustering.', 'Clustering is the process of putting items together so that members of the same group (cluster) be more common with their peers than members of other groups. Clustering looks at all of the input data and is commonly used in machine learning (ML) methods.', 'When machine learning practitioners create a cluster, they examine all of the different data points and group them together based on what features they have in common with other data. The algorithm determines the clustering strategy.', 'Clustering procedures can involve calculating the average distance between data points in dimensional spaces, counting the number of intervals for each set of data, predicting the number of clusters, or basing them on dense data areas. Clustering produces explicit links between data points, as well as explanations for why each data point belongs in its cluster.', 'Distance-based clustering algorithms, such as k-means, take into account the pairwise distance between points when deciding whether or not they should be grouped together. DBSCAN and other density-based clustering algorithms adopt a more global approach, assuming that data occurs in continuous zones of high density surrounded by low-density regions.\xa0', 'Many density-based clustering methods have the advantage of being able to handle clusters of any shape without having to define the number of clusters in advance. They, on the other hand, usually necessitate greater parameter adjustment.', 'CLASSIX is a method that shares characteristics of both distance and density-based methods. The approach is divided into two stages: aggregation and merging. Data points are sorted along with their first principal component and then grouped using a greedy aggregation technique during the aggregation phase.', 'Sorting is essential for traversing the data with almost linear complexity, as long as the number of pairwise distance computations is modest. While the initial sorting requires an average-case complexity, it is only performed on scalar values regardless of data point dimensionality. As a result, the cost of this initial sorting is almost insignificant when compared to calculations on full-dimensional data.', 'Following the aggregation step, overlapping groups are merged into clusters using either a distance or density-based criterion. Although the density-based merging criterion produces marginally better clusters than the distance-based criterion, the latter is significantly faster. CLASSIX is controlled by only two parameters, and its setup is simple.\xa0', 'In summary, the radius parameter determines the least permissible cluster size, whereas the minPts parameter specifies the tolerance for grouping in the aggregation phase. This is identical to the settings used in DBSCAN, however, CLASSIX does not run spatial range searches for each data point due to the initial sorting of the data points.', 'In this section, we’ll perform clustering on the IRIS dataset by removing the target column and making a completely unsupervised problem. As discussed earlier we are going to use the method CLASSIX to cluster data, here I am setting the radius to 0.35, groping method to density, minimum points in clustering to be 3 points.', 'Now let’s first quickly install, import dependencies and prepare the dataset.\xa0', 'Now we just need to call the function by setting the parameters as above mentioned and fit the data.\xa0', 'After the fitment, this method will give you clustering results as below.', 'As we have set minPts to 3 the algorithm will agglomerate the cluster having points lesser than the minPts to the bigger clusters. Now let’s check this visually.', 'Apart from this, this algorithm is so capable that it can give a brief explanation of how it has clustered the data by using method .explain().\xa0']","""# install library\n!pip install ClassixClustering \n\n# imports\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom classix import CLASSIX\n\n# prepare data\ndata = pd.read_csv('/content/IRIS.csv')\ndata.drop(['species']
 inplace=True, axis=1)"", ""# initailize the clustering\nclx = CLASSIX(radius=0.35, minPts=3,  group_merging='density')\n# fitting the data\nclx.fit(data)\n"", '# visualize the clusters\nplt.figure(figsize=(5,5))\nplt.scatter(data.values[:,0], data.values[:,2], c=clx.labels_)\nplt.show()\n', '# explaining the clusters\nclx.explain()\n'"
194,to_clarans_clustering_algorithm/,https://analyticsindiamag.com/comprehensive-guide-to-clarans-clustering-algorithm/,"['CLARANS (Clustering Large Applications based on RANdomized Search) is a Data Mining algorithm designed to cluster spatial data. We have already covered K-Means and K-Medoids clustering algorithms in our previous articles. This article talks about another clustering technique called CLARANS along with its Pythonic demo code. CLARANS was introduced by Raymond T. Ng and Jiawei Han \xa0of IEEE Computer Society (research paper).', 'CLARANS is a partitioning method of clustering particularly useful in spatial data mining. We mean recognizing patterns and relationships existing in spatial data (such as distance-related, direction-relation or topological data, e.g. data plotted on a road map) by spatial data mining.\xa0', 'As mentioned in our K-Medoids algorithm’s article, the K-Medoids clustering technique can resolve the limitation of the K-Means algorithm of being adversely affected by noise/outliers in the input data. But K-Medoids proves to be a computationally costly method for considerably large values of ‘k’ (number of clusters) and large datasets.', 'The CLARA algorithm was introduced as an extension of K-Medoids. It uses only random samples of the input data (instead of the entire dataset) and computes the best medoids in those samples. It thus works better than K-Medoids for crowded datasets. However, the algorithm may give wrong clustering results if one or more sampled medoids are away from the actual best medoids.', 'CLARANS algorithm takes care of the cons of both K-Medoids and CLARA algorithms besides dealing with difficult-to-handle data mining data, i.e. spatial data. It maintains a balance between the computational cost and the influence of data sampling on clusters’ formation.', 'Image source: Research paper', 'Here’s a demonstration of using CLARANS algorithm on the sklearn library’s Breast Cancer Wisconsin dataset. Though the dataset is primarily used for binary classification tasks, we use it to show how CLARANS algorithm can form separate clusters of the constituent data points falling under one of the two target categories (‘malignant’ or ‘benign’). The pyclustering data mining library has been used here for Pythonic implementation of CLARANS. The code has been implemented using Google colab with Python 3.7.10 and pyclustering 0.10.1.2 versions. Step-wise explanation of the code is as follows:', '!pip install pyclustering', 'Sample condensed output:', 'bc_data = bc_dataset.data', 'Convert the dataset from a numpy array to a list because a list of lists is fed as an input to the CLARANS’ implementation of the pyclustering library.', 'bc_data = bc_data.tolist()', 'Display the data in the form of a list\xa0', 'print(bc_data[:5])', 'Sample condensed output:', '[[17.99, 10.38, 122.8, 1001.0, 0.1184, 0.2776, 0.3001, 0.1471, 0.2419, 0.07871, 1.095, 0.9053, 8.589, 153.4, 0.006399, 0.04904, 0.05373, 0.01587, 0.03003, 0.006193, 25.38, 17.33, 184.6, 2019.0, 0.1622, 0.6656, 0.7119, 0.2654, 0.4601, 0.1189], [20.57, 17.77, 132.9, 1326.0, 0.08474, 0.07864, 0.0869, 0.07017, 0.1812, 0.05667, 0.5435, 0.7339, 3.398, 74.08, 0.005225, 0.01308, 0.0186, 0.0134, 0.01389, 0.003532, 24.99, 23.41, 158.8, 1956.0, 0.1238, 0.1866, 0.2416, 0.186, 0.275, 0.08902],…', 'Execution time :\xa0 80.50073736700006\xa0', 'clst = clarans_obj.get_clusters()', 'med = clarans_obj.get_medoids()', 'Sample condensed output:']","'!pip install pyclustering'
 'bc_data = bc_dataset.data', 'bc_data = bc_data.tolist()', 'print(bc_data[:5])', 'Execution time :\xa0 80.50073736700006\xa0', 'clst = clarans_obj.get_clusters()', 'med = clarans_obj.get_medoids()'"
195,goal_reinforcement_learning_environment/,https://analyticsindiamag.com/exploring-panda-gym-a-multi-goal-reinforcement-learning-environment/,"['With the latest breakthroughs in artificial intelligence and more developing research every day, it \xa0 is very believable that intelligent and self-sufficient machines are just on the horizon of arrival. Machines these days can understand verbal commands, distinguish between pictures, drive cars and play games, sometimes even better than an average human does. One can only wonder how much longer, and maybe it’ll walk among us?\xa0', 'But, in developing an artificially intelligent machine, Reinforcement Learning and the learning environment it is trained in play a major role. The development environment used to train for machine learning is as important as the machine learning methods used to solve the predictive modeling problem. The Environment makes up for the basic and fundamental elements in a reinforcement learning problem. Therefore, it is important to understand the underlying environment with which the RL agent is to interact. This helps to come up with the right design and learning technique for the agent being delivered.\xa0', 'The environment is the Agent’s world in which it lives, and the agent interacts with the environment by performing some action, but it does not have the right to influence the rules or dynamics of the environment by performing those actions. So, for example, just like humans are an agent in the earth’s environment and are confined with the laws. We can interact with the environment with our actions but cannot change the laws. The environment also gives a Reward to the agent; a scalar value returned that acts as feedback for the agent informing him whether its action was good or bad. Within Reinforcement Learning, multiple paradigms attain a winning strategy, i.e., making the agent perform the desired action in multiple ways. In complex situations, calculating the exact winning strategy or reward-value function becomes hard, especially when the agents start learning from interactions rather than the prior-gained experience.\xa0', 'There are several types of learning environments present. The different types of Reinforcement Learning Environments are as follows:', 'The gym is an open-source toolkit for developing and comparing reinforcement learning algorithms. What makes it easier to work with is that it makes it easier to structure your environment using only a few lines of code and compatible with any numerical computation library, such as TensorFlow or Theano. The gym library is a collection of test problems and environments that one can use to train and develop stronger reinforcement learning models. The present environments have a shared interface, allowing to write general algorithms as well. Furthermore, it provides a wide variety of simulated environments such as Atari games, board games, 2D and 3D physical simulations, and much more, so you can train multiple agents, compare them, or develop new Machine Learning algorithms for Reinforcement Learning problems. OpenAI is an artificial intelligence research company that Elon Musk partly funds. Its goal is to promote and develop friendly AI systems that will benefit humanity and work towards its betterment, rather than exterminating it!', 'Panda-Gym, is an open-source library that provides a set Reinforcement Learning (RL) environment for the Franka Emika Panda robot integrated with OpenAI Gym. The Robot Simulation Environment consists of five tasks: reach, push, slide, pick & place and stack. It follows a Multi-Goal RL framework, allowing the use of goal-oriented RL algorithms. To foster open research, it also makes use of the open-source physics engine PyBullet. The implementation chosen for this package allows us to define new tasks easily or even create new robots.', 'The environments presented consist of a Panda robotic arm known as Franka Emika1, which is already widely used in simulation and real-life academic works. It has been designed with\xa0 7 degrees of freedom and a parallel finger gripper to perform tasks. The robot is simulated with the PyBullet physics engine, which, being open-source, helps show simulation performance. Furthermore, the environments are integrated with OpenAI Gym, allowing all learning algorithms based on the API.\xa0', 'The simulation task consists of a challenge in moving either the gripper or objects to a target position. A task is considered as completed when the distance between the entity to move and the target position is less than 5 cm. The five tasks presented can be further tuned with an increasing level of difficulty. In the PandaReach-v1 task, a target position must be reached with the gripper. This target position is randomly generated in a volume of 30 cm × 30 cm × 30 cm. For PandaPush-v1,a cube placed on a table must be pushed to a target position on the table surface while the gripper is blocked. Here the target position and the initial position of the cube are randomly generated in a 30 cm × 30 cm square around the neutral position of the robot. PandaSlide-v1 simulation task consists of a\xa0 flat cylinder that must be moved to a target position on the surface of a table while the gripper is blocked. The target position is randomly generated in a 50 cm × 50 cm square located 40 cm in front of the neutral position of the robot.\xa0', 'Since the target positions are out of reach of the robot, it is necessary to give an impulse to the object instead of just pushing it. For the PandaPickAndPlace-v1 simulation, a cube must be brought to a target position generated in a volume of 30 cm×30 cm×20 cm above the table. To lift the cube, it is necessary to pick it up with the fingers of the gripper. PandaStack-v1 Two cubes must be stacked at a target position on the table surface. The target position is generated in a square of 30 cm × 30 cm. The stacking must be done correctly: the red cube must be under the green cube. All these simulation challenges are still under research and yet to be completely solved with a perfect solution.\xa0', 'Image Source', 'In this article, we will try to perform two simulations from the Panda Gym Challenge and understand what it takes to develop and set up the environment. The following implementation is inspired by the creators of panda gym, whose official website link can be found here.\xa0', 'To get started, we will first install the panda-gym library; you can run the following code to do so,', 'Now we will be importing the dependencies required to set up the environment,', 'Fetch, Pick and Place', 'To set up the environments, you can run the following lines of code,', 'The Hyperparameters can be further tuned according to the required performance; here, we will just perform a basic demo simulation.', 'Next up, we will install the numpngw library, a python package that defines the function write_png that writes a NumPy array to a PNG file and write_apng to a sequence f arrays of an animated PNG (APNG) file.\xa0', 'Displaying the results,\xa0', 'As we can observe, the gripper moves the block! Furthermore, you can see the two positions of the block. Although the simulation might not be very clear, it can be further hyperparameter tuned or run on an even better computational system for better render performance.', 'We can do the same for another simulation task of gripper slide.', 'Fetch and Slide', 'The rendering time and other learning parameters and environment can be further set up accordingly. This tool is very satisfying for testing deep reinforcement learning algorithms. However, some points can be limited, such as limitations in the gripper control, as it can only be controlled by high-level actions such as grasp and move. Additional work is required to allow the deployment of a policy learned in simulation. Also, the simulation is not completely realistic; the main concern is the gripper’s shape for picking up subjects in the environment.']","'!pip install panda-gym'
 '#importing dependencies\n \nimport gym\nimport panda_gym\n', ""#assigning the simulation task to environment\n \nenv = gym.make('PandaPickAndPlace-v1')\nstate = env.reset()\n \n#setting the environment\ndone = False\n \n#rendering agent learnings\nimages = [env.render('rgb_array')]\nwhile not done:\n    action = env.action_space.sample()\n    state, reward, done, info = env.step(action)\n    images.append(env.render('rgb_array'))\n \nenv.close()\n"", ""#installing numpngw\n!pip3 install numpngw\nfrom numpngw import write_apng\n \nwrite_apng('anim.png', images, delay = 100) # real-time rendering = 40 ms between frames\n"", '#rendering the simulation\n \nfrom IPython.display import Image\n \nImage(filename=""anim.png"")', 'import gym\nimport panda_gym\n \nenv = gym.make(\'PandaSlide-v1\')\nstate = env.reset()\n \ndone = False\nimages = [env.render(\'rgb_array\')]\nwhile not done:\n    action = env.action_space.sample()\n    state, reward, done, info = env.step(action)\n    images.append(env.render(\'rgb_array\'))\n \nenv.close()\n\n!pip3 install numpngw\nfrom numpngw import write_apng\n \nwrite_apng(\'anim.png\', images, delay = 70) # real-time rendering = 40 ms between frames\n\nfrom IPython.display import Image\n \nImage(filename=""anim.png"")'"
196,model_based_reinforcement_learning/,https://analyticsindiamag.com/complete-guide-to-mbrl-python-tool-for-model-based-reinforcement-learning/,"['The recent revolution of big data and its technologies has changed our world’s way of looking at the future. With developments in such technologies, many more aspects of Machine Learning & Artificial Intelligence have boomed. The things that can be implemented with such tremendous amounts of data are surreal and have opened many doors for creativity and innovation. Artificial Intelligence, in particular, has seen a lot of developments that have led to the industrial revolution and automation. With Artificial Intelligence, creating automated self-learning systems has become a lot easier than expected. However, one of the most intimidating discoveries in self-learning systems, making the use of Data Science, has been a topic known as Reinforcement Learning. Reinforcement Learning has been one of the hottest topics in recent years. But, what exactly is Reinforcement Learning? Let’s Have a look at it!', 'Reinforcement learning is the method of training machine learning models to enable them to make a sequence of decisions. The learning agent is created to achieve a goal in an uncertain and potentially complex, unknown environment. In Reinforcement Learning, through artificial intelligence techniques, the computer employs trial and error methods to solve the assigned problem. To get the machine to do what the programmer wants, the artificial intelligence gets rewards or penalties for the actions it performs. Its goal would be to maximize the total reward earned. Although the programmer sets the reward policy or the rules, the model is given no hints or suggestions for solving the problem. Now, it’s up to the model to figure out how to perform the task to maximize the reward, Making assumptions from totally random trials and finishing with sophisticated tactical solutions and newly learned skills. By leveraging the power of search and many trials, reinforcement learning is currently the most effective way to hint at a machine’s creativity.', 'Reinforcement learning is very different from supervised learning. In supervised learning, the training data has the answers in it, so the model is trained with the correct answer itself, whereas in reinforcement learning, there is no answer, but the reinforcement agent decides what to do to perform the given task and find a solution to the assigned problem. Due to the absence of a training dataset, it is bound to learn from its experience. Reinforcement Learning solves a specific problem where decision making is sequential, but the goal is long-term, such as automated game-playing, robotics, etc. Here we do not need to pre-program the agent, and it learns from its own experience and failures without any human intervention. The agent continues to do three things in particular to learn and explore the environment: take action, change state, remain in the same state, and get feedback.\xa0', 'Model-based Reinforcement Learning (MBRL) for continuous control is an area of research investigating machine learning agents explicitly modelling themselves by interacting with the world. MBRL can learn rapidly from a limited number of trials and enables programmers to integrate specialized domain knowledge into the learning agent about how the world environment works. The library MBRL-Lib is an Open-source Python Library created to provide shared foundations, tools, abstractions, and evaluations for continuous-action MBRL.\xa0', 'The MBRL library helps in performing and creating:\xa0', 'MBRL algorithms involve the interplay of multiple components whose internal operations are often hidden from each other. The algorithm has been written in a way that makes it possible to alter some of the component choices, for example: replace the trajectory sampling method without affecting the others. The MBRL-Lib follows a “mix-and-match” approach, where new algorithms or variants of existing ones can be easily written and tested without a lot of code being involved. By minimizing the amount of code required for users of the library and making heavy use of configuration files and a large set of utilities to perform common operations, it uplifts performance and can accommodate new functionality. Using the library has been made as frictionless as possible. It provides high performance in sample complexity and running time, and well-tuned hyperparameters have been provided for creating algorithms. More importantly, when performance gaps exist, future improvements are provided to the components for the end-users.\xa0\xa0', 'In this article, we will try to explore the functionalities of the MBRL-Lib library and create a demo Reinforcement Learning model where we will :', 'The following code implementation is partially inspired by the official MBRL paper and Github repository. You can find the link to MBRL-Lib’s official git repository through the link here.\xa0', 'The first step would be to install the necessary libraries required to create the model, you can use the following code to do so,', 'Here we are using omegaconf, which will provide us support for merging configurations from multiple sources. It is important to note that to help the MBRL library execution, it requires the latest version of Pytorch, so we have installed that as well. Using MBRL-Lib, we will use an ensemble of neural networks (NNs) modelling Gaussian distributions and a trajectory optimizer agent that uses CEM. We will also rely on several of the utilities available in the mbrl.util module. Finally, we will wrap the dynamics model into a gym environment to plan action sequences.', 'Now that we have installed the necessary components, we will be importing them one by one.', 'The ensemble model is trained to predict the environment’s dynamics, and the planner tries to find high-reward trajectories over the model dynamics.', 'First, we will instantiate the environment and specify the reward function and termination function with the gym environment wrapper and some utility objects. The termination function will tell the wrapper if an observation should cause a learning episode to end or not. A reward function is used to compute the value of the reward given an observation.\xa0', 'MBRL-Lib uses Hydra to manage its configurations. You can think of the configuration object as a dictionary with key/value pairs and equivalent attributes that specify the model and its algorithmic options.\xa0', 'Using the following two lines of code, we will create a wrapper for 1-D transition reward models.', 'Creating a replay buffer further,', 'Lets now pass an agent of type planning.RandomAgent to generate the actions,', 'Further configuring our agent deployed,', 'Now that we have created a model and an agent, we can now run a simple loop, and a few function calls. We will be using a Probabilistic Dynamics Model called PETS. The first code block creates a callback to pass to the model trainer to accumulate the training losses and validation scores observed & the second block is just a utility function to update the agent’s visualization.', 'Further creating the training environment,\xa0', 'As we can see, we can track the reward received by the model for each number of trials.', 'Let’s check the training loss and validation score,', 'In this article, we tried to learn about Reinforcement Learning and how it works. We also explored the Model-Based Reinforcement Learning Library, its uses and how it can be used to create a Reinforcement Learning model. The following implementation can be found as a Colab notebook, which can be accessed from the link here.\xa0']","'!pip install omegaconf \n!pip install mbrl\n!pip install matplotlib==3.1.3\n!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n'
 'from IPython import display\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport omegaconf\n \nimport mbrl.env.cartpole_continuous as cartpole_env\nimport mbrl.env.reward_fns as reward_fns\nimport mbrl.env.termination_fns as termination_fns\nimport mbrl.models as models\nimport mbrl.planning as planning\nimport mbrl.util.common as common_util\nimport mbrl.util as util\n \n \n%load_ext autoreload\n%autoreload 2\n \nmpl.rcParams.update({""font.size"": 16})\n \ndevice = \'cuda:0\' if torch.cuda.is_available() else \'cpu\'\n', '#providing seed\nseed = 0\nenv = cartpole_env.CartPoleEnv()\nenv.seed(seed)\nrng = np.random.default_rng(seed=0)\ngenerator = torch.Generator(device=device)\ngenerator.manual_seed(seed)\nobs_shape = env.observation_space.shape\nact_shape = env.action_space.shape\n \n# Evaluate the true rewards given an observation \nreward_fn = reward_fns.cartpole\n# Know if an observation should make the episode end\nterm_fn = termination_fns.cartpole\n', 'trial_length = 200\nnum_trials = 10\nensemble_size = 5\n \ncfg_dict = {\n    # dynamics model configuration\n    ""dynamics_model"": {\n        ""model"": {\n            ""_target_"": ""mbrl.models.GaussianMLP"",\n            ""device"": device,\n            ""num_layers"": 3,\n            ""ensemble_size"": ensemble_size,\n            ""hid_size"": 200,\n            ""use_silu"": True,\n            ""in_size"": ""???"",\n            ""out_size"": ""???"",\n            ""deterministic"": False,\n            ""propagation_method"": ""fixed_model""\n        }\n    },\n    # options for training the dynamics model\n    ""algorithm"": {\n        ""learned_rewards"": False,\n        ""target_is_delta"": True,\n        ""normalize"": True,\n    },\n    # these are experiment specific options\n    ""overrides"": {\n        ""trial_length"": trial_length,\n        ""num_steps"": num_trials * trial_length,\n        ""model_batch_size"": 32,\n        ""validation_ratio"": 0.05\n    }\n}\ncfg = omegaconf.OmegaConf.create(cfg_dict)#using omegaconf', '# Create a 1-D dynamics model for this environment\ndynamics_model = common_util.create_one_dim_tr_model(cfg, obs_shape, act_shape)\n \n# Create a gym-like environment to encapsulate the model\nmodel_env = models.ModelEnv(env, dynamics_model, term_fn, reward_fn, generator=generator)\n', 'replay_buffer = common_util.create_replay_buffer(cfg, obs_shape, act_shape, rng=rng)\n', 'common_util.rollout_agent_trajectories(\n    env,\n    trial_length, # initial exploration steps\n    planning.RandomAgent(env),\n    {}, # keyword arguments to pass to agent.act()\n    replay_buffer=replay_buffer,\n    trial_length=trial_length\n)\n \nprint(""# samples stored"", replay_buffer.num_stored)\n\n# samples stored 200\n', 'agent_cfg = omegaconf.OmegaConf.create({\n\n    # Creating a class that evaluates trajectories and picks the best one\n    ""_target_"": ""mbrl.planning.TrajectoryOptimizerAgent"",\n    ""planning_horizon"": 15,\n    ""replan_freq"": 1,\n    ""verbose"": False,\n    ""action_lb"": ""???"",\n    ""action_ub"": ""???"",\n\n    # Defining optimizer to generate and choose a trajectory\n    ""optimizer_cfg"": {\n        ""_target_"": ""mbrl.planning.CEMOptimizer"",\n        ""num_iterations"": 5,\n        ""elite_ratio"": 0.1,\n        ""population_size"": 500,\n        ""alpha"": 0.1,\n        ""device"": device,\n        ""lower_bound"": ""???"",\n        ""upper_bound"": ""???"",\n        ""return_mean_elites"": True\n    }\n})\n\nagent = planning.create_trajectory_optim_agent_for_model(\n    model_env,\n    agent_cfg,\n    num_particles=20\n)\n', 'train_losses = []\nval_scores = []\n \ndef train_callback(_model, _total_calls, _epoch, tr_loss, val_score, _best_val):\n    train_losses.append(tr_loss)\n    val_scores.append(val_score.mean().item())# returns val score per ensemble model\n \n#setting the hyperparameters\n \ndef update_axes(_axs, _frame, _text, _trial, _steps_trial, _all_rewards, force_update=False):\n    if not force_update and (_steps_trial % 10 != 0):\n        return\n    _axs[0].imshow(_frame)\n    _axs[0].set_xticks([])\n    _axs[0].set_yticks([])\n    _axs[1].clear()\n    _axs[1].set_xlim([0, num_trials + .1])\n    _axs[1].set_ylim([0, 200])\n    _axs[1].set_xlabel(""Trial"")\n    _axs[1].set_ylabel(""Trial reward"")\n    _axs[1].plot(_all_rewards, \'bs-\')\n    _text.set_text(f""Trial {_trial + 1}: {_steps_trial} steps"")\n    display.display(plt.gcf())  \n    display.clear_output(wait=True)\n', '# Create a trainer for the model\nmodel_trainer = models.ModelTrainer(dynamics_model, optim_lr=1e-3, weight_decay=5e-5)\n \n# Create visualization objects\nfig, axs = plt.subplots(1, 2, figsize=(14, 3.75), gridspec_kw={""width_ratios"": [1, 1]})\nax_text = axs[0].text(300, 50, """")\n    \n# Main PETS loop\nall_rewards = [0]\nfor trial in range(num_trials):\n    obs = env.reset()    \n    agent.reset()\n    \n    done = False\n    total_reward = 0.0\n    steps_trial = 0\n    update_axes(axs, env.render(mode=""rgb_array""), ax_text, trial, steps_trial, all_rewards)\n    while not done:\n       \n        if steps_trial == 0:\n            dynamics_model.update_normalizer(replay_buffer.get_all())  # update normalizer stats\n            \n            dataset_train, dataset_val = common_util.get_basic_buffer_iterators(\n                replay_buffer,\n                batch_size=cfg.overrides.model_batch_size,\n                val_ratio=cfg.overrides.validation_ratio,\n                ensemble_size=ensemble_size,\n                shuffle_each_epoch=True,\n                bootstrap_permutes=False,  # build bootstrap dataset using sampling with replacement\n            )\n                \n            model_trainer.train(\n                dataset_train, \n                dataset_val=dataset_val, \n                num_epochs=50, \n                patience=50, \n                callback=train_callback)\n \n        # --- Doing env step using the agent and adding to model dataset ---\n        next_obs, reward, done, _ = common_util.step_env_and_add_to_buffer(\n            env, obs, agent, {}, replay_buffer)\n            \n        update_axes(\n            axs, env.render(mode=""rgb_array""), ax_text, trial, steps_trial, all_rewards)\n        \n        obs = next_obs\n        total_reward += reward\n        steps_trial += 1\n        \n        if steps_trial == trial_length:\n            break\n    \n    all_rewards.append(total_reward)\n \nupdate_axes(axs, env.render(mode=""rgb_array""), ax_text, trial, steps_trial, all_rewards, force_update=True)\n', 'fig, ax = plt.subplots(2, 1, figsize=(12, 10))\nax[0].plot(train_losses)\nax[0].set_xlabel(""Total training epochs"")\nax[0].set_ylabel(""Training loss (avg. NLL)"")\nax[1].plot(val_scores)\nax[1].set_xlabel(""Total training epochs"")\nax[1].set_ylabel(""Validation score (avg. MSE)"")\nplt.show()\n'"
197,web_scraping_with_selenium/,https://analyticsindiamag.com/comprehensive-guide-to-web-scraping-with-selenium/,"['The overall amount of data is booming like never before that in an unstructured manner. By the end of this decade, it is estimated that we will be having nearly 100’s of zettabytes data and roughly 80% of it unstructured. Unstructured data is nothing but images, audio, text, videos, and so on, and those can not be utilised directly for model building. Nowadays, industries are making an effort to leverage this unstructured data as it can contain a vast amount of information. A huge amount of information available on the internet and taking the right steps on data can result in potential business benefits. By putting the right method to implementation can bring useful insight.\xa0\xa0\xa0\xa0', 'Web scraping, surveys, questionnaires, focus groups, etc., are some of the widely used mechanisms for gathering insightful data. However, web scraping is considered the most reliable and efficient data collection method out of all these methods. Web scraping, also termed as web data extraction, is an automatic method for scraping large data from websites. It processes the HTML of a web page to extract data for manipulation, such as collecting textual data and storing it into some data frames or in a database.', 'Following are the common use case where web scraping is used;', 'To proceed with web scraping, we will proceed with a tool called selenium. It is a powerful web browser automation tool that can simulate operations that we humans like to do over the web. It extends its support to various browsers like Chrome, Internet Explorer, Safari, Edge, Firefox. To scrape data from these browsers, selenium provides a module called WebDriver, which is useful to perform various tasks like automated testing, getting cookies, getting screenshots, and many more. Some common use cases of selenium for web scraping are submitting forms, automated login, adding and deleting data, and handling alert prompt. For more details on selenium, you can follow this official documentation.\xa0\xa0', 'There is a difference between static web pages and dynamic web pages. In static pages, the content remains the same until someone changes them manually.', 'On the other hand, content in dynamic web pages can differ from different visitors; for example, contain can be changed according to the user profile. This increases its time complexity as dynamic web pages can render at the client-side, unlike static pages at the server-side.\xa0\xa0\xa0\xa0', 'The static web page content is downloaded locally, and the relevant script is used to gather data. In contrast, dynamic web page content is generated uniquely for every request after the initial load request.\xa0\xa0', 'To scrap the data from the web page, selenium provides some standard locators which help to locate the content from the page under test; locators are nothing but keywords associated with HTML pages.', 'In this article, I’m going to simulate this automated behaviour; firstly, we scrap the data from Naukri.com and make the pandas Dataframe out of it; secondly, we will scrap the user comments from multiple pages Sephora.com using dynamic tagging.\xa0', 'Web driver manager is used to install drives required for browsers.', 'Create a variable called driver, which holds an instance for Google Chrome, and further, we will be using the drive variably to initialize commands; driver.maximum window opens chrome on full screen.', 'The above commands open the window like this;', 'Next, we will open Naukri.com, which shows a queried result. I have searched for Data Scientist Jobs available over the portal; we scrap the two elements from the page, namely Job tile and Company who provides.', ""driver.get('https://www.naukri.com/data-scientist-jobs?k=data%20scientist')"", 'After opening the web page, you need to inspect the web page by clicking the right button on your mouse; inspection helps you to find Xpath associated with each data available over the page. Inspection is nothing but the HTML view of the page.\xa0\xa0', 'The below image shows how to trace the Xpath;', 'The highlighted element in the inspection console is HTML code for the Job Title; using that, we can create our desired Xpath. X path can be obtained directly by right-clicking to the code, under copy; you can copy it as Xpath, which looks like below;', '//*[@id=""root""]/div[3]/div[2]/section[2]/div[2]/article[1]/div[1]/div[1]/a', 'After observing each Job title, you notice that the class name associated with it is the same, so you will scrap the title using the class name as below,', 'job_title = driver.find_elements_by_xpath(\'//a[@class=""title fw500 ellipsis""]\')', 'So now all the Job names are stored in job_title, so we need to extract that web element using the .text method for each title as below;', 'The title list shows as below;', 'The same approach can be applied for our second observation, i.e. company that is offering the job.', 'Let’s do this all together, an automated script that gives you a Data frame containing the job title and company name available on a particular page.', 'This section will see how to get data from multiple pages; for that, we will visit product sephora.com; from that page, we will take user comments and user names from a few pages.', 'Firstly we define an empty list for those two variables in which comments and user id will be appended; driver initializing process and scrapping process is the same as previous but change is navigation between pages.', 'To navigate between pages, we will take xpath from the negation bar of that page; by using that xpath we can navigate between pages.\xa0\xa0\xa0', 'After scrolling through the page will look like below;', 'To scrap the data, use inspection console to get xpath shown as below image;', 'After appending the data, you can navigate the next page and append data again; the xpath for navigation can be obtained by inspecting page numbers.', 'After adding the desired amount of data, we can create a data frame out of it before and the CSV file.', 'After scraping all the data, the data frame looks like as above;']","'! pip install webdriver_manager\n! pip install selenium'
 'from selenium import webdriver\nfrom webdriver_manager.chrome import ChromeDriverManager\nimport pandas as pd\n', 'driver\xa0 = webdriver.Chrome(ChromeDriverManager().install())\ndriver.maximize_window()\n', ""driver.get('https://www.naukri.com/data-scientist-jobs?k=data%20scientist')"", '//*[@id=""root""]/div[3]/div[2]/section[2]/div[2]/article[1]/div[1]/div[1]/a', 'job_title = driver.find_elements_by_xpath(\'//a[@class=""title fw500 ellipsis""]\')', 'title = []\nfor i in range(len(job_title)):\n     title.append(job_title[i].text)\n\ntitle', 'driver\xa0 = webdriver.Chrome(ChromeDriverManager().install())\ndriver.maximize_window()\ndriver.get(\'https://www.naukri.com/data-scientist-jobs?k=data%20scientist\')\n\njobs = driver.find_elements_by_xpath(\'//a[@class=""title fw500 ellipsis""]\')\ncompany = driver.find_elements_by_xpath(\'//a[@class=""subTitle ellipsis fleft""]\')\n\ntitle = []\nfor i in range(len(jobs)):\n\xa0\xa0\xa0\xa0title.append(jobs[i].text)\n\ndata = pd.DataFrame(title)\xa0\xa0\xa0\xa0\n\ncompany_ = []\nfor i in range(len(company)):\n\xa0\xa0\xa0\xa0company_.append(company[i].text)\n\ndata[\'company\'] = company_\n\ndata.to_csv(\'Job List.csv\',index=False)\ndata\n\ndriver.close()', ""user = []\ncommnent = []\n\n#initialize the driver\ndriver\xa0 = webdriver.Chrome(ChromeDriverManager().install())\ndriver.maximize_window()\n\n# open the product page\nurl='https://www.sephora.com/product/fresh-rose-hyaluronic-acid-deep-hydration-moisturizer-P471237?icid2=new%20arrivals:p471237:product'\n\ndriver.get(url)\n"", '# scrape the data from page\ncomments = driver.find_elements_by_xpath(\'//div[@class=""css-1x44x6f eanm77i0""]\')\nuser_id = driver.find_elements_by_xpath(\'//strong[@data-at=""nickname""]\')\n\n# append the data to list\nfor i in range(len(user_id)):\n\xa0\xa0\xa0\xa0user.append(user_id[i].text)\n\nfor i in range(len(comments)):\n\xa0\xa0\xa0\xa0commnent.append(comments[i].text)\xa0\xa0\xa0\xa0', '# navigate between pages\n\ndriver.find_element_by_xpath(\'//*[@id=""ratings-reviews-container""]/div[2]/ul/li[3]/button\').click()\n\ndata = pd.DataFrame(user)\xa0\xa0\ndata[\'commnents\'] = commnent\ndata.to_csv(\'Sentiment.csv\',index=False)\n\ndriver.close()'"
198,with_implementation_in_python/,https://analyticsindiamag.com/beginner-guide-to-web-scraping-with-selenium-with-implementation-in-python/,"['The Internet is the ‘Large hub’ of Data. Whether you need the textual or image data for your company or personal research use, you can scrape all kinds of worthy data by using Selenium. There are plenty of tools and frameworks you can use to do web Scraping, today we are going to discuss selenium, which basically automates browsers. That’s it!', 'This means you can use your choice of browser to do automated scraping tasks for you.', 'Selenium was originally developed by Jason Huggins in 2004 as an internal tool at ThoughtWorks. It was mostly used for testing at that time, but now it’s widely used for browser automation platforms and, of course, web scraping!\xa0', 'It is available as Selenium WebDriver, Selenium IDE, and Selenium Grid.\xa0', 'Selenium WebDriver is used to automate browsers to test, scale, and distribute scripts with a language-specific binding to a browser.', 'Browser supported by Selenium (Chrome, Opera, Firefox, Safari, Internet Explorer)', 'Operating System Supported (Linux, Mac, Windows)', 'Programming languages Supported (Python, PHP, Ruby, Java, Javascript)', 'Selenium IDE (Integrated Development Environment) is a test tool used by testers and also can be used by someone who is not familiar with developing test cases for their websites. It is very easy to use, you just need to add the Selenium IDE extension to your browser, and you are good to go with a pre-built GUI function to easily record your sessions.', 'Selenium Grid is used to run parallel test sessions across different web browsers; it is based on\xa0 the hub- node architecture, where one server acts as a hub and other devices act as nodes consisting of their operating system and remote Web drivers. It also reduces the time that a test suite takes to complete because of the Hub-Node relation they are relying on.', 'Selenium, all suites source code, is made available under the Apache 2.0 license for a contribution at Github', 'We are going to use Python for coding with an additional Chrome driver(to make your script work in chrome browser) and a selenium framework for python.', 'To check if your “ChromeDriver” and everything is setup use the command :', '\xa0', 'This code will open analytics india magazine homepage into your chrome.\xa0', 'If you don’t want to give your ChromeDriver location every time you run a programme, just put your driver location into the environment variable path.', 'And the same result will be achieved by this programme too!', 'Other driver function you can use:', 'To scrape the specific amount of data, we have plenty of handful functions you can try.', 'Usually, to scrape a specific type of data we need to find the element bound to that data, let’s say locating all the heading(title) we need to use', 'We can search for bikes images and download them if we want to with making a google search query like this:', 'So this is one of many ways we can use Selenium to do our task from scraping to automating web surfing tasks and extract images and Report generation.', 'Another thing we can achieve is to automate the whole task of downloading reports from a website by filling in all the details of different users.']","'pip install selenium'
 'chromedriver', ""from selenium import webdriver\n\nDRIVER_PATH = '/path/to/chromedriver'\ndriver = webdriver.Chrome(executable_path=DRIVER_PATH)\ndriver.get('https://analyticsindiamag.com/')\n"", ""from selenium import webdriver\ndriver = webdriver.Chrome()\ndriver.get(‘https://analyticsindiamag.com/')\n"", 'print(driver.title)\nprint(driver.window_handles)\nprint(driver.page_source)\nprint(driver.current_url)\ndriver.refresh()\n', ""from selenium import webdriver\nimport time\ndriver = webdriver.Chrome()\ndriver.get('https://analyticsindiamag.com/')\nprint(driver.page_source)\n"", 'search_url=“https://www.google.com/search?q={q}&tbm=isch&tbs=sur%3Afc&hl=en&ved=0CAIQpwVqFwoTCKCa1c6s4-oCFQAAAAAdAAAAABAC&biw=1251&bih=568"" \n\ndriver.get(search_url.format(q=\'Bikes\'))\n'"
199,web_applications_using_pywebio/,https://analyticsindiamag.com/creating-web-applications-using-pywebio/,"['If you want to create a web application using Python, you can choose to use sophisticated and highly customizable packages like Flask, FastAPI or Django; or create a barebones web application using Streamlit. PyWebIO is another Python package that enables you to create simple web applications without prior knowledge of\xa0 HTML and Javascript. The idea behind PyWebIO is to use the browser as a rich-text terminal, which is reflected in how web applications are written.\xa0\xa0', 'The key factor that differentiates the two libraries is how applications are coded and their execution flow. Streamlit works responsively, i.e., every time the user interacts with a widget, the script is re-executed and the output of the widget is updated with the new value returned in the run. On the other hand, PyWebIO linearly runs the code much like a series of terminal commands. The output functions display content in the browser in real-time, and the input function blocks the execution until the user submits the input, much like Python’s built-in input(). This enables developers to easily convert existing terminal programs into web applications by replacing the input and output functions.\xa0', 'Let’s go through the basic input and output functions of PyWebIO, but we’ll need to install it first.\xa0', 'pip install -U pywebio', 'PyWebIO provides a plethora of input functions to fetch user input through the browser. When an input function is called, a new browser tab pops up with an input form. Similar to the Python built-in input(), the PyWebIO input function is blocking, i.e., it stops the flow of execution until the form is successfully submitted.', 'You can learn more about the different input functions here.', 'It also provides a wide range of output functions to render all kinds of output in the browser. Refer here to find the list of all available output functions.\xa0']","'input()'
 'pip install -U pywebio', 'input()'"
200,frameworks_for_web_development/,https://analyticsindiamag.com/10-best-python-frameworks-for-web-development/,"['Python is known for its tools and frameworks. Moreover, Python frameworks have emerged as the go-to solution for developers to achieve their goals, with fewer lines of code.', 'Below is a list of the ten best Python frameworks for web development.', '(The list is in no particular order)', 'About: Django, an open-source framework, is a popular high-level web framework in Python which supports rapid web development and design.\xa0', 'Some of its features are-', 'Know more here.', 'About: CherryPy is a popular object-oriented web framework in Python. The framework allows building web applications in a much simpler way.', 'Some of its features are-', 'Know more here.', 'About: TurboGears is a Python web application framework. The next version, TurboGears 2, is built on top of several web frameworks, including TurboGears 1, Rails and Django.', 'Some of its features are:\xa0', 'Know more here.', 'About: Flask is a popular Python web framework used for developing complex web applications. The framework offers suggestions but doesn’t enforce any dependencies or project layout.', 'Some of its features are-', 'Know more here.', 'About: Written in Python, Web2Py is a free, open-source web framework for agile development of secure database-driven web applications. It is a full-stack framework.', 'Some of its features are-', 'Know more here.', 'About: Bottle is a fast, simple and lightweight WSGI micro web framework for Python web applications. The framework has no other dependencies than the standard Python library.\xa0', 'Some of its features are-', 'Know more here.', 'About: Falcon is a WSGI library for building speedy web APIs and app backends. The framework has CPython 3.5+ and PyPy 3.5+ support. Falcon complements more general Python web frameworks by providing extra reliability, flexibility, and performance.\xa0', 'Some of its features are-', 'Know more here.', 'About: Written in Python, CubicWeb is a free and open-source semantic web application framework. It empowers developers to efficiently build web applications by reusing components (called cubes) and following the well known object-oriented design principles.', 'Some of its applications are-', 'Know more here.', 'About: Quixote is a framework for writing Web-based applications using Python. The goal of this framework is to provide flexibility and high-performance during web development.\xa0', 'Some of its features are-', 'Know more here.', 'About: Pyramid is a lightweight and open-source Python web framework. The framework provides only the core tools needed for nearly all web applications: mapping URLs to code, security, and serving static assets (files like JavaScript and CSS).\xa0', 'Some of its features are-']",
201,using_python_and_scrapy/,https://analyticsindiamag.com/hands-on-guide-to-web-scraping-using-python-and-scrapy/,"['Web Scraping is a procedure to extract information from sites. This can be done with the assistance of web scraping programming known as web scrapers. They consequently load and concentrate information from the sites dependent on client prerequisites. Scrapy and Beautiful Soup are some of the famous web scrapers used to extract reviews from famous websites like Amazon, Zomato to analyze it.', 'Scrapy is an open-source web crawling system, written in Python. Initially intended for web scraping, it can likewise be utilized to separate information utilizing APIs or as a universally useful web crawler. This web crawler is used to create our own spiders. It helps to select specific parts from the webpage using selectors like CSS and XPath.', 'Here, we will cover the details of components that are used in Scrapy for web crawling purposes. Further, we will extract the data from a particular site using Python and Scrapy.', 'The engine receives an HTTP request from the spiders. It delivers that request to the Scheduler as it is responsible for tracking the order of request. The more preferred requests are sent back to the Engine by the Scheduler. The Engine sends a request to the downloader, and in return, it receives back a response. The response is then sent back to the spider for processing activity. Finally, the Engine sends a response to the item pipeline that gives specific parts of the data that are asked to extract.', 'Scrapy comes with whole new features of creating a spider, running it and then saving data easily by scraping it.', 'For this project use either Pycharm or Visual Studio as we can see the output in the terminal', 'It is a great idea to establish one virtual environment as it separates the program and doesn’t influence some other projects present in the machine.', 'To create a virtual environment first install it by using.', 'Create one folder and then activate it\xa0', 'It is difficult to install scrapy in Window 10. So, it is recommended to install it using anaconda navigator.', 'After installing Scrapy, we need to create a scrapy project.', 'In Scrapy, one Spider is made which slithers over the site and assists with fetching information, so to make one, move to the spider folder and make one python document over there.', 'First thing is to name the Spider by assigning it with a named variable and afterwards give the beginning URL through which the Spider will begin scraping.\xa0', 'Go to C drive and open the user Folder. We can see that there will be a folder in the name of the project “worldometer”.', 'Our main aim is to get every URL from the site. Get all the URLs or anchor labels from it. To do this, we have to make one more technique parse to get information from the given URL.\xa0', 'Presently for retrieving information from the given page, use selectors. These selectors can be either from CSS or from Xpath.', 'Open the corona.py folder in our IDE. We will fetch the data from the URL mentioned in the start_urls domain. In our project, the XPath selector is used to fetch the data from the world meter website. For working of the selectors, we need to right-click on the website to get the text and link. The yield command will give the items that are asked to fetch.', 'Finally, run the spider and get output in simple CSV file\xa0']","'sudo apt-get install python3-venv'
 './Scripts/activate', 'pip install scrapy.', 'scrapy startproject corona', 'import scrapy\nclass CoronaSpider(scrapy.Spider):\n\xa0\xa0\xa0\xa0name = \'corona\'\n\xa0\xa0\xa0\xa0allowed_domains = [\'www.worldometers.info/coronavirus/\']\n\xa0\xa0\xa0\xa0start_urls = [\'http://www.worldometers.info/coronavirus/\']\n\xa0\xa0\xa0\xa0def parse(self, response):\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0for country in response.xpath(""//td/a""):\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0name=country.xpath("".//text()"").get()\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0link=country.xpath("".//@href"").get()\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0yield{\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'country_name\':name,\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\'country_link\':link\n\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0\xa0}\xa0\xa0\xa0', 'scrapy crawl NAME_OF_SPIDER -o File_Name.csv'"
202,of_web_crawling_tools/,https://analyticsindiamag.com/scrapy-vs-beautiful-soup-a-comparison-of-web-crawling-tools/,"['One of the most critical assets for data-driven organisations is the kind of tools used by their data science professionals. Web crawler and other such web scraping tools are few of those tools that are used to gain meaningful insights. Web scraping allows efficient extraction of data from several web services and helps in converting raw and unstructured data into a structured whole.\xa0\xa0', 'There are several tools available for web scraping, such as lxml, BeautifulSoup, MechanicalSoup, Scrapy, Python Requests and others. Among these, Scrapy and Beautiful Soup are popular among developers.\xa0', 'In this article, we will compare these two web scraping tools, and try to understand the differences between them. Before diving deep into the tools, let us first understand what these tools are.', 'Scrapy is an open-source and collaborative framework for extracting the data you need from websites in a fast and simple manner. This tool can be used for extracting data using APIs. It can also be used as a general-purpose web crawler. Thus, Scrapy is an application framework, which can be used for writing web spiders that crawl websites and extract data from them.', 'The framework provides a built-in mechanism for extracting data – known as selectors – and can be used for data mining, automated testing, etc. Scrapy is supported under Python 3.5+ under CPython and PyPy starting with PyPy 5.9.', 'If you just want to install scrapy globally in your system, you can install scrapy library using the python package ‘pip’. Open your terminal or command prompt and type the following command.', 'pip install scrapy', 'If you want scrapy to be in your conda environment just type in and execute the following command in your terminal', 'conda install -c conda-forge scrapy', 'The scrapy shell: It allows to scrape web pages interactively using the command line.', 'To open scrapy shell type\xa0scrapy shell.', 'Scraping with Scrapy Shell', 'Follow the steps below to start scraping :', '1. Open the html file in a web browser and copy the url.', '2. Now in the scrapy shell type and execute the following command:', 'fetch(“url--”)', 'Replace url– with the url of the html file or any webpage and the fetch command will download the page locally to your system.', 'You will get a similar message in your console', '[scrapy.core.engine] DEBUG: Crawled (200)', '3. Viewing the response', 'The fetch object will store whatever page or information it fetched into a response object. To view the response object simply type in and enter the following command.', 'view(response)', 'The console will return a \xa0True and the webpage that was downloaded with fetch() will open up in your default browser.', '4. Now that all the data you need is available locally. You just need to know what data you need.', '5. Scraping the data: Coming back to the console, all the elements need to be printed behind the webpage that was fetched earlier. Enter the following command:', 'print(response.text)', 'Click here to get the detailed web scraping.', 'Beautiful Soup is one of the most popular Python libraries which helps in parsing HTML or XML documents into a tree structure to find and extract data. This tool features a simple, Pythonic interface and automatic encoding conversion to make it easy to work with website data.\xa0', 'This library provides simple methods and Pythonic idioms for navigating, searching, and modifying a parse tree, and automatically converts incoming documents to Unicode and outgoing documents to UTF-8.\xa0', 'Beautiful Soup library can be installed using PIP with a very simple command. It is available on almost all platforms. Here is a way to install it using\xa0Jupyter Notebook.', 'This library can be imported with the following code and assign it to an object.', 'We will be using this basic, and default, HTML doc to parse the data using Beautiful Soup.', 'The following code will expand HTML into its hierarchy:', 'To navigate through the tree, we can use the following commands:', 'Beautiful Soup has many attributes which can be accessed and edited.\xa0This extracted parsed data can be saved onto a text file.', 'Click here to know more about web scraping with BeautifulSoup.', 'Scrapy is an open-source framework, whereas Beautiful Soup is a Python library designed for quick turnaround projects like screen-scraping. A framework inverts the control of the program and informs the developer what they need. Whereas in the case of a library, the developer calls the library where and when they need it.', 'Due to the built-in support for generating feed exports in multiple formats, as well as selecting and extracting data from various sources, the performance of Scrapy can be said to be faster than Beautiful Soup. Working with Beautiful Soup can speed up with the help of Multithreading process.', 'Extensibility', 'Beautiful Soup works best when working on smaller projects. On the other hand, Scrapy may be the better choice for larger projects with more complexities, as this framework can add custom functionalities and can develop pipelines with flexibility and speed.\xa0\xa0', 'For a beginner who is trying hands-on web scraping for the first time, Beautiful Soup is the best choice to start with. Scrapy can be used for scraping, but it is comparatively more complex than the former.\xa0']","'pip install scrapy'
 'conda install -c conda-forge scrapy', 'scrapy shell', 'fetch(“url--”)', '[scrapy.core.engine] DEBUG: Crawled (200)', 'view(response)', 'print(response.text)'"
203,in_python_with_scrapy/,https://analyticsindiamag.com/beginners-guide-to-web-scraping-in-python-with-scrapy/,"['A Data Scientist with no data is only as good as a carpenter with no timber. They may have exceptional skills with all the tool sets but neither of them have what’s important to show off those exceptional skills.', 'The overall estimated size of data on the internet today is 1.2 million terabytes. That is a huge number and clearly, more than 70% of this data is available in the public domain. Data has become omnipresent yet it is possible that a Data Science enthusiast may end up with no interesting data. This is why Web-Scraping should be one of the top skills of a Data Scientist. Data Scientist may not always be provided with formatted data to understand the science of it. Sometimes he/she can just be shown the path to the right data or simply it is a choice to find interesting data. Web-Scraping is the easiest way to gather data from this huge virtual world of Internet.', 'This article is to help anyone with less than a basic programming knowledge especially python to Scrape anything from a webpage.', 'All readers should also bear in mind that not all websites or web pages are meant to be scraped as long as you don’t want to do anything against the law. Appending /robots.txt to a website-url will show you what you are allowed to scrape and what not.', 'Scrapy is an open source framework that facilitates programmers to scrape data from any webpage. Scrapy is available in python as a package.', 'Let’s Begin! ', 'If you just want to install scrapy globally in your system, you can install scrapy library using the python package ‘pip’. Open your terminal or command prompt and type the following command.', 'pip install scrapy', 'If you want scrapy to be in your conda environment just type in and execute the following command in your terminal', 'conda install -c conda-forge scrapy', 'Let’s start scraping!!', '', 'Here is a dummy webpage to learn scraping. Copy and paste the below html code to a file and save it as html.', '<!DOCTYPE html>', '<html>', '<head>', '<title>Scrapy Tutorial</title>', '</head>', '<style>', 'body {', 'background-image : url(“file:///Users/aim/Downloads/white.jpg”);', 'background-repeat: no-repeat;', 'background-size: cover;', '}', '</style>', '<body>', '<h1>Hi I am Scrapy!!</h1>', '<p>Scrapy is a free and open-source web-crawling framework written in Python. Originally designed for web scraping, it can also be used to extract data using APIs or as a general-purpose web crawler. It is currently maintained by Scrapinghub Ltd., a web-scraping development and services company.</p>', ' <h2>Official Page</h2>', '<p>Click <a href=”https://scrapy.org/”>here</a> to visit the official page</p> ', '<h2>History</h2>', '<span><p><i>Scrapy is a free and open-source web-crawling framework written in Python. Originally designed for web scraping, it can also be used to extract data using APIs or as a general-purpose web crawler. It is currently maintained by Scrapinghub Ltd., a web-scraping development and services company.</i></p></span>', '<h2>Reference</h2>', '<p>Wikipedia Link –> <a href=”https://en.wikipedia.org/wiki/Scrapy”>Click</a></p>', '</body>', '</html>', 'When you open the file with a browser you will see the page as shown below:', '', 'The scrapy shell is a utility that allows us to scrape web pages interactively using the command line.', 'To open scrapy shell type scrapy shell.', 'If you have installed scrapy in a virtual environment like conda, make sure to activate the environment using\xa0conda activate before using\xa0scrapy shell command', 'After executing you will find a similar console :', '', 'Follow the steps below to start scraping :', '1. Open the html file in a web browser and copy the url. ', 'For me it is : file:///Users/aim/Desktop/web_eg.html', '2. Now in the scrapy shell type and execute the following command:', ' fetch(“url--”)', 'Replace url– with the url of the html file or any webpage and the fetch command will download the page locally to your system.', 'You will get a similar message in your console', '[scrapy.core.engine] DEBUG: Crawled (200)  ', '3. Viewing the response', 'The fetch object will store whatever page or information it fetched into \xa0a response object. To view the response object simply type in and enter the following command.', 'view(response)', 'The console will return a \xa0True and the webpage that was downloaded with fetch() will open up in your default browser.', '4. Now that all the data you need is available locally. You just need to know what data you need. In this example we will scrape the main heading of the page, all subheadings, the content in History and the hyperlinks provided in the page.', 'For each web-page the level of creativity and the arrangement of elements and structuring can be entirely different and unique. To identify the elements or attributes that you need to scrape, most of the web browsers come with a feature called ‘inspect’ or ‘inspect element’ that allows users to see the construction of the page. See the below image.', '', '5. Scraping the data', 'Coming back to the console, we will now print all the elements behind the webpage that was fetched earlier. Enter the following command:', '', 'Execute the following commands one by one to extract the main heading of the page, all subheadings, the content in History and the hyperlinks provided in the page.', 'a. The main heading', 'Identified as the <h1> tag, execute the following code ', 'response.css(""h1"").extract()', 'We are only interested in the data, however the previous command returned the entire <h1> html tag.In order to get only the data we add the text selector “::text” as shown below:', 'response.css(""h1::text"").extract()', 'The output is always a list. What it means is that the command will extract any and all the information contained in any and all the <h1> tags present in the webpage. Let us understand by scraping the subheadings.', 'b. The Subheadings', 'By inspecting the subheadings on a browser we found they are contained by the <h2> tags. So type in the following command.', ' response.css(""h2::text"").extract()', 'Not so surprisingly it returns all the three Subheadings in the webpage in a list.', 'If you are only interested in the first subheading, you can use the extract_first() method as shown below:', 'response.css(""h2::text"").extract_first()', 'You can also traverse through the list like a normal python list using indexes as shown below the index 1 returns the second item in the list returned by the extract method:', 'response.css(""h2::text"").extract()[1]', 'c. The content in History ', 'Identified by the only italics style (<i> tag) in the page, we will extract the data using the following command:', 'response.css(""i::text"").extract() ', 'd. The hyperlinks', 'The links are present in the elements as attributes, to get an attribute from an html element using scrapy we use the “::attr()” selector. Let us extract the hyperlinks', 'response.css(""a::attr(href)"").extract()', 'Here is a serial execution of all the commands discussed above.', '', 'After Extraction', 'We can store all the extracted values into variables and put them in a well-formatted csv or excel files.']","'pip install scrapy'
 'conda install -c conda-forge scrapy', 'scrapy shell', 'conda activate', 'scrapy shell', ' fetch(“url--”)', '[scrapy.core.engine] DEBUG: Crawled (200)  ', 'view(response)', 'print(response.text)', 'response.css(""h1"").extract()', 'response.css(""h1::text"").extract()', ' response.css(""h2::text"").extract()', 'response.css(""h2::text"").extract_first()', 'response.css(""h2::text"").extract()[1]', 'response.css(""i::text"").extract() ', ""['Scrapy is a free and open-source web-crawling framework written in Python. Originally designed for web scraping, it can also be used to extract data using APIs or as a general-purpose web crawler. It is currently maintained by Scrapinghub Ltd., a web-scraping development and services company.']"", 'response.css(""a::attr(href)"").extract()', ""['https://scrapy.org/', 'https://en.wikipedia.org/wiki/Scrapy']"""
204,https://analyticsindiamag.com/beautiful_soup_webscraping_python/,https://analyticsindiamag.com/beautiful-soup-webscraping-python/,"['Web Scraping is the process of collecting data from the internet by using various tools and frameworks. Sometimes, It is used for online price change monitoring, price comparison, and seeing how well the competitors are doing by extracting data from their websites.', 'Web Scraping is as old as the internet is, In 1989 World wide web was launched and after four years World Wide Web Wanderer: The first web robot was created at MIT by Matthew Gray, the purpose of this crawler is to measure the size of the worldwide web.', 'Beautiful Soup is a Python library that is used for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree from page source code that can be used to extract data in a hierarchical and more readable manner.', 'It was first introduced by Leonard Richardson, who is still contributing to this project and this project is additionally supported by Tidelift (a paid subscription tool for open-source maintenance)', 'Beautiful soup3 was officially released in May 2006, Latest version released by Beautiful Soup is 4.9.2, and it supports Python 3 and Python 2.4 as well.', 'For installing Beautiful Soup we need Python made framework for the same, and also some other supported or additional frameworks can be installed by given PIP command below:pip install beautifulsoup4', 'Other frameworks we need in the future to work with different parser and frameworks:', 'A small code to see how BeautifulSoup is faster than any other tools, we are extracting the source code from demoblaze\xa0', 'Now “.prettify()” is a built-in function provided by the Beautiful Soup module, it gives the visual representation of the parsed URL Source code. i.e. it arranges all the tags in a parse-tree manner with better readability', 'For Excluding unwanted data and scrap reliable information only, we have to inspect the webpage.', '\xa0We can open the Inspect tab by doing any of the following in your Web browser:', 'Now after opening the inspect tab, you can search the element you wish to extract from the webpage.', 'By just hovering through the webpage, we can select the elements; and corresponding code will be available like shown in the above image.', 'The title for all the articles is inside Class=”post-article”, and inside that, we have our article title in-between “span” tags.', 'With this method, we can look into web pages’ backend and explore all the data with just hover and watch functionality provided by Chrome browser Inspect tools.', 'In this example, we are going to use Selenium for browser automation & source code extraction purposes.', 'A full tutorial about selenium is available here.', 'Our purpose is to scrape all the Titles of articles from the Analytics India Magazine homepage.', 'Let’s break down the above code line by line to understand how it can detect those article titles:', 'Note: We can extract given URL source code in many ways, but as we already know about selenium, So it’s easy to move forward with the same tool, and it has other functionalities too like scrolling through the hyperlinks and clicking elements.', 'After this, you can feed the data for data science work you can use this data to create a world, or maybe you can do text-analysis.', 'Beautiful Soup is a great tool for extracting very specific information from large unstructured raw Data, and also it is very fast and handy to use.', 'Its documentation is also very helpful if you want to continue your research.']","'pip install selenium\npip install requests\npip install lxml\npip install html5lib'
 'from bs4 import BeautifulSoupimport requests  URL = ""https://www.demoblaze.com/""r = requests.get(URL)  \n\nsoup = BeautifulSoup(r.content, \'html5lib\')\nprint(soup.prettify())', ""#importing modules\nfrom selenium import webdriver\nfrom bs4 import BeautifulSoup\n\noptions = webdriver.ChromeOptions()\noptions.add_argument('--ignore-certificate-errors')\noptions.add_argument('--incognito')\noptions.add_argument('--headless')\n\ndriver = webdriver.Chrome(chrome_options=options)\nsource =driver.get('https://analyticsindiamag.com/')\nsource_code=driver.page_source\n\nsoup = BeautifulSoup(source_code,'lxml')\narticle_block =soup.find_all('div',class_='post-title')\n\nfor titles in article_block:\n\ttitle =titles.find('span').get_text()\n\tprint(title)\n"", 'from selenium import webdriver\nfrom bs4 import BeautifulSoup\n', ""options = webdriver.ChromeOptions()\noptions.add_argument('--ignore-certificate-errors')\noptions.add_argument('--incognito')\noptions.add_argument('--headless')\n"", 'source_code=driver.page_source', ""soup = BeautifulSoup(source_code,'lxml')\narticle_block =soup.find_all('div',class_='post-title')\n"", ""for titles in article_block:\n\ttitle =titles.find('span').get_text()\n\tprint(title)\n"""
205,data_from_a_website/,https://analyticsindiamag.com/beautiful-soup-primer-how-to-scrape-data-from-a-website/,"['Beautiful Soup is a HTML and XML parser available on Python 2.6+. Soup is named after the unstructured HTML documents which are hard to understand and noisy. It parses the data from the HTML and XML documents from where it can be extracted. ', 'In this article, we will be going through functions which help us extract data from the HTML document. We will be using a toy HTML to explain how Beautiful Soup works and walk through the steps involved in Scraping — one of the techniques of data mining — data from a website’s HTML format. ', 'With the help of headless browsers such as Selenium and PhanthomJS, one can easily practice how to scrape data out of a website. With these browsers, it will be easy to scrape through multiple pages or extract a large amount of data from the websites. Using a headless browser will also increase the computation speed which will result in the consumption of less memory. In fact, PhanthomJS assigns unique processes to each browser as well.', 'Beautiful Soup library can be installed using PIP with a very simple command. It is available on almost all platforms. Here is a way to install it using Jupyter Notebook.', '', 'We can import this library with the following code and assign it to an object.', '', 'Beautiful Soup has a default parser available in the standard Python Library. We can use a different parser depending on the objective. The most common alternative parsers are “lxml” and “html5lib”. This can be installed with the help of the following code:', '', 'Below is a tabular representation of various parsers with their advantages and disadvantages:', 'We will be using this basic, and default, HTML doc to parse the data using Beautiful Soup.', '', 'The following code will expand HTML into its hierarchy:', '', 'To navigate through the tree, we can use the following commands:', '', 'Beautiful Soup has many attributes which can be accessed and edited. This extracted parsed data can be saved onto a text file.', '', 'To extract the text from the string, we can use the get_text() command. ', '', 'The string can be accessed using the strings command. But it also includes white space which can be stripped easily.', '', 'Since the above output has a lot of white space, the striped.strings command will help us remove it.', '', 'We can obtain the parent of a particular HTML with .parent attribute, like here:', '', 'To access the siblings — previous as well as the next — we can use the following commands:', '', 'This function is used to search for a very particular field throughout the HTML document. It is one of the key features required while data mining or scraping a data from a website with the help of Selenium and PhanthomJS.', '']",
206,libraries_selenium_beautiful_soup/,https://analyticsindiamag.com/guide-to-web-scraping-with-python-libraries-selenium-beautiful-soup/,"['', 'Web scraping is a method for transforming unstructured data on the web into machine-readable, structured data for analysis. In general web, scraping is a complex process, but Python programming language has made it an easy and effective means. Python libraries such as Selenium, Beautiful soup and Pandas are used for web scraping.', 'It is essential that for practising any of the new data related technologies we need well-designed data sets. Many users believe they have to collect their own data but it’s simply not true.', 'There are hundreds of open data sets accessible, ready to be used and analyzed by anyone willing to look for them. Below is a list of most globally interesting open data websites.', '1.US Census Bureau http://www.census.gov/data.html', '2.Socrata', '3.European Union Open Data Portal http://open-data.europa.eu/en/data/', '4.Data.gov.uk http://data.gov.uk/ Data from the UK Government.', '5.UNICEF volunteers statistics on the sphere of women and children worldwide.', 'Many of these open source datasets are from the government and public organisations, where they bury the data in drill-down links and tables. This often requires the users to use best guess navigation which helps in finding the specific data users are looking for. Scraping the data with the help of Python and saving it as JSON is what users need to do to get started.', 'Most of the open source datasets websites use JavaScript links which makes it tough to analyse them. Methods using Python libraries will not work without some extensions.', 'Selenium package is used to automate web browser intercommunication from Python. With Selenium, programming a Python script to automate a web browser is conceivable. Afterwards, those complex JavaScript links are no longer a problem.', 'Code:', 'From selenium import webdrivere', 'From selenium.webdriver.common.keys import Keys', 'From bs4 import BeautifulSoup', 'Import re', 'Import pandas as pd', 'Import os', 'Selenium will now begin a browser session. For Selenium to work, it must access the browser driver. By default, it will look in the corresponding directory as the Python script. Connections to Chrome, Firefox, Edge, and Safari drivers available here. ', 'The sample code below uses Chrome', '#launch url', 'url= “http:// website name/division/sub_division.format”', '#create a new chrome session', 'driver=webdriver.chrome()', 'driver.implicity_wait(30)', 'driver.get(url)', 'python_button=driver.find_element_by_id(‘@@@@@@@@@@@@@@@@’) #$$$$$', 'python_button.click()#click $$$$$ link', 'The python_button.click() mentioned in the code is telling Selenium to click the JavaScript link on the page. After appearing at the specified page, Selenium hands over the page source to Beautiful Soup.', 'Beautiful Soup is the best way to cross the DOM(Document Object Model) and scrape the data. After representing an empty list and a counter variable, it is time to examine Beautiful Soup to seize all the links on the page that coordinate a regular expression.', '', 'Code:', '#Selenium hands the page source to Beautiful Soup', 'soup_level1=BeautifulSoup(driver.page_source,’####’)', 'datalist=[] #empty list', 'x=() #counter', 'For link in soup_level1.find_all(‘a’,id=re.compile(“^##file_location##”));', '## code to execute in for loop ##', '\xa0', '#Beautiful soup grabs all the specified links', 'All specified links ', 'For link in soup_level.find_all(‘variable’, id=re.compile(“^##data_set path”));', '# selenium visits each specified page', 'python_button=driver.find_element_by_##variable(‘##path’ + str(x))', 'python_button.click() #click link', '# Selenium hands of the source of the specific page to Beautiful Soup', 'soup_level2=BeautifulSoup(driver.page_source,’###’)', '#beautiful Soup grabe the HTML table on the page', 'table=soup_level2.find_all(‘table’)[0]', '#giving the HTML table to pandas to put in a dataframe object', 'df=pd.read_html(str(table),header=0)', '#Store the dataframe in a list', 'datalist.append(df[0])', '#Ask Selenium to click the back button', 'driver.execute_script(“window.history.go(-1)”)', '#increment the counter variable before starting the loop over', 'X +=1', 'Beautiful Soup transfers the conclusions to Pandas. Pandas use its read_htmlfunction to read the HTML table data into a data frame. The data frame is added to the previously defined empty list. Before the code block of the loop is terminated, Selenium needs to click the back button in the browser. This is so the next link in the loop will be available to click on the specified listing page.', 'When the for/in a loop has completed, Selenium will visit every specified title link. Beautiful Soup will recover the table from each page. Pandas will store the data from each table in a data frame. Each data frame is an item in the data list. The individual table data frames will merge into one extended data frame. The data will then be converted to JSON format.', '\xa0', 'Code:', '#loop has completed', '#end the Selenium browser session', 'driver.quit()', '#combine all pandas dataframes in the list into one gaint dataframe', 'result=pd.concat([pd.Dataframe(datalist(i]) for i in range(len(datalist))], ignore_index=true)', '#convert the pandas dataframe to JSON', 'json_records=result.to_json(orient=’records’)', '\xa0', '#get current working directory', 'path=os.getcwd()', '#open, write, and close the file', 'f=open(path+”\\##specified_path##”,”w”)', '#specified file', 'f.write(specified_records)', 'f.close()']",
207,scraping_custom_dataset_tutorial/,https://analyticsindiamag.com/mechanicalsoup-web-scraping-custom-dataset-tutorial/,"['A python library for automating website interaction and scaping! But what exactly is new in the MechanicalSoup which we didn’t cover in Beautiful Soup.', 'MechanicalSoup is a python package that automatically stores and sends cookies, follows redirects, and also can follow hyperlinks and forms in a webpage. It was created by M Hickford. He was always amazed by the Mechanize library. Mechanize was a John J. Lee project which enables programmatic web browsing in Python, and it was later taken over by Kovid Goyal in 2017.', 'Some of the features of Mechanize was:', 'Unfortunately, Mechanize is not compatible with Python3 programming language, and also its development stalled for many years.', 'So Hickford came up with a solution: MechanicalSoup, which provides the same API, built on Python Requests(for better HTTP sessions), and BeautifulSoup(for data navigation). Since 2017 this project has been maintained by Hemberger and moy.', 'MechanicalSoup is designed to mimic the behavior of how humans interact with web browsers. Some of the possible use-cases include:', 'You can install MechanicalSoup using PyPI (python package manager).', 'Mechanical soup install command will download BeautifulSoup, requests, six, Urllib, and other libraries.\xa0', 'Or, you can download and install the development version from GitHub:', 'Or. Installing from source (installs the version in the current working directory):', 'Note: git must be installed to use the above command.', 'The additional library we are going to install:', 'Testing our library for any errors', 'On passing regular expressions “forms” to follow_link(), which followed the link whose text matched with the given expression i.e. forms and get_url() will return the new URL.', 'It will return the current page source code like the beautifulsoup prettify function because get_current_page() is bs4.BeautifulSoup. MechanicalSoup uses beautiful soup for data extraction.', 'find_all() tag navigate threw the given tag in this case “legend,” and it will exclude the rest of the part from the source code.', 'Now we have already discussed all the MechanicalSoup essential functions if you need some more information about the API support, you can go here.', 'It’s a good use-case. The very first step of every data science project is to create or collect data, and then further processing, cleaning, analysis, modeling, and tuning part comes. Now, as we already familiar with essential API, let’s straight jump to code:', 'We are setting the google search query and making it open in the browser with search text cat.', 'Python startswith function to filter the URLs not having HTTPS\xa0', 'A full dataset of cats images in out local computer ready for further data science prediction/analysis. ????', 'You can find .ipynb(python notebook) here. It contains all the codes used in this tutorial to MechanicalSoup.', 'Did you notice that MechanicalSoup is a composition of Requests, BeautifulSoup, and also having Selenium realtime browser surfing capabilities?', 'Indeed MechanicalSoup is a powerful multipurpose automation and web scraping tool, as we know; Mechanize is a base library on the basis of which MechanicalSoup has been also created MechanicalSoup is a composition of BeautifulSoup and Requests it can also act like selenium and show us results in a web browser but with lighter processing.\xa0']","'pip install MechanicalSoup'
 'pip install git https://github.com/MechanicalSoup/MechanicalSoup', 'git clone https://github.com/MechanicalSoup/MechanicalSoup.git\ncd MechanicalSoup\npython setup.py install\n', 'pip install wget', 'import mechanicalsoup\n\nbrowser = mechanicalsoup.StatefulBrowser()\nurl = “http://httpbin.org/”\n\nbrowser.open(url)\nprint(browser.get_url())\n', 'browser.follow_link(""forms"")\nbrowser.get_url()\n', 'browser.get_current_page()', ""browser.get_current_page().find_all('legend')"", 'browser.select_form(\'form[action=""/post""]\')\nbrowser.get_current_form().print_summary()\n', 'browser[""custname""] = ""Mohit""\nbrowser[""custtel""] = ""9081515151""\nbrowser[""custemail""] = ""mohitmaithani@aol.com""\nbrowser[""comments""] = ""please make pizza dough more soft""\nbrowser[""size""] = ""large""\nbrowser[""topping""] = ""mushroom""\n \n#launch browser\nbrowser.launch_browser()\n', 'import mechanicalsoup\n \nbrowser = mechanicalsoup.StatefulBrowser()\nurl = ""https://www.google.com/imghp?hl=en""\n \nbrowser.open(url)\n \n#get HTML\nbrowser.get_current_page()\n \n#target the search input\nbrowser.select_form()\nbrowser.get_current_form().print_summary()\n \n#search for a term\nsearch_term = \'cat\'\nbrowser[""q""] = search_term \n \n#submit/""click"" search\nbrowser.launch_browser()\nresponse = browser.submit_selected()\n \nprint(\'new url:\', browser.get_url())\nprint(\'response:\\n\', response.text[:500])', ""#open URL\nnew_url = browser.get_url()\nbrowser.open(new_url)\n \n#get HTML code\npage = browser.get_current_page()\nall_images = page.find_all('img')\n \n#target the source attributes of image\nimage_source = []\nfor image in all_images:\n    image = image.get('src')\n    image_source.append(image)\n \nImage_source[5:25]\n"", '#save cleaned links in ""image_source""\nimage_source = [image for image in image_source if image.startswith(\'https\')]\n\nprint(image_source)', 'import os\n \npath = os.getcwd()\npath = os.path.join(path, search_term + ""s"")\n \n#create the directory\nos.mkdir(path)\n#print path where cats images are going to save\npath\n', ""##download wget by uncommenting below line\n#pip install wget  \n\n##download images\ncounter = 0\nfor image in image_source:\n    save_as = os.path.join(path, search_term + str(counter) + '.jpg')\n    wget.download(image, save_as)\n    counter += 1"""
208,with_all_major_tools/,https://analyticsindiamag.com/complete-learning-path-to-web-scraping-with-all-major-tools/,"['The history of web scraping is very long; In 1989 the World Wide Web(WWW) was launched and after some years\xa0 World Wide Web Wanderer: The first Perl based web robot was created by Matthew Gray at MIT, the purpose of this web crawler is to measure the size of the World Wide Web. While the wanderer was the first web robot it was not actually been used in data scraping tasks because in 90’s we don’t have an abundance of information(data) but with time as the internet user grows and a wave of digitization came, Web scraping became more and more popular.', 'We have covered many articles on web scraping and today we are going to discuss all the different web scraping tools, frameworks, and languages.', 'Let’s answer some of the most asked questions and doubts about web scraping and then we will begin with all the different tools and frameworks for web scraping as we already discussed the short definition and history of web scraping let’s move forward with Q&A.', 'Web Scraping became a major part of the E-commerce, Travel, and Recruitment industries. Retail and E-commerce industries make all the marketing strategies, service offering, campaign, and customer services based on data available to them. The E-commerce industry uses web scraping to extract the data and use it to analyze their customer behaviour, sentiments, likes, and recommendations.', 'Travel industries use web scraping to gather information like hotel reviews, prices, and analytics. Many travels, tourism, and hospitality industries use this data to build business intelligence.', 'Recruitment agencies do the same as the number of job seekers is increasing so it’s hard to manually pick resumes and find specific skill set candidates, so Web scraping comes very handy for recruiters to pick the best guy according to requirements and job descriptions.', 'Yes, we have done a couple of tutorials on web scraping where we discussed a non-coder approach to web scrapings like parsehub and Diffbot both provides a Graphical User Interface(GUI) and great documentation of usage guidelines for data extraction they are very easy to use and you can see the processing and outputs in realtime as the utility run.', 'Selenium was originally developed by Jason Huggins in 2004 as an internal tool at ThoughtWorks(a global software consultancy firm). In Earlier days, It was mostly used for testing, but now it’s widely used for browser automation platforms and, of course, web scraping! ', 'Read more', 'Beautiful Soup is a Python library that is used for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree from source code that can be used to structure data in a hierarchical and more readable manner', 'BeautifulSoup can work with your favourite web parser to provide the best way to search, navigate, and modify the parse tree. Some of the features of BeautifulSoup are as follows:', 'See implementation here', 'URllib is a python package that combines several modules to preprocess the URLs, in other words, it is a library used for HTTP requests using python language on URLs,', 'now Urllib is used first before start scraping data from website using any module like selenium, BeautifulSoup, or mechanical soup. Selenium and other libraries don’t have any feature to request data from URL so we use request and Urllib for requesting data.', 'some of the features of UrlLib library are as follows:', 'Request is another open-source python library that makes HTTP request more humans friendly and simple to use than urllib as it is made on top of urllib library, it was developed around February 2011.', 'Some of the features of the Request library are as follows:', 'Read more about Implementation', 'It is a python library used to simulate human behavior on web pages. It is made on top of a web parsing library BeautifulSoup. And it is also widely used for web scraping tasks when the websites are having multiple pages and many other elements like popups or timer.', 'Some of the features of MechanicalSoup are as follows:', 'Read more', 'Enough about Python-based Web scraping tools, Let’s talk about Puppeteer.', 'Puppeteer is a Nodejs library used to provide high-level API to control Chrome-like browser and it is used widely by nodejs enthusiasts for web scraping tasks.\xa0', 'Node.js is an open-source server runtime environment that runs on various operating systems like macOS, Linux, Windows, etc. It’s not a programming language, it uses javascript as its main programming interface.', 'Some of the features of Puppeteer is as follows:', 'Learn more about the implementation here', 'We have another Node.Js based Web scraping tool that is specifically designed for web scraping tasks as the Puppeteer is not that fast and also used for automation most of the time.\xa0', 'Cheerio help in interpreting and analyzing the web pages using jQuery-like syntax. It is fast and flexible in comparison to puppeteer and there are many features that make it a more scraping friendly tool like:', 'Read more', 'We have already seen scraping data using python and node.js but is it possible to scrape data using PHP? The answer is yes due to the abundance of data the research in web scraping techniques is increasing and today we have a variety of web scraping tools based on different programming languages.\xa0', 'Goutte is a PHP based web scraping tool that was originally developed by Fabien Potencier, Who is better known as the creator of Symfony framework, Goutte is based on PHP 5.5+ version and Guzzle 6+(an HTTP client i.e. a requirement of Goutte framework)', 'Some of the features of using Goutte for your web scraping work are as follows:', 'Learn More\xa0', 'Is there any tool that supports multiple languages, almost all programming languages for web scraping?', 'Yes, we have one tool that is easy to use and supports almost all programming languages like python, ruby, java, PHP, nodejs, and many more. We are talking about ScrapingBee a multi-language based Web Scraping API that was created by Kevin Sahin and Pierre Dewulf, some of the astonishing features of Scraping Bee are as follows:', 'Read more', 'Yes, a great issue with web scraping was that data is needed by every domain even if its HR, recruitment agency, consultancy firms, Managers, Decision analyst, CEO or any other person who is not into a technical domain that much, but now he/she can also scrape data using GUI (graphical user interface) software like ParseHub.', 'Parsehub is a visual-based tool for web scraping for non-technical domain professionals, which enables everyone to create their own data extraction workflows without worrying about coding.', 'A video demonstration of how easy it is to use parsehub interface without coding', 'Some of the features of ParseHub are as follows:', 'Diffbot is another web scraping tool and it is the most advance of all the GUI-based web scraping tools because it supports machine learning and computer vision that makes the scraping process very fast and handy.', '.', 'Diffbot was created by Mike Tung in 2008 at Stanford University. It was the first company to leverage AI for scraping tasks and it comes out very well today Diffbot is been used by top fortune companies on a daily basis. Now according to an MIT Technology Review report as well.', 'Diffbot is working on the new state of the art AI techniques like GPT-3, but with a different approach, they are trying to vacuuming up a large amount of human-written text data and extracting facts from it instead of training a model directly out of it.', 'It has multiple features and almost contains every feature for web scraping task as follows:']",
209,https://analyticsindiamag.com/scrapingbee_api/,https://analyticsindiamag.com/scrapingbee-api/,"['Web scraping is a technique of extracting data from the internet, and it is the main source of information, as the data is increasing the Web scraping techniques are trending more and more, you can see below how the worldwide popularity of Web Scraping has increased over the years.', 'Now there are many tools for web scraping in the market some provide free service, some paid and other Graphical interfaces for noncoders now based on different needs we have already covered many web scraping tools as follows:', 'Today we are going to discuss ScrapingBee that is very popular and used by many fortune companies on a daily basis for Web scraping tasks.', 'ScrapingBee is a web scraping tool created by Kevin Sahin and Pierre de Wulf, Kevin is a web scraping expert and author of the Java web scraping book, Pierre is a data scientist. ScrapingBee makes scraping the web easy and also they provide API so no need to worry about programming languages. It works well with every language. They solved many problems like headless chrome surfing on the server and dynamically changing proxy IP to never get blocked, ScrapingBee waits for 2000 milliseconds before returning the Source code, because it scrapes web pages HTML like a real browser in the headless environment does. Here are some of the features you should know before getting started.', 'Go to the ScrapingBee website and signup, and they provide a free plan which includes 1000 free API calls, that’s enough to learn and test this API.', 'Now access the dashboard and copy the API key we needed later in this tutorial. ScrapingBee supports multi-language support so you can directly use the API key in your projects from now on.', 'Scaping bee provides REST API support so it can be used with any programming language like CURL, Python, NodeJS, Java, Ruby, Php, and Go. We are going to use Python with the Request framework and BeautifulSoup for further Scraping. Install them using PIP as follows:', 'Use the below code to initiate ScrapingBee web API, here we are creating a Request call with parameters URL, API key and in return, the API responds with an HTML content of the target URL.', 'We can use BeautifulSoup to make this output more readable by just adding a prettify code, learn more about BeautifulSoup.', 'You can also encode the URL you want to scrape by using urllib.parse as follows:', 'Examples are taken from here', 'Example credit', 'We are going to use python language and a simple code using Request, Beautifulsoup, and SrapingBee API for URL requesting and we will extract all the smartphones more specifically tablets from OLX with their names and price:', '', 'In this tutorial, we learned about ScrapingBee: an API used for Web scraping, this API is special because it provides you Javascript rendering of pages for which you need tools like Selenium that supports headless browsing. Javascript rendering is based on the DOM model. Also, we have seen an example where we scraped the product’s name and price from OLX using this API.']","'#  Install the Python Requests library:\npip install requests\n\n# Additional modules we needed during this tutorial:\npip install BeautifulSoup\n'
 'import requests\ndef send_request():\n    response = requests.get(\n        url=""https://app.scrapingbee.com/api/v1/"",\n        params={\n            ""api_key"": ""INSERT-YOUR-API-KEY"",\n            ""url"": ""https://example.com/"",\n        },\n    )\n    print(\'Response HTTP Status Code: \', response.status_code)\n    print(\'Response HTTP Response Body: \', response.content)\nsend_request()\n', 'import urllib.parse\nencoded_url = urllib.parse.quote(""YOUR URL"")\n', 'import java.io.IOException;\nimport org.apache.http.client.fluent.*;\n\npublic class SendRequest\n{\n  public static void main(String[] args) {\n    sendRequest();\n  }\n\n  private static void sendRequest() {\n\n    // Classic (GET )\n\n    try {\n\n      // Create request\n      Content content = Request.Get(""https://app.scrapingbee.com/api/v1/?api_key=YOUR-API-KEY&url=YOUR-URL"")\n\n      // Fetch request and return content\n      .execute().returnContent();\n\n      // Print content\n      System.out.println(content);\n    }\n    catch (IOException e) { System.out.println(e); }\n  }\n}\n', '<?php\n\n// get cURL resource\n$ch = curl_init();\n\n// set url\ncurl_setopt($ch, CURLOPT_URL, \'https://app.scrapingbee.com/api/v1/?api_key=YOUR-API-KEY&url=YOUR-URL\');\n\n// set method\ncurl_setopt($ch, CURLOPT_CUSTOMREQUEST, \'GET\');\n\n// return the transfer as a string\ncurl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);\n\n\n\n// send the request and save response to $response\n$response = curl_exec($ch);\n\n// stop if fails\nif (!$response) {\n  die(\'Error: ""\' . curl_error($ch) . \'"" - Code: \' . curl_errno($ch));\n}\n\necho \'HTTP Status Code: \' . curl_getinfo($ch, CURLINFO_HTTP_CODE) . PHP_EOL;\necho \'Response Body: \' . $response . PHP_EOL;\n\n// close curl resource to free up system resources\ncurl_close($ch);\n?>\n', '#IMPORT MODULES\nImport requests\nfrom bs4 import BeautifulSoup\nfrom time import sleep\n', ""KEY = 'Your_API_key'\nURL = 'https://www.olx.in/tablets_c1455'\nparams = {'api_key': KEY, 'url': URL, 'render_js': 'False'}\n"", ""r = requests.get('http://app.scrapingbee.com/api/v1/', params=params, timeout=20)"", ""if r.status_code == 200:\n    html = r.text\n    soup = BeautifulSoup(html, 'lxml')\n    links = soup.select('.EIR5N')\n"", ""for span in links:\n\n        product_name = span.find('span', {'data-aut-id': 'itemTitle'})\n        print(product_name.text)\n\n        price = span.find('span', {'data-aut-id': 'itemPrice'})\n        print(price.text)\n"", ""import requests\nfrom bs4 import BeautifulSoup\nfrom time import sleep\n\ndef main():\n    KEY = 'G2B2GSAPF1LTBJBAR7F0UT8H0VSLC6V7V6EGJRO3MFWFO3EH'\n    URL = 'https://www.olx.in/tablets_c1455'\n\n    params = {'api_key': KEY, 'url': URL, 'render_js': 'False'}\n\n    r = requests.get('http://app.scrapingbee.com/api/v1/', params=params, timeout=20)\n\n    if r.status_code == 200:\n        html = r.text\n        soup = BeautifulSoup(html, 'lxml')\n        classes = soup.select('.EIR5N')\n        for span in classes:\n            \n            product_name = span.find('span', {'data-aut-id': 'itemTitle'})\n            print(product_name.text)\n            \n            price = span.find('span', {'data-aut-id': 'itemPrice'})\n            print(price.text)\nmain()\n"""
210,https://analyticsindiamag.com/cheerio/,https://analyticsindiamag.com/cheerio/,"['Web scraping is a technique of using robot scripts to crawl the internet for you and return reliable data. Web scrapers are capable of crawling thousands of websites in minutes if implemented correctly with a write toolset and programming language. It is a powerful way of obtaining large amounts of information that can be further cleaned and processed to extract insights. Even in some cases of counterfeit goods, web scraping tools can be used to surf the internet to find fake selling products. And we can report them easily as now we have links to all the sites. Before the web scraping era, it was a very hectic job to manually search and go through each website by yourself. While web scraping may seem simple, the actual process not. Web scraping is a complex process that requires technical knowledge. Fortunately, the industry introduces some tools that can be used without technical knowledge like Diffbot and parsehub.', 'Today we are going to discuss Cheerio is a node.js framework that helps in interpreting and analyzing the web-pages using jQuery-like syntax.', 'Cheerio is a fast, flexible, and lean implementation for the server, but why do we need it when we have puppeteer the same Node.js based web scraping tool because puppeteer is more used for automating browser task as it supports real-time visual surfing of the internet as the script runs. Puppeteer can work in websites built with angular and react, you can take screenshots and make pdfs, but in term of speed Cheerio is a clear winner it is a minimalist tool for doing web scraping and you can combine it with other modules to make an end to end script that will save the output in CSV and return other things too. Cheerio is a perfect fit for web scraping tasks. It works with chrome and raw HTML data.', 'Also, Cheerio works with the simplest consistent DOM model, as shown below:', 'The Document Object Model(DOM) is an interface that treats an HTML document or XML as a tree-like structure as shown in the above images, where each node is an object and it represents the part of the document. DOM representation of a document is in logical tree order. Each branch of the DOM tree contains an object.', 'The history of DOM was also linked back to the “browser wars” of late 1990-', 'Cheerio is not a browser, it is a module in node.js which parses markup and provides an API for manipulating the data structure. It never interprets the result as a web browser does. Specifically, it does not produce a visual representation, apply CSS, loading of external resources, or executing javascript.', 'To use npm commands first install node.js from here, which comes with a prebuilt (npm) node package manager.', 'Always initialize a project in a separate folder with a separate environment using the npm command, you may need to run this every time you manually install dependencies to your project.', 'The above commands will create a package.json file in your directory which contains all the information about the dependencies, author, GitHub repository, and its version too.', 'Now, open your command prompt or terminal and type node, and write the below command to check if everything is installed and working, this command will import the cheerio module.', 'An example we are using is this small HTML document, save this code in a file.html in the same directory.', 'Now for loading a file from a local directory, we need to install one more module, i.e. file system for file handling as we do in python and other languages to read lines or update data in a file. Use the following command to install fs in your environment. Considering Cheerio is already installed from the above quickstart tab.', 'To load a HTML document in with cheerio use following commands:', 'Cheerio selector commands are simple and identical to jQuery’s if you are familiar with the language. Append the below command in the last of your javascript file and save it in demo.js', '', 'Similarly,', '.attr(name, value) is used to modify attributes. If you set an attribute value to NULL, it means it is removed. You can also pass a map and function.', '', '.prop(name, value)\xa0 for setting properties.', '.data(name, value) for setting data attributes. Run the below commands and see what’s the output.', 'There are many others methods For forms we have methods like:', '\xa0[.serialize()](#serialize) – [.serializeArray()](#serializearray).', 'For traversing we have:', '\xa0\xa0[.find(selector)](#findselector)\xa0 [.nextAll([selector])](#nextallselector) functionindex-element-) – [.filter( selector )', '.filter( selection )', '.filter( element ) .not( function(index, elem) )](#not-selector—not-selection—not-element—not-functionindex-elem-) – [.has( selector )', 'For manipulation :', '– [.append( content, [content, …] )](#append-content-content–) – [.appendTo( target )](#appendto-target-) – [.html( [htmlString] )](#html-htmlstring-) – [.css( [propertName] )', '.css( [ propertyNames] )', '.css( [propertyName], [value] )', '.css( [propertName], [function] )', '.css( [properties] )](#css-propertname—css–propertynames—css-propertyname-value—css-propertname-function—css-properties-)', 'Read more about their usage here.', 'We are going to scrape the table rows from the Wikipedia page: List of largest companies by revenue here, this example is taken from StackOverflow, we are going to learn how this script can extract the table from Wikipedia and also exported in CSV for our further data analysis.', 'Cheerio is a great tool and performs very fast in web scraping tasks. We learned about some of the methods of cheerio and also gone into the history of DOM models and then finished it with a web scraper that can scrape data from Wikipedia in seconds, Cheerio is actively maintained by cheeriojs, so if you are interested in contributing, you can start reading their contribution instruction here. Cheeriojs have other projects too like dom-serializer(render dom nodes), jsdoc(an API documentation generator for JavaScriopt), cheerio-select(CSS selector engine supporting jQuery selectors). You should learn more about cheerio here.']","'npm i cheerio'
 'mkdir foldername\ncd foldername\n\n#Initialize npm and \n#store dependencies in node_modules folder\nnpm init -y\n\n# install cheerio in this environment\nnpm i cheerio\n', ""const cheerio = require('cheerio');"", '<ul id=""fruits"">\n  <li class=""apple"">Apple</li>\n  <li class=""orange"">Orange</li>\n  <li class=""pear"">Pear</li>\n</ul>\n', 'npm i fs', ""#importing modules\nconst cheerio = require('cheerio');\nvar fs = require('fs');\n\n#loading HTML document\nconst $ =cheerio.load(fs.readFileSync('file.html'));\n"", "" $('.apple', '#fruits').text()"", ""$('ul .pear').attr('class')\n//output=> pear\n\n$('li[class=orange]').html()\n//output=> Orange\n"", ""$('ul').attr('id')\n\n$('.apple').attr('id', 'favorite').html()\n"", '$(\'input[type=""checkbox""]\').prop(\'checked\')\n\n$(\'input[type=""checkbox""]\').prop(\'checked\', true).val()\n', '$(\'<div data-apple-color=""red""></div>\').data()\n\n$(\'<div data-apple-color=""red""></div>\').data(\'apple-color\')\n\nconst apple = $(\'.apple\').data(\'kind\', \'mac\')\n', 'npm i request\nnpm i json2csv\nnpm i fs\n', 'var request = require(""request-promise"")\nvar cheerio = require(""cheerio"")\nvar fs = require(""fs"") \nvar json2csv = require(""json2csv"").Parser\n', 'const wiki = ""https://en.wikipedia.org/wiki/List_of_largest_companies_by_revenue"";', '(async () => {\n        const response = await req({\n            uri: wiki,\n            headers: {\n                accept:\n                    ""text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9"",\n                ""accept-encoding"": ""gzip, deflate, br"",\n                ""accept-language"": ""en-US,en;q=0.9""\n            },\n            gzip: true,\n', ""}).then(function (html) {\n            let $ = cheerio.load(html);\n            let data = [];\n            let data2 = [];\n            let name, rank, cols, col;\n            let rows = $('table.wikitable tbody tr').each((idx, elem) => {\n                rank =$(elem).find('th').text().replace(/[\\n\\r]+/g,'');\n                //name = $(elem).find('td a').html();\n                data2 = [];\n                cols = $(elem).find('td').each((colidx, colelem) => {\n                    col = $(colelem).text().replace(/[\\n\\r]+/g,'');\n                    data2.push(col,);    \n                });\n\n                data.push({\n                    rank,\n                    ...data2,\n                });\n            });\n"", '            // exporting data into csv\n            const j2cp = new json2csv()\n            const csv = j2cp.parse(data);\n\n            fs.writeFileSync(""./output.csv"", csv, ""utf-8"");\n        }).catch(function (err) {\n            console.log(err);\n        });\n    }\n    )();\n'"
211,https://analyticsindiamag.com/web_scraping_frameworks/,https://analyticsindiamag.com/web-scraping-frameworks/,"['Web scraping provides a way for people & businesses to understand what one can achieve from a fair amount of data, you can challenge your competitors and surpass them just by doing a good data analysis and research on scraped data from the web. Maybe, from an individual perspective too, if you are looking for a job; Automated web scraping can help you process all the jobs posted on the internet in your spreadsheet where you can filter them out by your skills and experience. In contrast, when you spend hours to get the information you want, now it’s easy for you to create your web scraping script that can work like a charm for all your manual hours of labor.\xa0', 'There is so much information on the internet, and new data is generated every second, so manually scraping and researching is not possible, That’s why we need automated web scraping to accomplish our goals.', 'Web scraping became an essential part of every business, individual, and even government.', 'There are some challenges too in the web scraping domain like the continuous change in websites so our web scraper may not work for next time, that’s why people came up with a more diverse approach like Diffbot: A visual-based web scraping tool which uses computer vision, machine learning, and NLP combined to achieve a universal web scraping techniques that are more powerful, accurate, and easy to use.', 'Another problem faced in web scraping is the variety all web sites are different in design and coding structure so we can’t use a single web scraping script everywhere to achieve results. There must be a continuous change in code as the website changes.', 'Today we are going to discuss some of the libraries that can reduce your web scraper building time and are essentials for web scraping purposes, as they are the building blocks on which everything is built.', 'Urllib is a package that combines several modules to preprocess the URLs. In simple words, it is an HTTP client for python programming languages, the latest version of\xa0', 'Urllib is urllib3 1.26.2 which supports thread-safe connection, connection pooling, client-side verification using SSL/TLS verification, multipart encoding, support for gzip, and brotli encoding. It brings many critical features that are missing from traditional python libraries.', 'Urllib3 is one of the widely downloaded packages on PyPi, and it is the first to execute in any web scraping script, it is available under the MIT license.', '', 'By using urllib.request we can simply open and read URLs.', 'urllib.error defines the exceptions and errors raised by the urllib.request command.', 'urllib.parse is used for parsing URLs.', 'urllib.robotparser is used for parsing robots.txt files.', 'We can easily fetch data without using any other module; just urllib and re (regular expression)', 'Let’s understand the code explained above:', 'We can use a span tag in the regular expression findall function instead, to extract all the titles of the article’s name as we did in this BeautifulSoup tutorial. But now with just the help of the two lightest modules urllib and re.', 'Requests is an open-source python library that makes HTTP requests more human-friendly and simple to use. It is developed by Kenneth Reitz, Cory Benfield, Ian Stapleton Cordasco, Nate Prewitt with an initial release in February 2011.', 'Requests module library is Apache2 licensed, which is written in Python.', 'Sounds pretty much the same as urllib, then why do we need it?', 'Because requests support a fully restful API, and it is easier to use and access.', 'Even though the requests library is powered by urllib3 it is still used more nowadays because of its readability and POST/GET freedom and many more.', 'Also, the urllib API is thoroughly broken, it was built for a different time and different web structure, Urllib requires more amount of work then Requests for the simplest task; So, now we need a more flexible HTTP client, i.e. requests', 'You don’t need to encode parameters like urllib3, just pass a dictionary as an argument, and you are good to go:', 'It also has its own JSON decoder:', 'Or If the response is text; Use :', 'We are using requests and beautiful soup for processing and finding information or we can use regular expressions as shown in the above demonstration of urllib3.', 'For this demonstration, we are using requests with beautiful soup, and we are scraping the articles from a website\xa0', 'We can use the requests module to request our web API to get answers like in this case we are using POST on web API: https://loan5.herokuapp.com/api', 'This API is used to predict loan approval. It returns 1 or 0, i.e. approved or disapproved on passing some attributes like gender, credit history, married, etc.', 'We have learned how the urllib and request two python modules can help in web scraping from scratch, there are many ways to execute your web scraper like in the previous article we used selenium for web scraping then we combined selenium with beautiful soup and now we have integrated request module instead of selenium with beautiful soup.']","'pip install urllib3'
 'git clone git://github.com/urllib3/urllib3.git\npython setup.py install\n', ""import urllib3\n\nhttp = urllib3.PoolManager()\nr = http.request('GET', 'http://httpbin.org/robots.txt')\nr.status\nR.data\n"", ""#1 libraries needed\nimport urllib.request\nimport urllib.parse \nimport re \n\n#2 search   \nurl = 'https://analyticsindiamag.com/'\nvalues = {'s':'Web Scraping', \n          'submit':'search'} \n\n#3 parse\ndata = urllib.parse.urlencode(values) \ndata = data.encode('utf-8') \nreq = urllib.request.Request(url, data) \nresp = urllib.request.urlopen(req) \nrespData = resp.read() \n\n#4 extract using regular expressions\ndocument = re.findall(r'<p>(.*?)</p>',str(respData)) \n   \nfor line in document: \n    print(line) \n"", ""import requests\n\nresp = requests.get('http://www.yourwebsite.com/user')\nresp = requests.post('http://www.yourwebsite.com/user')\nresp = requests.put('http://www.yourwebsite.com/user/put')\nresp = requests.delete('http://www.yourwebsite.com/user/delete')\n"", 'attributes = {""firstname"": ""John"", ""lastname"": ""Edison"", ""password"": ""jeddie123""} \nresp = requests.post(\'http://www.yourwebsite.com/user\', data=attributes)\n', 'resp.json()', 'resp.text', ""#1 importing modules\nimport requests\nfrom bs4 import BeautifulSoup\n\n#2 using .GET()\nres = requests.get('https://analyticsindiamag.com/')\n\n#3 beautiful for extracting only reliable data\nsoup = BeautifulSoup(res.text, 'html.parser')\narticle_block =soup.find_all('div',class_='post-title')\nfor titles in article_block:\n\ttitle =titles.find('span').get_text()\n\tprint(title)\n"", ""#1\nimport json\nimport requests\nurl= 'https://loan5.herokuapp.com/api'\n\n#2 sample data\ndata={'Gender':1, 'Married':1, 'Dependents':2, 'Education':0, 'Self_Employed':1,'Credit_History':0,'Property_Area':1, 'Income':1}\ndata = json.dumps(data)\n\n#3 sending requesting with data to webapi and it will \nReturn the answer.\nsend_req = requests.post(url, data)\nprint(send_req.json())\n"""
212,scraping_without_twitters_api/,https://analyticsindiamag.com/complete-tutorial-on-twint-twitter-scraping-without-twitters-api/,"['Web Scraping allows us to download data from different websites over the internet to our local system. It is data mining from different online portals using Hypertext Transfer Protocols and uses this data according to our requirements. Many companies use this for data harvesting and for creating search engine bots.\xa0', 'Python has a large variety of packages/modules that can help in the process of web scraping like beautiful soup, selenium. Several libraries are there which can automate the process of web scraping like Autoscraper. All these libraries use different APIs through which we can scrape data and store it into a data frame in our local machine.', 'Twint is an open-source python library that is used for twitter scraping i.e we can use twint in order to extract data from twitter and that too without using the twitter API. There are certain features of twint which makes it more useable and unique from other twitter scraping API, namely:', 'Twint can be used to scrape tweets using different parameters like hashtags, usernames, topics, etc. It can even extract information like phone number and email id’s from the tweets.', 'In this article, we will explore twint and see what different functionalities it offers for scraping data from twitter.', 'We will start by installing twint using pip install twint.', 'We will be scraping data from twitter using twint so we will import twint other than this we need to import net_asyncio which will handle all the notebook and runtime errors. Also, we will initiate the net_syncio in this step only.', 'import twint', 'import nest_asyncio', 'net_asyncio.apply()', 'We need to scrape data from twitter using twint before that we need to configure the twint object and call it whenever required.\xa0', 't = twint.Config()', 'Now let us start scraping different types of data from twitter.', 'Here, we will see how we can download the names of the followers of a particular user by using their username. Here I am using my own twitter username.', 't.Username = ""Himansh70809561""', 'twint.run.Followers(t)', 'Here you can see a list of my followers on twitter because I used my username, similarly, you can use the different usernames of different users and download the follower’s name.', 'We can also store the information into a data frame. Let us see how to store the follower’s details in a data frame.', 't.Limit = 30', ""t.Username = 'Analyticsindiam'"", 't.Pandas = True', 'twint.run.Followers(t)', 'follow_df = twint.storage.panda.User_df', 'Here we saw that the top 30 followers are stored in a data frame. We can set the number of followers to the desired number.\xa0', 'Here we will try and extract all tweets which have a particular word in them which we define.', 't.Search = ""analytics""', 't.Store_object = True', 't.Limit = 10', 'twint.run.Search(t)', 'tlist = t.search_tweet_list', 'print(tlist)', 'The output contains tweet from different users with their usernames and tweet along with the date when a tweet is published.', 'We can also extract tweets from different users by entering their username as the parameter.', 't.Search = ""from:@Analyticsindiam""', 't.Store_object = True', 't.Limit = 10\xa0', 'twint.run.Search(t)', 'tlist = t.search_tweet_list', 'Here we can see some recent tweets from Analytics India Magazine along with their username and date on which they were published.', 'These are some of the ways with which we can extract data or scrape data from twitter using twint. Twint contributors are actively contributing to making it better and better day by day.']","'import twint'
 'import nest_asyncio', 'net_asyncio.apply()', 't = twint.Config()', 't.Username = ""Himansh70809561""', 'twint.run.Followers(t)', 't.Limit = 30', ""t.Username = 'Analyticsindiam'"", 't.Pandas = True', 'twint.run.Followers(t)', 'follow_df = twint.storage.panda.User_df', 't.Search = ""analytics""', 't.Store_object = True', 't.Limit = 10', 'twint.run.Search(t)', 'tlist = t.search_tweet_list', 'print(tlist)', 't.Search = ""from:@Analyticsindiam""', 't.Store_object = True', 't.Limit = 10', 'twint.run.Search(t)', 'tlist = t.search_tweet_list'"
213,https://analyticsindiamag.com/diffbot/,https://analyticsindiamag.com/diffbot/,"['Earlier, we have seen many web scrapers that can extract data from websites, but many times in the case when sites are changing dynamically over time, it’s hard to scrape and locate elements. Businesses do everything to make their websites free from web crawlers so for solving these problems and making a multi-functional, and more diverse tool Diffbot introduced machine learning and computer vision algorithms and public APIs for extracting information from web pages.', 'Diffbot was created by Mike Tung in 2008 at Stanford University. It was the first company that was funded by StartX\xa0 (Stanford’s on-campus venture for funding).', 'Diffbot was the first company to introduce Computer Vision technology to scrape information from web pages, no more conditional programming for each element instead Diffbot visually parses the website’s pages and returns the important elements.', '\xa0In 2012 they introduced Page Classifier API, which can automatically categorize the web pages into specific categories. This adoption of AI systems into their tools was a good move as they were able to analyze 750,000 web pages from Twitter.', '\xa0In 2019 they introduced Knowledge Graph which automatically extracts data from webpages and they build a knowledge base of 2 billion attributes(products, articles, people, companies, and more) and 10 trillion “facts”.', 'This was a huge shot because now their web crawler was able to scrape tiny details from websites which are impossible for other web scraping service providers.', 'Now according to Financial Express report as OpenAI showcased there GPT-3, an advanced version of AI bot, and now according to MIT Technology Review report as well, Diffbot is working on the same, but with a different approach, they are trying to vacuuming up a large amount of human-written text and extracting facts from it instead of training a model directly out of it.', 'You can read more here.\xa0', 'This product is more for business purposes. So You need your work email to sign up.', 'Diffbot provides basic four services:', 'After Signup, you’ll get a 14-days free trial which includes 10,000 free credits, access to the knowledge graph, Diffbot cloud dashboard, Excel and google sheets integration and Developer APIs.', 'If Login was successful, then you can see your dashboard right here at:\xa0', 'https://app.diffbot.com/', 'Diffbot offers many APIs for extracting data from webpages using computer vision and NLP(Natural language processing), and they are able to categorize the whole page into different attributes and return as JSON.', 'If you know what type of content your URL contains, use one of the page-type specific APIs as follows:', 'Custom API The Custom API can be used to create an entirely new custom web scrapers by defining rules. You can also use the Custom API programmatically.', 'You can integrate Diffbot with your choice of favorite apps like Google Sheets, Excel, Tableau(Data Visualization tool), and salesforce.', 'And specify your own rules for extracting data, custom APIs are not focused on this Demonstration as we are going to deep dive into the knowledge graph.', 'From last 6 Years the APIs is not being maintained', 'This API is mainly designed for developers. Thus, you can take control of the full API from your IDE but it is not maintained and used widely as it has only 13 stars and ten forks on Github.', 'Copy your unique token from the Diffbot dashboard!', 'For extracting a specific part from source code, you can achieve by doing the following:', 'To POST data (text or HTML) to the API, use the text or HTML arguments:', 'Knowledge is one heck of a powerful tool able to scrape the whole internet in a minute and will give you corresponding results with customizable entities.', '1000 Samsung smartphones details with price, selling site, category and more within just 10 seconds ????', 'Download dataset from here', 'Use Case: You are a Data Scientist. You are given a task to do sentiment analysis on your product like what people have been writing about it, how positive or negative impact it is making on the internet and what are the drawbacks we need to focus?', '\xa0So one way to answer all these questions we can create a large dataset of all articles published on the internet of our product and then do sentiment analysis and research on the same to find out.', 'Our product name is Diffbot, Here we are scraping all the ‘Diffbot’ named articles i.e all the articles written on Diffbot over the years we are going to extract with attributes like publishers, sentiment, tags, URLs, text (important ) and more.', 'Structured dataset ready for sentiment analysis for our data science project ????', '', 'Download dataset from here', 'We saw different types of services, tools, and a full demonstration on Diffbot Knowledge Graph with two Use – case also we have used python API too.', 'Diffbot is a great tool as we have already seen and it has maintained its reputation over the years with its AI power services and further they are trying to improve their knowledge graph. 1000 of developers from fortune 500 companies rely on Diffbot on a daily basis because of its simplicity and accessibility.', 'Want to hear more about web scraping frameworks click on these links:']","'pip install diffbot'
 ""import diffbot\njson_result = diffbot.article('https://github.com', token='your token here')"", ""import diffbot\nclient = diffbot.Client(token='#')\n\njson_result = client.api('article', 'https://github.com', html='''\n... <h1>Introducing GitHub Traffic Analytics</h1>\n... <p>We want to kick off 2014 with a bang, so today we're happy to launch\n... traffic analytics!</p>\n... ''')\n"""
214,predictive_models_using_pyhealth/,https://analyticsindiamag.com/how-to-build-healthcare-predictive-models-using-pyhealth/,"['Machine learning has been applied to many health-related tasks, such as the development of new medical treatments, the management of patient data and records, and the treatment of chronic diseases. To achieve success in those SOTA applications, we must rely on the time-consuming technique of model building evaluation. To alleviate this load, Yue Zhao et al have proposed a PyHealth, a Python-based toolbox. As the name implies, this toolbox contains a variety of ML models and architecture algorithms for working with medical data. In this article, we will go through this model to understand its working and application. Below are the major points that we are going to discuss in this article.', 'Let’s first discuss the use case of machine learning in the healthcare industry.\xa0\xa0', 'Machine learning is being used in a variety of healthcare settings, from case management of common chronic conditions to leveraging patient health data in conjunction with environmental factors such as pollution exposure and weather.', 'Machine learning technology can assist healthcare practitioners in developing accurate medication treatments tailored to individual features by crunching enormous amounts of data. The following are some examples of applications that can be addressed in this segment:', 'The ability to swiftly and properly diagnose diseases is one of the most critical aspects of a successful healthcare organization. In high-need areas like cancer diagnosis and therapy, where hundreds of drugs are now in clinical trials, scientists and computationalists are entering the mix. One method combines cognitive computing with genetic tumour sequencing, while another makes use of machine learning to provide diagnosis and treatment in a range of fields, including oncology.', 'Medical imaging, and its ability to provide a complete picture of an illness, is another important aspect in diagnosing an illness. Deep learning is becoming more accessible as data sources become more diverse, and it may be used in the diagnostic process, therefore it is becoming increasingly important. Although these machine learning applications are frequently correct, they have some limitations in that they cannot explain how they came to their conclusions.', 'ML has the potential to identify new medications with significant economic benefits for pharmaceutical companies, hospitals, and patients. Some of the world’s largest technology companies, like IBM and Google, have developed ML systems to help patients find new treatment options. Precision medicine is a significant phrase in this area since it entails understanding mechanisms underlying complex disorders and developing alternative therapeutic pathways.', 'Because of the high-risk nature of surgeries, we will always need human assistance, but machine learning has proved extremely helpful in the robotic surgery sector. The da Vinci robot, which allows surgeons to operate robotic arms in order to do surgery with great detail and in confined areas, is one of the most popular breakthroughs in the profession.\xa0', 'These hands are generally more accurate and steady than human hands. There are additional instruments that employ computer vision and machine learning to determine the distances between various body parts so that surgery can be performed properly.', 'Health data is typically noisy, complicated, and heterogeneous, resulting in a diverse set of healthcare modelling issues. For instance, health risk prediction is based on sequential patient data, disease diagnosis based on medical images, and risk detection based on continuous physiological signals.\xa0', 'Electroencephalogram (EEG) or electrocardiogram (ECG), for example, and multimodal clinical notes (e.g., text and images). Despite their importance in healthcare research and clinical decision making, the complexity and variability of health data and tasks need the long-overdue development of a specialized ML system for benchmarking predictive health models.', 'PyHealth is made up of three modules: data preprocessing, predictive modelling, and assessment. Both computer scientists and healthcare data scientists are PyHealth’s target consumers. They can run complicated machine learning processes on healthcare datasets in less than 10 lines of code using PyHealth.', 'The data preprocessing module converts complicated healthcare datasets such as longitudinal electronic health records, medical pictures, continuous signals (e.g., electrocardiograms), and clinical notes into machine learning-friendly formats.\xa0', 'The predictive modelling module offers over 30 machine learning models, including known ensemble trees and deep neural network-based approaches, using a uniform yet flexible API geared for both researchers and practitioners.\xa0', 'The evaluation module includes a number of evaluation methodologies (for example, cross-validation and train-validation-test split) as well as prediction model metrics.', 'There are five distinct advantages to using PyHealth. For starters, it contains more than 30 cutting-edge predictive health algorithms, including both traditional techniques like XGBoost and more recent deep learning architectures like autoencoders, convolutional based, and adversarial based models.\xa0', 'Second, PyHealth has a broad scope and includes models for a variety of data types, including sequence, image, physiological signal, and unstructured text data. Third, for clarity and ease of use, PyHealth includes a unified API, detailed documentation, and interactive examples for all algorithms—complex deep learning models can be implemented in less than ten lines of code.', 'Fourth, unit testing with cross-platform, continuous integration, code coverage, and code maintainability checks are performed on most models in PyHealth. Finally, for efficiency and scalability, parallelization is enabled in select modules (data preprocessing), as well as fast GPU computation for deep learning models via PyTorch.', 'PyHealth is a Python 3 application that uses NumPy, scipy, scikit-learn, and PyTorch. As shown in the diagram below, PyHealth consists of three major modules: First is the data preprocessing module can validate and convert user input into a format that learning models can understand;\xa0', 'Second is the predictive modelling module is made up of a collection of models organized by input data type into sequences, images, EEG, and text.\xa0 For each data type, a set of dedicated learning models has been implemented, and the third is the evaluation module can automatically infer the task type, such as multi-classification, and conduct a comprehensive evaluation by task type.', 'Most learning models share the same interface and are inspired by the scikit-API learn to design and general deep learning design: I fit learns the weights and saves the necessary statistics from the train and validation data; load model chooses the model with the best validation accuracy, and inference predicts the incoming test data.\xa0', 'For quick data and model exploration, the framework includes a library of helper and utility functions (check parameter, label check, and partition estimators). For example, a label check can check the data label and infer the task type, such as binary classification or multi-classification, automatically.', 'PyHealth for model building', 'Now below we’ll discuss how we can leverage the API of this framework. First, we need to install the package by using pip.\xa0', '! pip install pyhealth', 'Next, we can load the data from the repository itself. For that, we need to clone the repository. After cloning the repository inside the datasets folder there is a variety of datasets like sequenced based, image-based, etc. We are using the mimic dataset and it is in the zip form we need to unzip it. Below is the snippet clone repository, and unzip the data.\xa0', 'The unzipped file is saved in the current working directory with the name of the folder as a mimic. Next to use this dataset we need to load the sequence data generator function which serves as functionality to prepare the dataset for experimentation.\xa0', 'Now we have loaded the dataset. Now we can do further modelling as below.', 'Here is the fitment result.']",'! pip install pyhealth'
215,for_automatic_music_transcription/,https://analyticsindiamag.com/a-guide-to-omnizart-a-general-toolbox-for-automatic-music-transcription/,"['The activity of notating, reproducing, or otherwise memorizing existing pieces of music is known as music transcription. Music transcription includes melodies, chords, basslines, entire orchestral arrangements, and other aspects. In this post, we will take a look at Yu-Te Wu et al’s recently published Python-based toolbox, which transcribes a given audio music file into the various modes stated above. The following are the important points to be discussed in this article.', 'Let’s start the discussion by understanding what is Music Transcription?\xa0', 'Transcription, in music, is the process of notating a piece of sound that was previously unannotated and/or despised as written music, such as a jazz improvisation or a video game soundtrack. When a musician is tasked with creating sheet music from a recording, he or she creates a musical transcription by writing down the notes that make up the composition in music notation.\xa0', 'Transcription also refers to the practice of adapting a solo or ensemble piece of music for a different instrument or instruments than those originally intended. In this context, transcription and arrangement are sometimes used interchangeably, however, transcriptions are strictly speaking faithful adaptations, whereas arrangements change significant characteristics of the original composition.', 'Omnizart is a new Python library that offers a simplified solution for automatic music transcription (AMT). Omnizart includes modules that build the life-cycle of deep learning-based AMT and is designed for ease of use with a small command-line interface. Omnizart is the first transcription toolkit to include models for a wide range of instruments, including solo, instrument ensembles, percussion instruments, and vocal, as well as models for chord recognition and beat/downbeat tracking, two MIR tasks closely related to AMT. Omnizart includes the following features:\xa0', 'Let’s briefly discuss how does it transcribe the music into six types.', 'In the Omnizart model, the piano solo transcription model is a U-net structure that generates a time pitch representation with a time resolution of 20ms and a pitch resolution of 25 cents (1/4 semitone). The pitch activation (i.e. piano roll) channel, the onset channel, and the offset channel are the three 2-D channels in the output time-pitch representation. These output channels are used to get the MIDI transcription results.', 'The multi-instrument transcription model is identical to the piano solo model, but its output includes 11 instrument classes from the MusicNet training dataset, including piano, violin, viola, cello, flute, horn, bassoon, clarinet, harpsichord, contrabass, and oboe. By default, this model supports the problematic instrument-agnostic transcription scenario, which implies that the instrument classes in the test music piece are unknown.\xa0', 'To achieve multi-instrument transcription, the model generates 11 channels of piano rolls, each representing a distinct type of instrument. This model has the same time and pitch resolutions as the piano solo transcription model.', 'The model is built on a convolutional neural network (CNN) and is designed to anticipate the commencement of percussive events from auditory input. It has five convolutional layers, one attention layer, and three fully-connected layers, with a total of roughly 9.4 million parameters.\xa0', 'Because the onsets of percussive events are significantly connected with beats, the input spectrogram is processed using an automatic beat-tracker in the data pre-processing pipeline. The model is then fed the processed input, which contains rich beat information, for onset prediction.', 'The voice transcription model is a hybrid network that receives a multi-channel feature consisting of the spectrum, generalized cepstrum, and generalized cepstrum of spectrum derived from the input audio and outputs the transcribed MIDI result.', 'The Harmony Transformer (HT), a deep learning model for harmony analysis, is used to construct Omnizart’s harmony recognition feature. The HT model identifies the chord changes and chord progression of input music using an encoder-decoder architecture.\xa0', 'The encoder conducts chord segmentation on the input, and the decoder detects the chord progression based on the outcome of the segmentation. The HT exhibited its potential capability of harmony recognition with this unique technique.', 'Most open-source beat/downbeat tracking tools, such as madmom [24] and librosa [25], only accept audio signal input, but the adopted approach does not. The model accepts MIDI data as input and produces beat and downbeat positions in seconds with a 10ms time precision.\xa0', 'The model is based on a bidirectional LSTM (BLSTM) recurrent neural network (RNN) with an optional attention mechanism and a fully connected layer. Piano roll, spectral flux, and inter-onset interval are all collected from MIDI and used as input characteristics.\xa0', 'By default, the BLSTM network’s hidden units have a dimension of 25. To forecast the probability values of the beat and downbeat for each time step, the model uses the multi-tasking learning (MTL) architecture.', 'In this section, we take music from YouTube and will try to transcribe it on the type that Omnizart offers.', 'Below is the audio version of the file that I have chosen.']","'!pip install -U pip\n!pip install git+https://github.com/Music-and-Culture-Technology-Lab/omnizart\n!omnizart download-checkpoints\n!apt install fluidsynth\n!pip install pyfluidsynth\n!curl -L https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp -o /usr/local/bin/yt-dlp\n!chmod a+rx /usr/local/bin/yt-dlp'
 'import os\nfrom google.colab import files\nfrom IPython import display as dsp\n \nurl = input(""Enter your YouTube link: "")\n \ntry:\n  id = url.split(""watch?v="")[1].split(""&"")[0]\n  vid = dsp.YouTubeVideo(id)\n  dsp.display(vid)\nexcept Exception:\n  pass\n \nprint(""Downloading..."")\n \n!yt-dlp -x --audio-format mp3 --no-playlist ""$url""\n!yt-dlp --get-filename --no-playlist ""$url"" > tmp\n \nuploaded_audio = os.path.splitext(open(""tmp"").readline().strip())[0]\n!ffmpeg -i ""$uploaded_audio"".mp3 ""$uploaded_audio"".wav &> /dev/null\n \nprint(f""Finished: {uploaded_audio}"")\n', '# available_mode_for_trasncription \ntrasncription_modes = [""music-piano"", ""music-piano-v2"", ""music-assemble"", ""chord"", ""drum"", ""vocal"", ""vocal-contour"", ""beat""]\n \nmode = ""drum"" \nmodel = """"\nif mode.startswith(""music""):\n  mode_list = mode.split(""-"")\n  mode = mode_list[0]\n  model = ""-"".join(mode_list[1:])\n \n \nfrom omnizart.music import app as mapp\nfrom omnizart.chord import app as capp\nfrom omnizart.drum import app as dapp\nfrom omnizart.vocal import app as vapp\nfrom omnizart.vocal_contour import app as vcapp\nfrom omnizart.beat import app as bapp\n \napp = {\n    ""music"": mapp,\n    ""chord"": capp,\n    ""drum"": dapp,\n    ""vocal"": vapp,\n    ""vocal-contour"": vcapp,\n    ""beat"": bapp\n}[mode]\n \nmodel_path = {\n    ""piano"": ""Piano"",\n    ""piano-v2"": ""PianoV2"",\n    ""assemble"": ""Stream"",\n    ""pop-song"": ""Pop"",\n    """": None\n}[model]\n \nmidi = app.transcribe(f""{uploaded_audio}.wav"", model_path=model_path)\n \n# Synthesize MIDI and play\nimport scipy.io.wavfile as wave\nfrom omnizart.remote import download_large_file_from_google_drive\n \nSF2_FILE = ""general_soundfont.sf2""\nif not os.path.exists(SF2_FILE):\n  print(""Downloading soundfont..."")\n  download_large_file_from_google_drive(\n      ""16RM-dWKcNtjpBoo7DFSONpplPEg5ruvO"",\n      file_length=31277462,\n      save_name=SF2_FILE\n    )\n \nif mode == ""vocal-contour"":\n  os.rename(f""{uploaded_audio}_trans.wav"", f""{uploaded_audio}_synth.wav"")\nelse:\n  print(""Synthesizing MIDI..."")\n  out_name = f""{uploaded_audio}_synth.wav""\n  raw_wav = midi.fluidsynth(fs=44100, sf2_path=SF2_FILE)\n  wave.write(out_name, 44100, raw_wav)\n \n!ffmpeg -i ""$out_name"" ""tmp_synth.mp3"" &>/dev/null\n!mv tmp_synth.mp3 ""$uploaded_audio""_synth.mp3\n \nout_name = out_name.replace("".wav"", "".mp3"")\nprint(f""Finished: {out_name}"")\ndsp.Audio(out_name)\n'"
216,stock_data_with_code/,https://analyticsindiamag.com/top-python-libraries-to-get-historical-stock-data-with-code/,"['Stock market analysis has always been a very interesting work not only for investors but also for analytics professionals. To analyze the stock market, it needs to have the historical data of the stocks. Finding historical data used to be tedious, time-consuming and costly in the past. With the advancement of financial technologies (FinTech) and the trend toward inclusive finance, there are now a variety of free-market data sources available online. In this post, we will discuss the popular python packages which can be used to retrieve the historical data of a single or multiple stocks. We will see how with only a few lines of codes, we can download the data of years within seconds. The python packages that we are going to cover in this article are listed below.', 'The first method that we are going to see is for collecting data with Pandas-DataReader. Pandas is a Python library for data analysis and manipulation that is a free source. As a result, the Pandas-DataReader subpackage supports the user in building data frames from various internet sources. It allows users to connect to a range of sources, such as Naver Finance, Bank of Canada, Google Analytics, Kenneth French’s data repository, and 16 more such sources as mentioned in its documentation.\xa0 Following the connection, we can extract the data and read it in as a data frame.', 'While retrieving any stock price or data in sequence certain arguments that need to be defined in most of all the packages are;', 'When you get output values of any stock, in most of the cases the output of the query is a pandas data frame and the fields of those data frames are described below:\xa0', 'Pandas DataRedears is not a data source in and of itself, but rather an API in the PyData stack that enables a multitude of data sources. The data will be downloaded as a pandas Dataframe, as the name implies. The complete document is available here. The sources that it currently supports are listed below. We will only go through a few of them.\xa0', 'Alpha Vantage provides enterprise-grade financial market data through a set of powerful and developer-friendly APIs. To set up this environment you will need to have an API key, it can be straightly taken from the documentation here.\xa0', 'Here’s how the obtained data frame looks like:', 'The Federal Reserve Economic Data (FRED) database is managed by the Research division of the Federal Reserve Bank of St. Louis and contains over 765,000 economic time series from 96 sources. All such huge data can be accessed by the DataReader API just under the symbol category we need to mention for which indicator we want the data. Indicators can be found here.\xa0', 'As passed above, it shows the trading categories that are imported from Japan and China.', 'Yahoo! Finance is a component of Yahoo’s network. It is the most widely used business news website in the United States, featuring stock quotes, press announcements, financial reports, and original content, as well as financial news, data, and commentary. They provide market data, fundamental and option data, market analysis, and news for cryptocurrencies, fiat currencies, commodities futures, equities, and bonds, as well as fundamental and option data, market analysis, and news.\xa0', 'The above image is the web interface of Yahoo Finance which markets the status of different cryptocurrencies. To retrieve such data Yahoo finance has its dedicated tool called yfinance. It is really simple and straightforward, as you will go through the below API under which you need to change only the symbol ( left-most column in the above image) \xa0', 'Also, we can take multiple trades into account as given below.', 'Twelve Data was created in 2009 and has recently gained traction. The following are the major elements of the services they provide:', 'The TwelveData project’s main purpose is to offer a single location where all Pythonistas may receive fast access to all financial markets and analyze them with just a few lines of code.', 'We must first register on their website and obtain our API KEY, the same as we did with Alpha Vantage.\xa0', 'Using the Twelve data we will query the stock price of Microsoft corporation and also we will plot an interactive Plotly Dashboard.\xa0\xa0']","""## Alpha vintage\nimport pandas as pd\nimport pandas_datareader as pdr\nts = pdr.av.time_series.AVTimeSeriesReader('IBM'
 api_key=PUT_YOUR_API_KEY_HERE)\ndf = ts.read()\ndf.index = pd.to_datetime(df.index, format='%Y-%m-%d')\n# plotting the opening and closing value \ndf[['open','close']].plot()\n"", ""### Fred \nimport pandas_datareader as pdr\nstart = datetime(2021, 1, 1)\nend = datetime(2021, 9, 30)\nsyms = ['IMPCH', 'IMPJP']\ndf = pd.DataFrame()\nfor sym in syms:\n  ts = pdr.fred.FredReader(sym, start=start, end=end)\n  df1 = ts.read()\n  df = pd.concat([df, df1], axis=1)\ndf\xa0"", ""!pip install yfinance\nimport yfinance as yf  \nimport matplotlib.pyplot as plt\n \ndata = yf.download('BTC-USD','2021-01-01','2021-09-30')\ndata.head()\n"", 'data = yf.download([\'BTC-USD\',\'AMD\'],\'2021-01-01\',\'2021-09-30\')\ndata[""Close""].plot()\nplt.show()\n', '!pip install twelvedata[pandas,matplotlib,plotly]\n!pip install websocket_client\nfrom twelvedata import TDClient\n# Initialize client\ntd = TDClient(apikey=""PUT_YOUR_API_KEY_HERE"")\n# Construct the necessary time serie\nts = td.time_series(\n   symbol=""MSFT"",\n   interval=""1min"",\n   outputsize=500,)\n# returns Plotly dash\nts.as_plotly_figure().show()\n'"
217,converting_calculations_in_latex/,https://analyticsindiamag.com/hands-on-tutorial-on-handcalcs-python-library-for-converting-calculations-in-latex/,"['We have seen several python libraries and packages that support various multiple tasks in the field of Data Science. Different libraries are used from Data preprocessing to data visualisation, etc. Through this article, we will discuss a new python library HandCalcs that is used to render different calculation code in Latex that is dependent on the writing of calculations. This library is helpful in terms of verifying the calculations. As there are no prerequisites of using this library which becomes easy for a person who does not know python as well. The library renders the symbolic formula, the numerical substitution, and then the result for any calculation. In this article, we will also see different usages of this library.\xa0', 'What you will learn from this article?', 'We need to first install this library. We can install this using the pip command. Use the below code for the same.\xa0', 'pip install handcalcs', 'Now we will see how it is used. For this, we need to first import the required libraries. Use the below code for the same. This library is responsible for using latex as well as rendering the code.\xa0', 'import handcalcs.render', 'Now suppose we want to render any calculations. For this, we just need to use %%render at the starting of the code. Refer to the below example for the same.', '%%render', 'a = 2', 'b = 3', 'c= a*8 + 3/b', 'We can also convert the python code into latex. For this, we need to import a library and math function. Use the below code for the same. Similar to render we need to write %%tex before the code. Refer to the below example.', 'import handcalcs.render', 'from math import sqrt, pi', '%%tex', 'a = 4 / 8 * cube(pi)', 'Three different comment tags are supported by the library for formatting your code. Let’s see these customisation tags.\xa0', '%%render', '# Parameters', 'a=1', 'b=2', 'c=3', '%%render', '# short', 'f = d / a+b # Comment', 'g = d*f / a # Comment', '%%render', '# long', 'f = d / a+b # Comment', 'g = d*f / a # Comment', '%%render', '# Symbolic', 'r = sqrt(a**2 + b**2)', 'x = b**2 + 4*a*c', 'If we want to put all the symbolic code in one cell and all other numerical results in another cell we can do that using this. In the above example, we have just used the symbolic code in one line now we will print the numerical result. Refer to the below example.\xa0', '%%render', '# Parameters', 'r\xa0', 'x', 'This library was designed in a manner that it can be used with the forallpeople package. Let us see some examples of it. Refer to the below example. For this, we need to first import the library and use the below code for the same.\xa0', 'import handcalcs.render', 'import forallpeople', ""%env 'structural'"", '%%render', '# Parameters', 'phi = 0.9', 'f_X = 67', 'We can also display the variable values in a separate cell. Refer to the below example.', 'a = 2', 'b = 4', 'We can also get just the latex code without using render by just writing %%tex before the calculations. Use the below code to do so.', '%%tex', '# Symbolic', 'r = sqrt(a**2 + b**2)', 'x = b**2 + 4*a*c', 'We can also write in subscripts using _ after the variable name. Refer to the below example for the same.\xa0', '%%render', 'a_x = 4', 'b_x_y = 2', 'We can also use greek symbols using this library. Refer to the below example for converting into greek symbols.\xa0', '%%render', 'alpha = 28', 'beta = 2', 'gamma = 5\xa0', 'This library also supports rendered-in line comments that can be helpful to write comments after each line of code or as notes. Refer to the below example for the same.\xa0', '%%render', 'lamb = 2.0303 # This comment will be displayed\xa0', 'a = 45', 'b = 34', 'c = lamb *a*b\xa0', 'Anything that is written within the parentheses will be totally rendered. Refer to the example below for the same.\xa0', '%%render', 'a = 2.8', 'b = (2*a+10)', 'Conclusion', 'In this article, we discussed the HandCalcs python library. How it can be used to convert calculations into latex and what are different usages of the library for representing different calculations. We can also export this jupyter notebook in HTML format and PDF format via latex. You can also check the GitHub repo for more information about the library whereas all the instructions are given.\xa0']","'pip install handcalcs'
 'import handcalcs.render', '%%render', 'a = 2', 'b = 3', 'c= a*8 + 3/b', 'import handcalcs.render', 'from math import sqrt, pi', '%%tex', 'a = 4 / 8 * cube(pi)', '%%render', '# Parameters', 'a=1', 'b=2', 'c=3', '%%render', '# short', 'f = d / a+b # Comment', 'g = d*f / a # Comment', '%%render', '# long', 'f = d / a+b # Comment', 'g = d*f / a # Comment', '%%render', '# Symbolic', 'r = sqrt(a**2 + b**2)', 'x = b**2 + 4*a*c', '%%render', '# Parameters', 'r\xa0', 'x', 'import handcalcs.render', 'import forallpeople', ""%env 'structural'"", '%%render', '# Parameters', 'phi = 0.9', 'f_X = 67', 'a = 2', 'b = 4', '%%tex', '# Symbolic', 'r = sqrt(a**2 + b**2)', 'x = b**2 + 4*a*c', '%%render', 'a_x = 4', 'b_x_y = 2', '%%render', 'alpha = 28', 'beta = 2', 'gamma = 5\xa0', 'lamb = 2.0303 # This comment will be displayed\xa0', 'a = 45', 'b = 34', 'c = lamb *a*b\xa0', '%%render', 'a = 2.8', 'b = (2*a+10)'"
218,drug_discovery_platform_torchdrug/,https://analyticsindiamag.com/pytorch-releases-drug-discovery-platform-torchdrug/,"['PyTorch recently announced the release of its machine learning drug discovery platform TorchDrug to accelerate drug discovery research. The library is open-sourced and can be installed through pip if you have PyTorch and torch-scatter installed using\xa0', 'pip install torchdrug, or through\xa0', 'conda conda install -c milagraph -c conda-forge torchdrug.', 'TorchDrug covers many recent techniques such as graph machine learning, deep generative models, and reinforcement learning. It also provides reusable training and evaluation routines for popular drug discovery tasks, including property prediction, pretrained molecular representations, de novo molecule design, retrosynthesis and biomedical knowledge graph reasoning. It is easy to build a prototype for one’s own dataset and application based on these techniques and modules.', 'For advanced users, the platform provides multiple levels of building blocks for different customisation demands. These include low-level data structures and operations (e.g. molecules and graph masking), mid-level layers and modules (e.g. graph convolutions and GNNs) and high-level task routines (e.g. property prediction). TorchDrug is flexible for all kinds of customisation. It also provides graph data structures and operations for manipulating biomedical objects, as well as reusable layers, models and tasks for building machine learning models.', 'The core data structures of TorchDrug are graphs, which can be used to represent a wide range of biological objects, including molecules, proteins and biomedical knowledge graphs. Visualisation API in the library can be used to check graph objects.', 'PackedGraph data structure, which builds a unified large graph and re-index each small graph in the batch, can be used to create a batch of variable-size graphs.', 'Code for calculating a batch of 4 molecules:\xa0', 'Graphs also support a wide range of indexing operations. Typical usages include applying node masking, edge masking or graph masking. The optimiser can be used for parameters in the task and combine everything into the core. The engine provides convenient routines for training and testing. To test the model on the validation set, it only takes one line.', 'TorchDrug is designed to cater to all kinds of development. This ranges from low-level data structures and operations, mid-level layers and models, to high-level tasks. One can easily customise modules at any level with minimal effort by utilising building blocks from a lower level.', 'The correspondence between modules and the hierarchical interface is :']","'mols=data.PackedMolecule.from_smiles([""CCSCCSP(=S)(OC)OC""
 ""CCOC(=O)N"", ""N(Nc1ccccc1)c2ccccc2"", ""NC(=O)c1cccnc1""])\nmols.visualize()\nmols = mols.cuda()\nprint(mols)\n# PackedMolecule(batch_size=4, num_nodes=[12, 6, 14, 9], num_edges=[22, 10, 30, 18], device=\'cuda:0\')\n'"
